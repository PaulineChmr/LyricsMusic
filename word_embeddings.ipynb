{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lyric Mood Classification - Word Embeddings\n",
    "\n",
    "The majority of the code used in this workbook can be found in `lyrics2vec.py`.\n",
    "\n",
    "The notebook is split into two parts:\n",
    "\n",
    "1. Lyrics & Vocabulary Working Examples\n",
    "2. Tensorflow & Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jcworkma/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jcworkma/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Project Imports\n",
    "from scrape_lyrics import configure_logging, logger\n",
    "from index_lyrics import read_file_contents\n",
    "import lyrics2vec\n",
    "\n",
    "# Python and Package Imports\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import datetime\n",
    "import random\n",
    "import string\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "\n",
    "# NLTK materials - make sure that you have stopwords and punkt\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# setup logging\n",
    "configure_logging(logname='lyrics2vec_notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lyrics & Vocabulary Working Examples\n",
    "### How many unique words do we have?\n",
    "\n",
    "We begin by tackling this question as an exercise to familiarize ourselves with accessing and reading the lyrics and building a vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/294299 lyric files processed. 0.00 minutes elapsed. 0 contents processed. 0 unique words acquired.\n",
      "10000/294299 lyric files processed. 0.01 minutes elapsed. 9547 contents processed. 122479 unique words acquired.\n",
      "20000/294299 lyric files processed. 0.02 minutes elapsed. 19145 contents processed. 195130 unique words acquired.\n",
      "30000/294299 lyric files processed. 0.03 minutes elapsed. 28736 contents processed. 257393 unique words acquired.\n",
      "40000/294299 lyric files processed. 0.04 minutes elapsed. 38320 contents processed. 308552 unique words acquired.\n",
      "50000/294299 lyric files processed. 0.05 minutes elapsed. 47911 contents processed. 357693 unique words acquired.\n",
      "60000/294299 lyric files processed. 0.06 minutes elapsed. 57519 contents processed. 402886 unique words acquired.\n",
      "70000/294299 lyric files processed. 0.07 minutes elapsed. 67117 contents processed. 446049 unique words acquired.\n",
      "80000/294299 lyric files processed. 0.08 minutes elapsed. 76664 contents processed. 484761 unique words acquired.\n",
      "90000/294299 lyric files processed. 0.09 minutes elapsed. 86238 contents processed. 523551 unique words acquired.\n",
      "100000/294299 lyric files processed. 0.10 minutes elapsed. 95793 contents processed. 559187 unique words acquired.\n",
      "110000/294299 lyric files processed. 0.11 minutes elapsed. 105391 contents processed. 594497 unique words acquired.\n",
      "120000/294299 lyric files processed. 0.12 minutes elapsed. 114962 contents processed. 627482 unique words acquired.\n",
      "130000/294299 lyric files processed. 0.13 minutes elapsed. 124552 contents processed. 660732 unique words acquired.\n",
      "140000/294299 lyric files processed. 0.14 minutes elapsed. 134145 contents processed. 692278 unique words acquired.\n",
      "150000/294299 lyric files processed. 0.15 minutes elapsed. 143727 contents processed. 724343 unique words acquired.\n",
      "160000/294299 lyric files processed. 0.16 minutes elapsed. 153261 contents processed. 754730 unique words acquired.\n",
      "170000/294299 lyric files processed. 0.17 minutes elapsed. 162819 contents processed. 783273 unique words acquired.\n",
      "180000/294299 lyric files processed. 0.18 minutes elapsed. 172393 contents processed. 809591 unique words acquired.\n",
      "190000/294299 lyric files processed. 0.19 minutes elapsed. 181982 contents processed. 837658 unique words acquired.\n",
      "200000/294299 lyric files processed. 0.20 minutes elapsed. 191524 contents processed. 865905 unique words acquired.\n",
      "210000/294299 lyric files processed. 0.21 minutes elapsed. 201096 contents processed. 892760 unique words acquired.\n",
      "220000/294299 lyric files processed. 0.22 minutes elapsed. 210674 contents processed. 918515 unique words acquired.\n",
      "230000/294299 lyric files processed. 0.23 minutes elapsed. 220261 contents processed. 944771 unique words acquired.\n",
      "240000/294299 lyric files processed. 0.24 minutes elapsed. 229819 contents processed. 969901 unique words acquired.\n",
      "250000/294299 lyric files processed. 0.25 minutes elapsed. 239386 contents processed. 993682 unique words acquired.\n",
      "260000/294299 lyric files processed. 0.26 minutes elapsed. 248957 contents processed. 1017986 unique words acquired.\n",
      "270000/294299 lyric files processed. 0.27 minutes elapsed. 258530 contents processed. 1041076 unique words acquired.\n",
      "280000/294299 lyric files processed. 0.28 minutes elapsed. 268134 contents processed. 1065321 unique words acquired.\n",
      "290000/294299 lyric files processed. 0.29 minutes elapsed. 277706 contents processed. 1090218 unique words acquired.\n",
      "Elapsed Time: 0.2931053082148234 minutes.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "unique_words = collections.defaultdict(lambda: 0)\n",
    "lyricfiles = os.listdir(lyrics2vec.LYRICS_TXT_DIR)\n",
    "num_files = len(lyricfiles)\n",
    "contents_processed = 0\n",
    "\n",
    "for count, lyricfile in enumerate(lyricfiles):\n",
    "\n",
    "    # progress update\n",
    "    if count % 10000 == 0:\n",
    "        print('{0}/{1} lyric files processed. {2:.02f} minutes elapsed. {3} contents processed. {4} unique words acquired.'.format(\n",
    "            count, num_files, (time.time() - start) / 60, contents_processed, len(unique_words)))\n",
    "\n",
    "    # read contents and look for unique words    \n",
    "    lyricfile = os.path.join(lyrics2vec.LYRICS_TXT_DIR, lyricfile)\n",
    "    contents = read_file_contents(lyricfile)\n",
    "    if contents and contents[0]:\n",
    "        split = contents[0].split()\n",
    "        for word in split:\n",
    "            unique_words[word] += 1\n",
    "        contents_processed += 1\n",
    "            \n",
    "end = time.time()\n",
    "elapsed = (end - start) / 60\n",
    "\n",
    "print('Elapsed Time: {0} minutes.'.format(elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Words: 1099635\n"
     ]
    }
   ],
   "source": [
    "print('Number of Unique Words: {0}'.format(len(unique_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the most common words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>1888745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>1607271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>1313028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>1119509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>1003483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>me</th>\n",
       "      <td>748343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>704609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my</th>\n",
       "      <td>614638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>613737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>582333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>510154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>And</th>\n",
       "      <td>497584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>your</th>\n",
       "      <td>460351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>409884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I'm</th>\n",
       "      <td>387302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>383197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>372311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be</th>\n",
       "      <td>337014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>326832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>312369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        count\n",
       "the   1888745\n",
       "I     1607271\n",
       "you   1313028\n",
       "to    1119509\n",
       "a     1003483\n",
       "me     748343\n",
       "and    704609\n",
       "my     614638\n",
       "in     613737\n",
       "of     582333\n",
       "it     510154\n",
       "And    497584\n",
       "your   460351\n",
       "on     409884\n",
       "I'm    387302\n",
       "that   383197\n",
       "is     372311\n",
       "be     337014\n",
       "for    326832\n",
       "all    312369"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import words into a pandas dataframe and display top N words\n",
    "df = pd.DataFrame.from_dict(unique_words, orient='index', columns=['count'])\n",
    "df = df.sort_values('count', ascending=False)\n",
    "df[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow & Word2vec\n",
    "\n",
    "To generate our word embeddings, we make use of the word2vec model as defined by [Mikolov et al](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) and the [implementation provided by TensorFlow](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py).\n",
    "\n",
    "We also utilized the following sources to provide more background and direction:\n",
    "* http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/\n",
    "* https://www.tensorflow.org/tutorials/representation/word2vec\n",
    "* https://github.com/PacktPublishing/TensorFlow-Machine-Learning-Cookbook/blob/master/Chapter%2007/doc2vec.py\n",
    "* https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-11-cnn-word2vec-41f5e28eda74\n",
    "\n",
    "Roughly, the steps we followed are\n",
    "\n",
    "1. Preprocess Lyrics\n",
    "2. Build Vocabulary\n",
    "3. Construct Dataset\n",
    "4. Train Model\n",
    "\n",
    "Our model and most of the supporting code can be found in the _lyrics2vec_ class in `lyrics2vec.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Lyrics\n",
    "\n",
    "First, when reading in the lyrics, there is some amount of preprocessing we must do to make the words more machine friendly. For our preprocessing, we\n",
    "\n",
    "1. Remove all stopwords\n",
    "2. Remove all punctuation\n",
    "3. Lowercase everything\n",
    "4. Perform tokenization with NLTK's word_tokenize function\n",
    "\n",
    "Below is an example of the output of the preprocessing. Note how contractions and certain slang words like 'wanna' are split. This is because these entities are handled as two different words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: I don't wanna die I sometimes wish I'd never been born at all\n",
      "After: [\"n't\", 'wan', 'na', 'die', 'sometimes', 'wish', \"'d\", 'never', 'born']\n"
     ]
    }
   ],
   "source": [
    "s = \"I don't wanna die I sometimes wish I'd never been born at all\"\n",
    "print('Before: {0}\\nAfter: {1}'.format(s, lyrics2vec.lyrics_preprocessing(s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Vocabulary\n",
    "\n",
    "We begin by initializing the lyrics2vec class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_vectorizer = lyrics2vec.lyrics2vec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the extract_words function to loop through all of the lyric txt files, apply the lyrics_preprocessing function, and append all tokens to a growing list.\n",
    "\n",
    "The list is saved to a file. The core one for this project being `logs/tf/vocabulary.txt` but the extract_words function can accept any abritrary words file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-25 08:35:23,531 - INFO: No word_file provided. Creating new word file at logs/tf/vocabulary.txt.\n",
      "2018-11-25 08:35:23,711 - DEBUG: 0/294299 lyric files processed. 0.00 minutes elapsed. 0 contents processed. 0 words acquired.\n",
      "2018-11-25 08:35:35,271 - DEBUG: 10000/294299 lyric files processed. 0.20 minutes elapsed. 9547 contents processed. 1259791 words acquired.\n",
      "2018-11-25 08:35:46,964 - DEBUG: 20000/294299 lyric files processed. 0.39 minutes elapsed. 19145 contents processed. 2515120 words acquired.\n",
      "2018-11-25 08:35:58,796 - DEBUG: 30000/294299 lyric files processed. 0.59 minutes elapsed. 28736 contents processed. 3779741 words acquired.\n",
      "2018-11-25 08:36:10,394 - DEBUG: 40000/294299 lyric files processed. 0.78 minutes elapsed. 38320 contents processed. 5016148 words acquired.\n",
      "2018-11-25 08:36:22,229 - DEBUG: 50000/294299 lyric files processed. 0.98 minutes elapsed. 47911 contents processed. 6281925 words acquired.\n",
      "2018-11-25 08:36:33,928 - DEBUG: 60000/294299 lyric files processed. 1.17 minutes elapsed. 57519 contents processed. 7538992 words acquired.\n",
      "2018-11-25 08:36:45,856 - DEBUG: 70000/294299 lyric files processed. 1.37 minutes elapsed. 67117 contents processed. 8816560 words acquired.\n",
      "2018-11-25 08:36:57,600 - DEBUG: 80000/294299 lyric files processed. 1.57 minutes elapsed. 76664 contents processed. 10063517 words acquired.\n",
      "2018-11-25 08:37:09,394 - DEBUG: 90000/294299 lyric files processed. 1.76 minutes elapsed. 86238 contents processed. 11319728 words acquired.\n",
      "2018-11-25 08:37:21,087 - DEBUG: 100000/294299 lyric files processed. 1.96 minutes elapsed. 95793 contents processed. 12568244 words acquired.\n",
      "2018-11-25 08:37:32,831 - DEBUG: 110000/294299 lyric files processed. 2.15 minutes elapsed. 105391 contents processed. 13816340 words acquired.\n",
      "2018-11-25 08:37:44,470 - DEBUG: 120000/294299 lyric files processed. 2.35 minutes elapsed. 114962 contents processed. 15057168 words acquired.\n",
      "2018-11-25 08:37:56,228 - DEBUG: 130000/294299 lyric files processed. 2.54 minutes elapsed. 124552 contents processed. 16318834 words acquired.\n",
      "2018-11-25 08:38:07,959 - DEBUG: 140000/294299 lyric files processed. 2.74 minutes elapsed. 134145 contents processed. 17573724 words acquired.\n",
      "2018-11-25 08:38:19,709 - DEBUG: 150000/294299 lyric files processed. 2.94 minutes elapsed. 143727 contents processed. 18823535 words acquired.\n",
      "2018-11-25 08:38:31,469 - DEBUG: 160000/294299 lyric files processed. 3.13 minutes elapsed. 153261 contents processed. 20075379 words acquired.\n",
      "2018-11-25 08:38:44,054 - DEBUG: 170000/294299 lyric files processed. 3.34 minutes elapsed. 162819 contents processed. 21319576 words acquired.\n",
      "2018-11-25 08:39:00,766 - DEBUG: 180000/294299 lyric files processed. 3.62 minutes elapsed. 172393 contents processed. 22565685 words acquired.\n",
      "2018-11-25 08:39:16,900 - DEBUG: 190000/294299 lyric files processed. 3.89 minutes elapsed. 181982 contents processed. 23828593 words acquired.\n",
      "2018-11-25 08:39:33,514 - DEBUG: 200000/294299 lyric files processed. 4.17 minutes elapsed. 191524 contents processed. 25083660 words acquired.\n",
      "2018-11-25 08:39:50,121 - DEBUG: 210000/294299 lyric files processed. 4.44 minutes elapsed. 201096 contents processed. 26344763 words acquired.\n",
      "2018-11-25 08:40:06,813 - DEBUG: 220000/294299 lyric files processed. 4.72 minutes elapsed. 210674 contents processed. 27602407 words acquired.\n",
      "2018-11-25 08:40:23,328 - DEBUG: 230000/294299 lyric files processed. 5.00 minutes elapsed. 220261 contents processed. 28858963 words acquired.\n",
      "2018-11-25 08:40:39,939 - DEBUG: 240000/294299 lyric files processed. 5.27 minutes elapsed. 229819 contents processed. 30128777 words acquired.\n",
      "2018-11-25 08:40:55,808 - DEBUG: 250000/294299 lyric files processed. 5.54 minutes elapsed. 239386 contents processed. 31369940 words acquired.\n",
      "2018-11-25 08:41:12,418 - DEBUG: 260000/294299 lyric files processed. 5.81 minutes elapsed. 248957 contents processed. 32620557 words acquired.\n",
      "2018-11-25 08:41:28,848 - DEBUG: 270000/294299 lyric files processed. 6.09 minutes elapsed. 258530 contents processed. 33862252 words acquired.\n",
      "2018-11-25 08:41:45,263 - DEBUG: 280000/294299 lyric files processed. 6.36 minutes elapsed. 268134 contents processed. 35122438 words acquired.\n",
      "2018-11-25 08:42:01,712 - DEBUG: 290000/294299 lyric files processed. 6.64 minutes elapsed. 277706 contents processed. 36370991 words acquired.\n",
      "2018-11-25 08:42:08,522 - INFO: Saving words to file logs/tf/vocabulary.txt\n",
      "2018-11-25 08:42:16,240 - INFO: 36906526 words found\n",
      "2018-11-25 08:42:16,240 - INFO: First 10 words:\n",
      "[\"'s\", 'taken', 'long', 'see', \"'ve\", 'wrong', 'see', \"'d\", 'gone', 'today']\n",
      "2018-11-25 08:42:16,241 - INFO: Elapsed Time: 6.878495792547862 minutes.\n"
     ]
    }
   ],
   "source": [
    "words = lyrics_vectorizer.extract_words(\n",
    "    preprocessing_func=lyrics2vec.lyrics_preprocessing,\n",
    "    root_dir=lyrics2vec.LYRICS_TXT_DIR,\n",
    "    words_file=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36906526 words found\n",
      "First 10 words:\n",
      "[\"'s\", 'taken', 'long', 'see', \"'ve\", 'wrong', 'see', \"'d\", 'gone', 'today']\n"
     ]
    }
   ],
   "source": [
    "print('{0} words found'.format(len(words)))\n",
    "print('First {0} words:\\n{1}'.format(10, words[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing Dataset From Vocabulary\n",
    "\n",
    "With the preprocessed vocabulary in hand, we are ready to build the dataset. The dataset will consist of four entities:\n",
    "\n",
    "* count: a dictionary that maps each unique token to its int num of occurences in the dataset\n",
    "* dictionary: a dictionary that maps each token to its int id\n",
    "* reversed_dictionary: a dictionary maps each int id to its token\n",
    "* data: a list of integer ids in order for all tokens in the dataset\n",
    "\n",
    "These four entities can be used in conjuction with one another to find, for example, a word given an integer id or vice versa or the number of times a word occurs in the dataset. They are all stored as data members of the lyrics2vec class as they are frequently referenced by the model itself.\n",
    "\n",
    "In an effort to not be weighted down by the more obscure words in the vocabulary, we've elected to maintain only the top 50,000 words in the vocabulary. The rest will be denoted as 'UNK'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_vectorizer.build_dataset(lyrics2vec.VOCAB_SIZE, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the words dictionary is no longer necessary. We remove it to free up memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory footprint is probably getting pretty large...\n",
    "# remove unneeded 'words'\n",
    "del words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are some numbers and examples from the output of build_dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of count: 50000\n",
      "count[:5]: [['UNK', 1697931], (\"'s\", 752206), (\"n't\", 698683), (\"'m\", 402884), ('love', 310636)]\n",
      "\n",
      "Length of dictionary: 50000\n",
      "dictionary[\"hello\"]: 805\n",
      "dictionary[\"world\"]: 49\n",
      "\n",
      "Length of reversed_dictionary: 50000\n",
      "reversed_dictionary[805]: hello\n",
      "reversed_dictionary[49]: world\n",
      "\n",
      "Length of data: 36906526\n",
      "data[:5]: [1, 893, 70, 17, 15]\n",
      "reversed_dictionary[data[:5]]: [\"'s\", 'taken', 'long', 'see', \"'ve\"]\n"
     ]
    }
   ],
   "source": [
    "print('Length of count: {0}'.format(len(lyrics_vectorizer.count)))\n",
    "print('count[:5]: {0}'.format(lyrics_vectorizer.count[:5]))\n",
    "print()\n",
    "print('Length of dictionary: {0}'.format(len(lyrics_vectorizer.dictionary)))\n",
    "print('dictionary[\"hello\"]: {0}'.format(lyrics_vectorizer.dictionary['hello']))\n",
    "print('dictionary[\"world\"]: {0}'.format(lyrics_vectorizer.dictionary['world']))\n",
    "print()\n",
    "print('Length of reversed_dictionary: {0}'.format(len(lyrics_vectorizer.reversed_dictionary)))\n",
    "print('reversed_dictionary[805]: {0}'.format(lyrics_vectorizer.reversed_dictionary[805]))\n",
    "print('reversed_dictionary[49]: {0}'.format(lyrics_vectorizer.reversed_dictionary[49]))\n",
    "print()\n",
    "print('Length of data: {0}'.format(len(lyrics_vectorizer.data)))\n",
    "print('data[:5]: {0}'.format(lyrics_vectorizer.data[:5]))\n",
    "l = list()\n",
    "for word_id in lyrics_vectorizer.data[:5]:\n",
    "    l.append(lyrics_vectorizer.reversed_dictionary[word_id])\n",
    "print('reversed_dictionary[data[:5]]: {0}'.format(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Word2Vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406 thinking -> 74 find\n",
      "406 thinking -> 304 tears\n",
      "113 another -> 406 thinking\n",
      "113 another -> 74 find\n",
      "41 man -> 74 find\n",
      "41 man -> 304 tears\n",
      "249 today -> 304 tears\n",
      "249 today -> 245 seen\n"
     ]
    }
   ],
   "source": [
    "batch, labels = lyrics_vectorizer._generate_batch(\n",
    "    lyrics_vectorizer.data,\n",
    "    batch_size=8,\n",
    "    num_skips=2,\n",
    "    skip_window=4)\n",
    "\n",
    "for i in range(batch_size):\n",
    "    print(batch[i], lyrics_vectorizer.reversed_dictionary[batch[i]], '->', labels[i, 0],\n",
    "        lyrics_vectorizer.reversed_dictionary[labels[i, 0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-25 09:04:27,529 - INFO: Building lyrics2vec graph\n",
      "2018-11-25 09:04:27,531 - INFO: V=50000, batch_size=128, embedding_size=128, skip_window=4, num_skips=2, num_sampled=2\n",
      "2018-11-25 09:04:29,322 - INFO: Beginning graph training\n",
      "2018-11-25 09:04:29,419 - INFO: Initialized\n",
      "2018-11-25 09:04:29,464 - DEBUG: Average loss at step 0: 284.8618469238281\n",
      "2018-11-25 09:04:29,465 - DEBUG: Time Elapsed: 0.0023838400840759276 minutes\n",
      "2018-11-25 09:04:29,492 - DEBUG: Nearest to say: ihnen, smilin, valued, cositas, bethlehem, grapes, fla, copa,\n",
      "2018-11-25 09:04:29,497 - DEBUG: Nearest to better: heap, circumstantial, barcelona, glistens, dolori, musa, sequel, ooooooo,\n",
      "2018-11-25 09:04:29,501 - DEBUG: Nearest to take: madrid, canne, cupido, llevamos, syrup, co-exist, nestle, jongen,\n",
      "2018-11-25 09:04:29,505 - DEBUG: Nearest to la: tombstone, titten, askin', rightfully, rudolph, this.., roule, organisms,\n",
      "2018-11-25 09:04:29,509 - DEBUG: Nearest to ``: orange, camden, sunce, cyclops, jy, jerz, vecchi, zona,\n",
      "2018-11-25 09:04:29,513 - DEBUG: Nearest to things: partant, grays, halcyon, qualcuno, nourish, betala, instante, colmo,\n",
      "2018-11-25 09:04:29,517 - DEBUG: Nearest to heart: garde, emme, sniffed, mayer, güei, cem, mer, ¡oi,\n",
      "2018-11-25 09:04:29,520 - DEBUG: Nearest to good: jezelf, laquelle, liste, sanas, tilts, supplier, insides, thizzle,\n",
      "2018-11-25 09:04:29,524 - DEBUG: Nearest to 'd: deal, ladrão, in-betweens, pousse, gandhi, imperium, maxim, initials,\n",
      "2018-11-25 09:04:29,528 - DEBUG: Nearest to chorus: 'sposed, autopsy, temor, declaring, sinners, cheerio, dondaine, igniting,\n",
      "2018-11-25 09:04:29,532 - DEBUG: Nearest to want: soweit, moonshine, airline, lämnade, magazine, propre, vendetta, funny,\n",
      "2018-11-25 09:04:29,536 - DEBUG: Nearest to always: gateway, schoolly, person, whitest, mystify, n'ta, scrappy, kenties,\n",
      "2018-11-25 09:04:29,540 - DEBUG: Nearest to 'm: rift, kopf, woe, croce, tame, contaminated, popped, errore,\n",
      "2018-11-25 09:04:29,543 - DEBUG: Nearest to wo: math, sette, dose, új, gracious, blåser, gastado, wager,\n",
      "2018-11-25 09:04:29,547 - DEBUG: Nearest to 2: tempo, unzip, affect, aussitôt, esperaba, toten, bicha, instruction,\n",
      "2018-11-25 09:04:29,551 - DEBUG: Nearest to make: uprise, jacksons, reins, lordy, assassine, gefahr, gazelles, birdland,\n",
      "2018-11-25 09:04:35,531 - DEBUG: Average loss at step 2000: 116.89333001995087\n",
      "2018-11-25 09:04:35,533 - DEBUG: Time Elapsed: 0.10350825786590576 minutes\n",
      "2018-11-25 09:04:41,573 - DEBUG: Average loss at step 4000: 54.614689724326134\n",
      "2018-11-25 09:04:41,574 - DEBUG: Time Elapsed: 0.204203732808431 minutes\n",
      "2018-11-25 09:04:47,597 - DEBUG: Average loss at step 6000: 35.05693628513813\n",
      "2018-11-25 09:04:47,599 - DEBUG: Time Elapsed: 0.3046190977096558 minutes\n",
      "2018-11-25 09:04:53,531 - DEBUG: Average loss at step 8000: 24.260316550850867\n",
      "2018-11-25 09:04:53,533 - DEBUG: Time Elapsed: 0.4035142779350281 minutes\n",
      "2018-11-25 09:04:59,533 - DEBUG: Average loss at step 10000: 18.818955437898637\n",
      "2018-11-25 09:04:59,535 - DEBUG: Time Elapsed: 0.5035471002260844 minutes\n",
      "2018-11-25 09:04:59,562 - DEBUG: Nearest to say: ono, faa, pillish, spoc, hungara, toc, boitshepo, pai,\n",
      "2018-11-25 09:04:59,571 - DEBUG: Nearest to better: sequel, bla, barcelona, faa, us, know, ooooooo, hungara,\n",
      "2018-11-25 09:04:59,578 - DEBUG: Nearest to take: s.e.x, hungara, granny, kiddies, moloch, falter, corrine, maya,\n",
      "2018-11-25 09:04:59,584 - DEBUG: Nearest to la: UNK, b.i.g, bla, patricia, tweet, faa, whop, moloch,\n",
      "2018-11-25 09:04:59,588 - DEBUG: Nearest to ``: terrorist, æ, bray, spoc, times, yuh, danny, loo,\n",
      "2018-11-25 09:04:59,593 - DEBUG: Nearest to things: boitshepo, yuh, j'oublierais, foo, blaguer, fix, heaven, chieva,\n",
      "2018-11-25 09:04:59,597 - DEBUG: Nearest to heart: boitshepo, hmmmm, 'm, granny, muriendo, toc, 365, garde,\n",
      "2018-11-25 09:04:59,601 - DEBUG: Nearest to good: skank, hungara, spoc, llenas, kama, chieva, toc, bizi,\n",
      "2018-11-25 09:04:59,605 - DEBUG: Nearest to 'd: moriré, visst, chemicals, home-, jingo, pillish, deal, falter,\n",
      "2018-11-25 09:04:59,608 - DEBUG: Nearest to chorus: s.e.x, dmt, pito, moriré, faa, bigfoot, hmmmm, spoc,\n",
      "2018-11-25 09:04:59,612 - DEBUG: Nearest to want: boitshepo, pillish, lämnade, bla, tmdr, faa, ono, body-ah,\n",
      "2018-11-25 09:04:59,616 - DEBUG: Nearest to always: loo, peau, klaar, johnny, boitshepo, abuela, perigo, ngaï,\n",
      "2018-11-25 09:04:59,620 - DEBUG: Nearest to 'm: 's, faa, toc, 're, ono, bigfoot, n't, chieva,\n",
      "2018-11-25 09:04:59,624 - DEBUG: Nearest to wo: hmmmm, maybelline, boitshepo, scarlet, clyde, gucci, bike, faa,\n",
      "2018-11-25 09:04:59,628 - DEBUG: Nearest to 2: tempo, boitshepo, tmdr, s.e.x, mome, spoc, dangle, foo,\n",
      "2018-11-25 09:04:59,639 - DEBUG: Nearest to make: 's, pillish, b.i.g, tmdr, pai, hmmmm, ono, pilots,\n",
      "2018-11-25 09:05:05,694 - DEBUG: Average loss at step 12000: 14.970750807881355\n",
      "2018-11-25 09:05:05,695 - DEBUG: Time Elapsed: 0.6062215884526571 minutes\n",
      "2018-11-25 09:05:11,710 - DEBUG: Average loss at step 14000: 12.297875972867011\n",
      "2018-11-25 09:05:11,712 - DEBUG: Time Elapsed: 0.7064940929412842 minutes\n",
      "2018-11-25 09:05:17,697 - DEBUG: Average loss at step 16000: 10.722489186167717\n",
      "2018-11-25 09:05:17,699 - DEBUG: Time Elapsed: 0.8062844355901082 minutes\n",
      "2018-11-25 09:05:23,714 - DEBUG: Average loss at step 18000: 9.562277947902679\n",
      "2018-11-25 09:05:23,716 - DEBUG: Time Elapsed: 0.9065634886423747 minutes\n",
      "2018-11-25 09:05:29,763 - DEBUG: Average loss at step 20000: 8.664537704586984\n",
      "2018-11-25 09:05:29,764 - DEBUG: Time Elapsed: 1.007368806997935 minutes\n",
      "2018-11-25 09:05:29,795 - DEBUG: Nearest to say: ono, spoc, pillish, faa, escalation, hungara, pai, boitshepo,\n",
      "2018-11-25 09:05:29,803 - DEBUG: Nearest to better: us, know, bla, sequel, barcelona, faa, 's, mod,\n",
      "2018-11-25 09:05:29,809 - DEBUG: Nearest to take: s.e.x, woi, 'm, escalation, svir, granny, kaller, hungara,\n",
      "2018-11-25 09:05:29,814 - DEBUG: Nearest to la: UNK, que, de, en, molly, patricia, chebba, bla,\n",
      "2018-11-25 09:05:29,819 - DEBUG: Nearest to ``: terrorist, escalation, molly, bray, jy, faltó, know, æ,\n",
      "2018-11-25 09:05:29,823 - DEBUG: Nearest to things: cairo, boitshepo, shyness, svir, faltó, j'oublierais, oop, hava,\n",
      "2018-11-25 09:05:29,827 - DEBUG: Nearest to heart: 's, hmmmm, boitshepo, 'm, toc, hava, elaine, love,\n",
      "2018-11-25 09:05:29,831 - DEBUG: Nearest to good: akobala, skank, lasso, toc, kama, hungara, bizi, spoc,\n",
      "2018-11-25 09:05:29,835 - DEBUG: Nearest to 'd: one, moriré, visst, pillish, chemicals, aaaah, home-, pusher,\n",
      "2018-11-25 09:05:29,839 - DEBUG: Nearest to chorus: s.e.x, dmt, pito, 's, hava, faa, spoc, crawly,\n",
      "2018-11-25 09:05:29,843 - DEBUG: Nearest to want: boitshepo, mod, pillish, crawly, hava, know, lämnade, bla,\n",
      "2018-11-25 09:05:29,847 - DEBUG: Nearest to always: love, peau, boitshepo, 's, danny, 'll, loo, klaar,\n",
      "2018-11-25 09:05:29,851 - DEBUG: Nearest to 'm: 's, 're, know, n't, toc, hava, crawly, like,\n",
      "2018-11-25 09:05:29,855 - DEBUG: Nearest to wo: aaaah, know, ca, hmmmm, love, boitshepo, crawly, stud,\n",
      "2018-11-25 09:05:29,859 - DEBUG: Nearest to 2: tempo, stjerner, mod, tmdr, boitshepo, s.e.x, oop, danzan,\n",
      "2018-11-25 09:05:29,863 - DEBUG: Nearest to make: 's, 'm, pillish, n't, l'alerte, b.i.g, tmdr, like,\n",
      "2018-11-25 09:05:35,874 - DEBUG: Average loss at step 22000: 7.888177440881729\n",
      "2018-11-25 09:05:35,875 - DEBUG: Time Elapsed: 1.1092168768246968 minutes\n",
      "2018-11-25 09:05:41,872 - DEBUG: Average loss at step 24000: 7.610825185656547\n",
      "2018-11-25 09:05:41,874 - DEBUG: Time Elapsed: 1.2092004299163819 minutes\n",
      "2018-11-25 09:05:47,862 - DEBUG: Average loss at step 26000: 7.0263105293512345\n",
      "2018-11-25 09:05:47,864 - DEBUG: Time Elapsed: 1.309027945995331 minutes\n",
      "2018-11-25 09:05:53,835 - DEBUG: Average loss at step 28000: 6.973290869951248\n",
      "2018-11-25 09:05:53,837 - DEBUG: Time Elapsed: 1.4085836172103883 minutes\n",
      "2018-11-25 09:05:59,867 - DEBUG: Average loss at step 30000: 6.597590457320213\n",
      "2018-11-25 09:05:59,869 - DEBUG: Time Elapsed: 1.5091060678164163 minutes\n",
      "2018-11-25 09:05:59,895 - DEBUG: Nearest to say: know, spoc, aie, well-oh, love, ono, pillish, faa,\n",
      "2018-11-25 09:05:59,902 - DEBUG: Nearest to better: us, get, know, bla, never, faa, sequel, mod,\n",
      "2018-11-25 09:05:59,907 - DEBUG: Nearest to take: aie, hawks, woi, regression, well-oh, s.e.x, escalation, na-na-na,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-25 09:05:59,912 - DEBUG: Nearest to la: que, UNK, de, diddit, well-oh, diddley, spoc, molly,\n",
      "2018-11-25 09:05:59,916 - DEBUG: Nearest to ``: '', escalation, aie, terrorist, molly, know, 's, bray,\n",
      "2018-11-25 09:05:59,920 - DEBUG: Nearest to things: cairo, boitshepo, know, aie, faltó, svir, j'oublierais, love,\n",
      "2018-11-25 09:05:59,923 - DEBUG: Nearest to heart: love, 's, na-na-na, well-oh, hmmmm, harrow, regression, aie,\n",
      "2018-11-25 09:05:59,927 - DEBUG: Nearest to good: na-na-na, akobala, 's, toc, love, well-oh, skank, spoc,\n",
      "2018-11-25 09:05:59,931 - DEBUG: Nearest to 'd: yauw, 'll, well-oh, ibiza, one, could, know, diddit,\n",
      "2018-11-25 09:05:59,935 - DEBUG: Nearest to chorus: know, diddit, faa, s.e.x, hava, dmt, pito, hmmmm,\n",
      "2018-11-25 09:05:59,939 - DEBUG: Nearest to want: know, boitshepo, well-oh, mod, crawly, n't, love, pillish,\n",
      "2018-11-25 09:05:59,950 - DEBUG: Nearest to always: 'll, na-na-na, 's, know, klaar, peau, well-oh, danny,\n",
      "2018-11-25 09:05:59,959 - DEBUG: Nearest to 'm: 're, know, 's, na-na-na, well-oh, crawly, aie, na,\n",
      "2018-11-25 09:05:59,967 - DEBUG: Nearest to wo: ca, know, n't, aaaah, hmmmm, aie, boitshepo, crawly,\n",
      "2018-11-25 09:05:59,974 - DEBUG: Nearest to 2: stjerner, boitshepo, tempo, s.e.x, mod, tmdr, oop, well-oh,\n",
      "2018-11-25 09:05:59,979 - DEBUG: Nearest to make: na-na-na, yauw, like, well-oh, aie, 'm, 's, pillish,\n",
      "2018-11-25 09:06:05,932 - DEBUG: Average loss at step 32000: 6.293437622666359\n",
      "2018-11-25 09:06:05,934 - DEBUG: Time Elapsed: 1.6101994434992473 minutes\n",
      "2018-11-25 09:06:11,970 - DEBUG: Average loss at step 34000: 6.280019340574741\n",
      "2018-11-25 09:06:11,972 - DEBUG: Time Elapsed: 1.7108249545097352 minutes\n",
      "2018-11-25 09:06:18,029 - DEBUG: Average loss at step 36000: 6.073101822018623\n",
      "2018-11-25 09:06:18,031 - DEBUG: Time Elapsed: 1.8118054032325746 minutes\n",
      "2018-11-25 09:06:24,022 - DEBUG: Average loss at step 38000: 5.921424589335919\n",
      "2018-11-25 09:06:24,024 - DEBUG: Time Elapsed: 1.9116951028505962 minutes\n",
      "2018-11-25 09:06:30,080 - DEBUG: Average loss at step 40000: 5.826526951134205\n",
      "2018-11-25 09:06:30,082 - DEBUG: Time Elapsed: 2.0126612265904744 minutes\n",
      "2018-11-25 09:06:30,113 - DEBUG: Nearest to say: know, spoc, aie, well-oh, ono, doo-ah, 's, faa,\n",
      "2018-11-25 09:06:30,121 - DEBUG: Nearest to better: us, get, never, know, bla, każdy, 'll, faa,\n",
      "2018-11-25 09:06:30,127 - DEBUG: Nearest to take: aie, 's, 'll, hawks, woi, well-oh, regression, get,\n",
      "2018-11-25 09:06:30,132 - DEBUG: Nearest to la: de, que, diddit, aie, diddley, molly, well-oh, spoc,\n",
      "2018-11-25 09:06:30,137 - DEBUG: Nearest to ``: '', escalation, aie, faltó, terrorist, doo-ah, molly, bray,\n",
      "2018-11-25 09:06:30,141 - DEBUG: Nearest to things: cairo, know, boitshepo, 're, aie, pyar, faltó, way,\n",
      "2018-11-25 09:06:30,145 - DEBUG: Nearest to heart: well-oh, hmmmm, na-na-na, aie, 's, toc, ibiza, hava,\n",
      "2018-11-25 09:06:30,149 - DEBUG: Nearest to good: rotton, well-oh, 's, na-na-na, toc, akobala, spoc, 're,\n",
      "2018-11-25 09:06:30,153 - DEBUG: Nearest to 'd: 'll, could, yauw, well-oh, ibiza, never, pyar, pillish,\n",
      "2018-11-25 09:06:30,157 - DEBUG: Nearest to chorus: know, faa, 's, well-oh, hava, aie, s.e.x, crawly,\n",
      "2018-11-25 09:06:30,161 - DEBUG: Nearest to want: know, well-oh, boitshepo, aie, crawly, 'll, pillish, faa,\n",
      "2018-11-25 09:06:30,165 - DEBUG: Nearest to always: 'll, 's, na-na-na, klaar, well-oh, danny, ibiza, boitshepo,\n",
      "2018-11-25 09:06:30,169 - DEBUG: Nearest to 'm: 're, know, na-na-na, 's, well-oh, diddley, like, aie,\n",
      "2018-11-25 09:06:30,172 - DEBUG: Nearest to wo: ca, know, n't, aua, aaaah, aie, hmmmm, faltó,\n",
      "2018-11-25 09:06:30,176 - DEBUG: Nearest to 2: oo-oo-oo-oo, 1, chorus, stjerner, baby, s.e.x, mome, boitshepo,\n",
      "2018-11-25 09:06:30,180 - DEBUG: Nearest to make: na-na-na, yauw, well-oh, 'll, aie, 's, way, like,\n",
      "2018-11-25 09:06:36,181 - DEBUG: Average loss at step 42000: 5.791693301916123\n",
      "2018-11-25 09:06:36,182 - DEBUG: Time Elapsed: 2.114338215192159 minutes\n",
      "2018-11-25 09:06:42,204 - DEBUG: Average loss at step 44000: 5.773185828328133\n",
      "2018-11-25 09:06:42,206 - DEBUG: Time Elapsed: 2.214726666609446 minutes\n",
      "2018-11-25 09:06:48,241 - DEBUG: Average loss at step 46000: 5.6665550659894945\n",
      "2018-11-25 09:06:48,243 - DEBUG: Time Elapsed: 2.3153435309727985 minutes\n",
      "2018-11-25 09:06:54,194 - DEBUG: Average loss at step 48000: 5.660135762691498\n",
      "2018-11-25 09:06:54,196 - DEBUG: Time Elapsed: 2.414556344350179 minutes\n",
      "2018-11-25 09:07:00,234 - DEBUG: Average loss at step 50000: 5.601017491102219\n",
      "2018-11-25 09:07:00,235 - DEBUG: Time Elapsed: 2.5152170221010843 minutes\n",
      "2018-11-25 09:07:00,265 - DEBUG: Nearest to say: know, spoc, aie, well-oh, orane, n't, doo-ah, ono,\n",
      "2018-11-25 09:07:00,273 - DEBUG: Nearest to better: get, us, never, know, bla, faa, każdy, aie,\n",
      "2018-11-25 09:07:00,279 - DEBUG: Nearest to take: aie, n't, hawks, get, well-oh, 's, orane, oey,\n",
      "2018-11-25 09:07:00,284 - DEBUG: Nearest to la: de, que, en, orane, UNK, diddit, eureka, molly,\n",
      "2018-11-25 09:07:00,288 - DEBUG: Nearest to ``: '', say, escalation, bray, aie, oey, faltó, doo-ah,\n",
      "2018-11-25 09:07:00,293 - DEBUG: Nearest to things: 've, know, cairo, never, way, boitshepo, aie, orane,\n",
      "2018-11-25 09:07:00,297 - DEBUG: Nearest to heart: well-oh, hmmmm, na-na-na, aie, 's, harrow, love, toc,\n",
      "2018-11-25 09:07:00,301 - DEBUG: Nearest to good: rotton, well-oh, na-na-na, toc, spoc, skank, 's, faltó,\n",
      "2018-11-25 09:07:00,304 - DEBUG: Nearest to 'd: could, 'll, chove, never, yauw, well-oh, ibiza, like,\n",
      "2018-11-25 09:07:00,308 - DEBUG: Nearest to chorus: know, well-oh, faa, hava, verse, spoc, crawly, eureka,\n",
      "2018-11-25 09:07:00,312 - DEBUG: Nearest to want: know, see, aie, eureka, well-oh, n't, crawly, boitshepo,\n",
      "2018-11-25 09:07:00,316 - DEBUG: Nearest to always: 's, 'll, na-na-na, love, never, well-oh, klaar, boitshepo,\n",
      "2018-11-25 09:07:00,320 - DEBUG: Nearest to 'm: 're, na-na-na, orane, know, well-oh, eureka, diddley, faa,\n",
      "2018-11-25 09:07:00,324 - DEBUG: Nearest to wo: ca, know, n't, ai, aaaah, eureka, hmmmm, aie,\n",
      "2018-11-25 09:07:00,327 - DEBUG: Nearest to 2: 1, chorus, oo-oo-oo-oo, baby, eureka, stjerner, s.e.x, 's,\n",
      "2018-11-25 09:07:00,331 - DEBUG: Nearest to make: 'll, na-na-na, well-oh, way, yauw, aie, orane, 's,\n",
      "2018-11-25 09:07:06,346 - DEBUG: Average loss at step 52000: 5.524343114197254\n",
      "2018-11-25 09:07:06,348 - DEBUG: Time Elapsed: 2.6170940955479938 minutes\n",
      "2018-11-25 09:07:12,327 - DEBUG: Average loss at step 54000: 5.6417739983797075\n",
      "2018-11-25 09:07:12,329 - DEBUG: Time Elapsed: 2.7167819579442343 minutes\n",
      "2018-11-25 09:07:18,305 - DEBUG: Average loss at step 56000: 5.440454009592533\n",
      "2018-11-25 09:07:18,307 - DEBUG: Time Elapsed: 2.8164114197095236 minutes\n",
      "2018-11-25 09:07:24,301 - DEBUG: Average loss at step 58000: 5.504188653290272\n",
      "2018-11-25 09:07:24,303 - DEBUG: Time Elapsed: 2.9163436770439146 minutes\n",
      "2018-11-25 09:07:30,334 - DEBUG: Average loss at step 60000: 5.302662311196327\n",
      "2018-11-25 09:07:30,335 - DEBUG: Time Elapsed: 3.016887060801188 minutes\n",
      "2018-11-25 09:07:30,366 - DEBUG: Nearest to say: know, spoc, orane, doo-ah, aie, well-oh, lovin´, ono,\n",
      "2018-11-25 09:07:30,373 - DEBUG: Nearest to better: get, know, us, pussyfootin, never, faa, aie, bla,\n",
      "2018-11-25 09:07:30,378 - DEBUG: Nearest to take: 's, hawks, aie, lovin´, well-oh, let, get, oey,\n",
      "2018-11-25 09:07:30,382 - DEBUG: Nearest to la: de, orane, doo-wop, molly, UNK, eureka, diddit, well-oh,\n",
      "2018-11-25 09:07:30,386 - DEBUG: Nearest to ``: '', say, said, escalation, bray, oey, faltó, aie,\n",
      "2018-11-25 09:07:30,390 - DEBUG: Nearest to things: know, never, way, cairo, 've, boitshepo, aie, orane,\n",
      "2018-11-25 09:07:30,394 - DEBUG: Nearest to heart: love, 's, well-oh, na-na-na, hmmmm, aie, lovin´, o-h-i-o,\n",
      "2018-11-25 09:07:30,397 - DEBUG: Nearest to good: 's, well-oh, na-na-na, rotton, spoc, love, toc, doo-wop,\n",
      "2018-11-25 09:07:30,401 - DEBUG: Nearest to 'd: could, never, would, chove, yauw, 'll, well-oh, ibiza,\n",
      "2018-11-25 09:07:30,404 - DEBUG: Nearest to chorus: verse, 's, well-oh, know, faa, doo-wop-de-wop, aie, crawly,\n",
      "2018-11-25 09:07:30,408 - DEBUG: Nearest to want: know, doo-wop-de-wop, aie, eureka, crawly, well-oh, love, orane,\n",
      "2018-11-25 09:07:30,412 - DEBUG: Nearest to always: never, 'll, 's, wit-you, na-na-na, know, love, lovin´,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-25 09:07:30,415 - DEBUG: Nearest to 'm: 're, know, orane, na-na-na, 's, eureka, well-oh, hava,\n",
      "2018-11-25 09:07:30,419 - DEBUG: Nearest to wo: ca, n't, eureka, ai, aaaah, thermals, know, doo-wop-de-wop,\n",
      "2018-11-25 09:07:30,423 - DEBUG: Nearest to 2: 1, chorus, oo-oo-oo-oo, eureka, s.e.x, 3, boitshepo, spoc,\n",
      "2018-11-25 09:07:30,426 - DEBUG: Nearest to make: na-na-na, well-oh, 's, lovin´, doo-wop-de-wop, yauw, like, aie,\n",
      "2018-11-25 09:07:36,391 - DEBUG: Average loss at step 62000: 5.29349892783165\n",
      "2018-11-25 09:07:36,392 - DEBUG: Time Elapsed: 3.11783713499705 minutes\n",
      "2018-11-25 09:07:42,432 - DEBUG: Average loss at step 64000: 5.257786111474037\n",
      "2018-11-25 09:07:42,434 - DEBUG: Time Elapsed: 3.2185301860173543 minutes\n",
      "2018-11-25 09:07:48,344 - DEBUG: Average loss at step 66000: 5.362080884873867\n",
      "2018-11-25 09:07:48,346 - DEBUG: Time Elapsed: 3.317063780625661 minutes\n",
      "2018-11-25 09:07:54,377 - DEBUG: Average loss at step 68000: 5.30212420552969\n",
      "2018-11-25 09:07:54,379 - DEBUG: Time Elapsed: 3.4176157037417094 minutes\n",
      "2018-11-25 09:08:00,351 - DEBUG: Average loss at step 70000: 5.227361513644457\n",
      "2018-11-25 09:08:00,353 - DEBUG: Time Elapsed: 3.5171718915303547 minutes\n",
      "2018-11-25 09:08:00,384 - DEBUG: Nearest to say: know, spoc, trans-europe, lovin´, orane, chorus, aie, well-oh,\n",
      "2018-11-25 09:08:00,391 - DEBUG: Nearest to better: get, kaluna, way, know, trans-europe, faa, aie, 's,\n",
      "2018-11-25 09:08:00,397 - DEBUG: Nearest to take: trans-europe, let, otsegolectric, get, 'll, hawks, lovin´, kaluna,\n",
      "2018-11-25 09:08:00,402 - DEBUG: Nearest to la: de, tu, que, orane, molly, doo-wop, eureka, trans-europe,\n",
      "2018-11-25 09:08:00,406 - DEBUG: Nearest to ``: '', said, say, escalation, bray, faltó, oey, aie,\n",
      "2018-11-25 09:08:00,410 - DEBUG: Nearest to things: know, way, trans-europe, want, kaluna, n't, boitshepo, 've,\n",
      "2018-11-25 09:08:00,414 - DEBUG: Nearest to heart: na-na-na, love, kaluna, well-oh, hmmmm, aie, ibiza, trans-europe,\n",
      "2018-11-25 09:08:00,418 - DEBUG: Nearest to good: 's, well-oh, trans-europe, rotton, love, na-na-na, spoc, know,\n",
      "2018-11-25 09:08:00,422 - DEBUG: Nearest to 'd: could, never, would, chove, 'll, yauw, well-oh, trans-europe,\n",
      "2018-11-25 09:08:00,426 - DEBUG: Nearest to chorus: verse, know, otsegolectric, kaluna, well-oh, eureka, lovin´, trans-europe,\n",
      "2018-11-25 09:08:00,430 - DEBUG: Nearest to want: know, trans-europe, n't, doo-wop-de-wop, aie, kaluna, eureka, well-oh,\n",
      "2018-11-25 09:08:00,434 - DEBUG: Nearest to always: 'll, never, know, love, kaluna, wit-you, 's, na-na-na,\n",
      "2018-11-25 09:08:00,438 - DEBUG: Nearest to 'm: 're, know, 's, orane, na-na-na, trans-europe, otsegolectric, get,\n",
      "2018-11-25 09:08:00,442 - DEBUG: Nearest to wo: ca, n't, know, kaluna, trans-europe, eureka, ai, thermals,\n",
      "2018-11-25 09:08:00,446 - DEBUG: Nearest to 2: 1, chorus, 3, UNK, verse, oo-oo-oo-oo, eureka, kaluna,\n",
      "2018-11-25 09:08:00,450 - DEBUG: Nearest to make: na-na-na, trans-europe, well-oh, know, kaluna, way, doo-wop-de-wop, lovin´,\n",
      "2018-11-25 09:08:06,450 - DEBUG: Average loss at step 72000: 5.147496344573796\n",
      "2018-11-25 09:08:06,452 - DEBUG: Time Elapsed: 3.6188294847806293 minutes\n",
      "2018-11-25 09:08:12,492 - DEBUG: Average loss at step 74000: 5.138783900141716\n",
      "2018-11-25 09:08:12,494 - DEBUG: Time Elapsed: 3.7195324500401816 minutes\n",
      "2018-11-25 09:08:18,531 - DEBUG: Average loss at step 76000: 5.207408241644502\n",
      "2018-11-25 09:08:18,533 - DEBUG: Time Elapsed: 3.820181183020274 minutes\n",
      "2018-11-25 09:08:24,513 - DEBUG: Average loss at step 78000: 5.073705090105533\n",
      "2018-11-25 09:08:24,515 - DEBUG: Time Elapsed: 3.919880227247874 minutes\n",
      "2018-11-25 09:08:30,591 - DEBUG: Average loss at step 80000: 5.0862520884275435\n",
      "2018-11-25 09:08:30,593 - DEBUG: Time Elapsed: 4.021181809902191 minutes\n",
      "2018-11-25 09:08:30,625 - DEBUG: Nearest to say: know, want, never, tell, lovin´, spoc, o-wim-o-weh, orane,\n",
      "2018-11-25 09:08:30,632 - DEBUG: Nearest to better: get, kaluna, know, faa, trans-europe, way, 'd, aie,\n",
      "2018-11-25 09:08:30,637 - DEBUG: Nearest to take: get, trans-europe, let, 'll, otsegolectric, kaluna, aie, lovin´,\n",
      "2018-11-25 09:08:30,642 - DEBUG: Nearest to la: de, que, doo-wop, eureka, orane, diddit, tu, diddley,\n",
      "2018-11-25 09:08:30,647 - DEBUG: Nearest to ``: '', said, say, faltó, escalation, oey, bray, pussyfootin,\n",
      "2018-11-25 09:08:30,651 - DEBUG: Nearest to things: way, trans-europe, know, 've, kaluna, boitshepo, aie, never,\n",
      "2018-11-25 09:08:30,655 - DEBUG: Nearest to heart: na-na-na, kaluna, well-oh, love, hmmmm, soulfinger, aie, boitshepo,\n",
      "2018-11-25 09:08:30,659 - DEBUG: Nearest to good: well-oh, trans-europe, know, na-na-na, doo-wop, o-wim-o-weh, kaluna, orane,\n",
      "2018-11-25 09:08:30,663 - DEBUG: Nearest to 'd: could, would, never, chove, yauw, 'll, well-oh, trans-europe,\n",
      "2018-11-25 09:08:30,666 - DEBUG: Nearest to chorus: verse, otsegolectric, kaluna, know, well-oh, eureka, o-wim-o-weh, crawly,\n",
      "2018-11-25 09:08:30,671 - DEBUG: Nearest to want: know, n't, trans-europe, o-wim-o-weh, aie, doo-wop-de-wop, need, say,\n",
      "2018-11-25 09:08:30,675 - DEBUG: Nearest to always: never, 'll, know, love, kaluna, trans-europe, o-wim-o-weh, wit-you,\n",
      "2018-11-25 09:08:30,679 - DEBUG: Nearest to 'm: 're, know, na-na-na, orane, 's, otsegolectric, trans-europe, see,\n",
      "2018-11-25 09:08:30,683 - DEBUG: Nearest to wo: ca, n't, want, let, know, ai, kaluna, trans-europe,\n",
      "2018-11-25 09:08:30,686 - DEBUG: Nearest to 2: 1, chorus, 3, oo-oo-oo-oo, eureka, verse, otsegolectric, crawly,\n",
      "2018-11-25 09:08:30,690 - DEBUG: Nearest to make: na-na-na, trans-europe, well-oh, yauw, kaluna, like, doo-wop-de-wop, lovin´,\n",
      "2018-11-25 09:08:36,724 - DEBUG: Average loss at step 82000: 5.114849985897541\n",
      "2018-11-25 09:08:36,726 - DEBUG: Time Elapsed: 4.1233963886896765 minutes\n",
      "2018-11-25 09:08:42,726 - DEBUG: Average loss at step 84000: 5.10062285387516\n",
      "2018-11-25 09:08:42,728 - DEBUG: Time Elapsed: 4.223424545923868 minutes\n",
      "2018-11-25 09:08:48,420 - DEBUG: Average loss at step 86000: 5.1125304037332535\n",
      "2018-11-25 09:08:48,422 - DEBUG: Time Elapsed: 4.31832312742869 minutes\n",
      "2018-11-25 09:08:53,765 - DEBUG: Average loss at step 88000: 5.030356343641877\n",
      "2018-11-25 09:08:53,766 - DEBUG: Time Elapsed: 4.407397127151489 minutes\n",
      "2018-11-25 09:08:59,029 - DEBUG: Average loss at step 90000: 5.063400269210339\n",
      "2018-11-25 09:08:59,030 - DEBUG: Time Elapsed: 4.4951306541760765 minutes\n",
      "2018-11-25 09:08:59,057 - DEBUG: Nearest to say: know, never, orane, spoc, o-wim-o-weh, tell, aie, trans-europe,\n",
      "2018-11-25 09:08:59,064 - DEBUG: Nearest to better: get, 'd, kaluna, trans-europe, way, know, faa, aie,\n",
      "2018-11-25 09:08:59,070 - DEBUG: Nearest to take: trans-europe, let, otsegolectric, get, hawks, aie, kaluna, back,\n",
      "2018-11-25 09:08:59,075 - DEBUG: Nearest to la: de, eureka, orane, diddit, doo-wop, whu, o-wim-o-weh, well-oh,\n",
      "2018-11-25 09:08:59,079 - DEBUG: Nearest to ``: '', said, say, escalation, faltó, pussyfootin, spoc, oey,\n",
      "2018-11-25 09:08:59,083 - DEBUG: Nearest to things: know, trans-europe, 've, way, could, 'd, kaluna, aie,\n",
      "2018-11-25 09:08:59,087 - DEBUG: Nearest to heart: love, kaluna, na-na-na, well-oh, aie, soulfinger, toc, boitshepo,\n",
      "2018-11-25 09:08:59,091 - DEBUG: Nearest to good: 's, well-oh, trans-europe, doo-wop, kaluna, na-na-na, spoc, o-wim-o-weh,\n",
      "2018-11-25 09:08:59,095 - DEBUG: Nearest to 'd: could, would, never, chove, well-oh, 'll, yauw, like,\n",
      "2018-11-25 09:08:59,099 - DEBUG: Nearest to chorus: verse, otsegolectric, baby, well-oh, o-wim-o-weh, kaluna, eureka, whu,\n",
      "2018-11-25 09:08:59,103 - DEBUG: Nearest to want: know, trans-europe, chorus, need, baby, doo-wop-de-wop, oey, o-wim-o-weh,\n",
      "2018-11-25 09:08:59,107 - DEBUG: Nearest to always: never, know, 'll, love, 's, trans-europe, kaluna, o-wim-o-weh,\n",
      "2018-11-25 09:08:59,111 - DEBUG: Nearest to 'm: 're, know, orane, hava, well, 's, well-oh, na-na-na,\n",
      "2018-11-25 09:08:59,115 - DEBUG: Nearest to wo: ca, kaluna, trans-europe, eureka, o-wim-o-weh, faltó, know, aie,\n",
      "2018-11-25 09:08:59,119 - DEBUG: Nearest to 2: 1, chorus, 3, oo-oo-oo-oo, otsegolectric, crawly, eureka, kaluna,\n",
      "2018-11-25 09:08:59,125 - DEBUG: Nearest to make: feel, well-oh, na-na-na, kaluna, trans-europe, lovin´, yauw, doo-wop-de-wop,\n",
      "2018-11-25 09:09:04,388 - DEBUG: Average loss at step 92000: 5.024641688644886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-25 09:09:04,390 - DEBUG: Time Elapsed: 4.584457103411356 minutes\n",
      "2018-11-25 09:09:09,487 - DEBUG: Average loss at step 94000: 5.034870611384511\n",
      "2018-11-25 09:09:09,489 - DEBUG: Time Elapsed: 4.66944450934728 minutes\n",
      "2018-11-25 09:09:15,121 - DEBUG: Average loss at step 96000: 4.993681974381208\n",
      "2018-11-25 09:09:15,123 - DEBUG: Time Elapsed: 4.76334220568339 minutes\n",
      "2018-11-25 09:09:21,105 - DEBUG: Average loss at step 98000: 4.9864495048522945\n",
      "2018-11-25 09:09:21,107 - DEBUG: Time Elapsed: 4.863083914915721 minutes\n",
      "2018-11-25 09:09:27,126 - DEBUG: Average loss at step 100000: 4.957673184514046\n",
      "2018-11-25 09:09:27,128 - DEBUG: Time Elapsed: 4.963425242900849 minutes\n",
      "2018-11-25 09:09:27,156 - DEBUG: Nearest to say: know, trans-europe, lovin´, lazerface, o-wim-o-weh, orane, chorus, well-oh,\n",
      "2018-11-25 09:09:27,164 - DEBUG: Nearest to better: get, 'd, way, kaluna, trans-europe, think, aie, faa,\n",
      "2018-11-25 09:09:27,170 - DEBUG: Nearest to take: let, trans-europe, back, lazerface, get, give, otsegolectric, baby,\n",
      "2018-11-25 09:09:27,175 - DEBUG: Nearest to la: de, orane, eureka, en, diddit, doo-wop, whu, o-wim-o-weh,\n",
      "2018-11-25 09:09:27,179 - DEBUG: Nearest to ``: '', said, say, faltó, escalation, UNK, pussyfootin, spoc,\n",
      "2018-11-25 09:09:27,184 - DEBUG: Nearest to things: could, think, trans-europe, 've, life, way, lazerface, kaluna,\n",
      "2018-11-25 09:09:27,188 - DEBUG: Nearest to heart: kaluna, love, na-na-na, lazerface, well-oh, hmmmm, aie, boitshepo,\n",
      "2018-11-25 09:09:27,192 - DEBUG: Nearest to good: 's, well-oh, spoc, trans-europe, kaluna, na-na-na, man, doo-wop,\n",
      "2018-11-25 09:09:27,196 - DEBUG: Nearest to 'd: could, would, never, well-oh, chove, think, yauw, like,\n",
      "2018-11-25 09:09:27,200 - DEBUG: Nearest to chorus: verse, otsegolectric, lazerface, well-oh, o-wim-o-weh, kaluna, trans-europe, crawly,\n",
      "2018-11-25 09:09:27,205 - DEBUG: Nearest to want: need, know, trans-europe, doo-ah, whu, say, o-wim-o-weh, oey,\n",
      "2018-11-25 09:09:27,208 - DEBUG: Nearest to always: 'll, 's, never, trans-europe, well, lazerface, kaluna, know,\n",
      "2018-11-25 09:09:27,212 - DEBUG: Nearest to 'm: 're, orane, lazerface, hava, well-oh, 's, well, still,\n",
      "2018-11-25 09:09:27,216 - DEBUG: Nearest to wo: ca, n't, trans-europe, lazerface, kaluna, eureka, want, o-wim-o-weh,\n",
      "2018-11-25 09:09:27,220 - DEBUG: Nearest to 2: 1, chorus, 3, UNK, crawly, otsegolectric, oo-oo-oo-oo, eureka,\n",
      "2018-11-25 09:09:27,224 - DEBUG: Nearest to make: trans-europe, kaluna, na-na-na, 'll, well-oh, lovin´, aie, na,\n",
      "2018-11-25 09:09:27,358 - INFO: Time Elapsed: 4.967259899775187 minutes\n",
      "2018-11-25 09:09:27,395 - DEBUG: pickled <class 'numpy.ndarray'> to logs/tf/lyrics2vec_embeddings.pickle\n"
     ]
    }
   ],
   "source": [
    "lyrics_vectorizer.train(V=lyrics2vec.VOCAB_SIZE)\n",
    "lyrics_vectorizer.save_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-25 09:12:44,796 - INFO: Beginning label plotting\n",
      "2018-11-25 09:12:44,796 - INFO: Beginning label plotting\n",
      "2018-11-25 09:20:47,172 - INFO: Elapsed Time: 8.039559098084768\n",
      "2018-11-25 09:20:47,172 - INFO: Elapsed Time: 8.039559098084768\n",
      "2018-11-25 09:20:47,174 - INFO: saved plot at logs/tf/2018-11-25_09-12-44_embeddings.png\n",
      "2018-11-25 09:20:47,174 - INFO: saved plot at logs/tf/2018-11-25_09-12-44_embeddings.png\n"
     ]
    }
   ],
   "source": [
    "embeddings_png = os.path.join(\n",
    "    lyrics2vec.LOGS_TF_DIR, \n",
    "    '{0}_{1}'.format(datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'), 'embeddings.png'))\n",
    "lyrics_vectorizer.plot_with_labels(embeddings_png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](logs/tf/2018-11-25_09-12-44_embeddings.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lyrics2vec Optimizations\n",
    "\n",
    "Because generating the vocabulary takes 6+ minutes and training the word embeddings takes 5 minutes, we've enabled several optimizations in the lyrics2vec class.\n",
    "\n",
    "1. Vocabulary Saving\n",
    "2. Dataset Pickling\n",
    "3. Embedding Saving\n",
    "\n",
    "lyrics2vec contains functions to do each of the above so that you only have to do each step once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-25 09:29:11,356 - DEBUG: pickled <class 'list'> to logs/tf/lyrics2vec_data.pickle\n",
      "2018-11-25 09:29:11,356 - DEBUG: pickled <class 'list'> to logs/tf/lyrics2vec_data.pickle\n",
      "2018-11-25 09:29:11,411 - DEBUG: pickled <class 'list'> to logs/tf/lyrics2vec_count.pickle\n",
      "2018-11-25 09:29:11,411 - DEBUG: pickled <class 'list'> to logs/tf/lyrics2vec_count.pickle\n",
      "2018-11-25 09:29:11,439 - DEBUG: pickled <class 'dict'> to logs/tf/lyrics2vec_dict.pickle\n",
      "2018-11-25 09:29:11,439 - DEBUG: pickled <class 'dict'> to logs/tf/lyrics2vec_dict.pickle\n",
      "2018-11-25 09:29:11,451 - DEBUG: pickled <class 'dict'> to logs/tf/lyrics2vec_revdict.pickle\n",
      "2018-11-25 09:29:11,451 - DEBUG: pickled <class 'dict'> to logs/tf/lyrics2vec_revdict.pickle\n",
      "2018-11-25 09:29:11,452 - INFO: datasets successfully pickled\n",
      "2018-11-25 09:29:11,452 - INFO: datasets successfully pickled\n",
      "2018-11-25 09:29:11,486 - DEBUG: pickled <class 'numpy.ndarray'> to logs/tf/lyrics2vec_embeddings.pickle\n",
      "2018-11-25 09:29:11,486 - DEBUG: pickled <class 'numpy.ndarray'> to logs/tf/lyrics2vec_embeddings.pickle\n"
     ]
    }
   ],
   "source": [
    "# vocabulary.txt was already saved as part of the extract_words step\n",
    "# save datasets\n",
    "lyrics_vectorizer.save_datasets()\n",
    "# save word embeddings\n",
    "lyrics_vectorizer.save_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also functions to let you pick up where you left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-25 09:29:59,722 - DEBUG: unpickled <class 'list'> from logs/tf/lyrics2vec_data.pickle\n",
      "2018-11-25 09:29:59,722 - DEBUG: unpickled <class 'list'> from logs/tf/lyrics2vec_data.pickle\n",
      "2018-11-25 09:30:00,378 - DEBUG: unpickled <class 'list'> from logs/tf/lyrics2vec_count.pickle\n",
      "2018-11-25 09:30:00,378 - DEBUG: unpickled <class 'list'> from logs/tf/lyrics2vec_count.pickle\n",
      "2018-11-25 09:30:00,395 - DEBUG: unpickled <class 'dict'> from logs/tf/lyrics2vec_dict.pickle\n",
      "2018-11-25 09:30:00,395 - DEBUG: unpickled <class 'dict'> from logs/tf/lyrics2vec_dict.pickle\n",
      "2018-11-25 09:30:00,404 - DEBUG: unpickled <class 'dict'> from logs/tf/lyrics2vec_revdict.pickle\n",
      "2018-11-25 09:30:00,404 - DEBUG: unpickled <class 'dict'> from logs/tf/lyrics2vec_revdict.pickle\n",
      "2018-11-25 09:30:00,418 - INFO: datasets successfully loaded via pickle\n",
      "2018-11-25 09:30:00,418 - INFO: datasets successfully loaded via pickle\n",
      "2018-11-25 09:30:00,438 - DEBUG: unpickled <class 'numpy.ndarray'> from logs/tf/lyrics2vec_embeddings.pickle\n",
      "2018-11-25 09:30:00,438 - DEBUG: unpickled <class 'numpy.ndarray'> from logs/tf/lyrics2vec_embeddings.pickle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_vectorizer.load_datasets()\n",
    "lyrics_vectorizer.load_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, the vocabulary and dataset can be loaded all in one go with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-25 09:30:46,179 - DEBUG: unpickled <class 'list'> from logs/tf/lyrics2vec_data.pickle\n",
      "2018-11-25 09:30:46,179 - DEBUG: unpickled <class 'list'> from logs/tf/lyrics2vec_data.pickle\n",
      "2018-11-25 09:30:46,700 - DEBUG: unpickled <class 'list'> from logs/tf/lyrics2vec_count.pickle\n",
      "2018-11-25 09:30:46,700 - DEBUG: unpickled <class 'list'> from logs/tf/lyrics2vec_count.pickle\n",
      "2018-11-25 09:30:46,712 - DEBUG: unpickled <class 'dict'> from logs/tf/lyrics2vec_dict.pickle\n",
      "2018-11-25 09:30:46,712 - DEBUG: unpickled <class 'dict'> from logs/tf/lyrics2vec_dict.pickle\n",
      "2018-11-25 09:30:46,721 - DEBUG: unpickled <class 'dict'> from logs/tf/lyrics2vec_revdict.pickle\n",
      "2018-11-25 09:30:46,721 - DEBUG: unpickled <class 'dict'> from logs/tf/lyrics2vec_revdict.pickle\n",
      "2018-11-25 09:30:46,722 - INFO: datasets successfully loaded via pickle\n",
      "2018-11-25 09:30:46,722 - INFO: datasets successfully loaded via pickle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lyrics2vec()>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics2vec.lyrics2vec.InitFromLyrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_w266_project",
   "language": "python",
   "name": ".venv_w266_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
