{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lyric Mood Classification - Word Embeddings\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Build vocabulary from _full_ set of song lyrics (including those without labels). Save vocabulary as \n",
    "\n",
    "word2vec\n",
    "* skip-gram\n",
    "* cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from index_lyrics import read_file_contents\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "lyrics_dir_root = 'data/lyrics/txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many unique words do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/294299 lyric files processed. 0.00 minutes elapsed. 0 contents processed. 0 unique words acquired.\n",
      "10000/294299 lyric files processed. 0.01 minutes elapsed. 9547 contents processed. 122479 unique words acquired.\n",
      "20000/294299 lyric files processed. 0.02 minutes elapsed. 19145 contents processed. 195130 unique words acquired.\n",
      "30000/294299 lyric files processed. 0.03 minutes elapsed. 28736 contents processed. 257393 unique words acquired.\n",
      "40000/294299 lyric files processed. 0.04 minutes elapsed. 38320 contents processed. 308552 unique words acquired.\n",
      "50000/294299 lyric files processed. 0.05 minutes elapsed. 47911 contents processed. 357693 unique words acquired.\n",
      "60000/294299 lyric files processed. 0.06 minutes elapsed. 57519 contents processed. 402886 unique words acquired.\n",
      "70000/294299 lyric files processed. 0.07 minutes elapsed. 67117 contents processed. 446049 unique words acquired.\n",
      "80000/294299 lyric files processed. 0.08 minutes elapsed. 76664 contents processed. 484761 unique words acquired.\n",
      "90000/294299 lyric files processed. 0.09 minutes elapsed. 86238 contents processed. 523551 unique words acquired.\n",
      "100000/294299 lyric files processed. 0.10 minutes elapsed. 95793 contents processed. 559187 unique words acquired.\n",
      "110000/294299 lyric files processed. 0.11 minutes elapsed. 105391 contents processed. 594497 unique words acquired.\n",
      "120000/294299 lyric files processed. 0.12 minutes elapsed. 114962 contents processed. 627482 unique words acquired.\n",
      "130000/294299 lyric files processed. 0.13 minutes elapsed. 124552 contents processed. 660732 unique words acquired.\n",
      "140000/294299 lyric files processed. 0.14 minutes elapsed. 134145 contents processed. 692278 unique words acquired.\n",
      "150000/294299 lyric files processed. 0.15 minutes elapsed. 143727 contents processed. 724343 unique words acquired.\n",
      "160000/294299 lyric files processed. 0.16 minutes elapsed. 153261 contents processed. 754730 unique words acquired.\n",
      "170000/294299 lyric files processed. 0.17 minutes elapsed. 162819 contents processed. 783273 unique words acquired.\n",
      "180000/294299 lyric files processed. 0.18 minutes elapsed. 172393 contents processed. 809591 unique words acquired.\n",
      "190000/294299 lyric files processed. 0.19 minutes elapsed. 181982 contents processed. 837658 unique words acquired.\n",
      "200000/294299 lyric files processed. 0.20 minutes elapsed. 191524 contents processed. 865905 unique words acquired.\n",
      "210000/294299 lyric files processed. 0.21 minutes elapsed. 201096 contents processed. 892760 unique words acquired.\n",
      "220000/294299 lyric files processed. 0.22 minutes elapsed. 210674 contents processed. 918515 unique words acquired.\n",
      "230000/294299 lyric files processed. 0.23 minutes elapsed. 220261 contents processed. 944771 unique words acquired.\n",
      "240000/294299 lyric files processed. 0.24 minutes elapsed. 229819 contents processed. 969901 unique words acquired.\n",
      "250000/294299 lyric files processed. 0.25 minutes elapsed. 239386 contents processed. 993682 unique words acquired.\n",
      "260000/294299 lyric files processed. 0.26 minutes elapsed. 248957 contents processed. 1017986 unique words acquired.\n",
      "270000/294299 lyric files processed. 0.27 minutes elapsed. 258530 contents processed. 1041076 unique words acquired.\n",
      "280000/294299 lyric files processed. 0.28 minutes elapsed. 268134 contents processed. 1065321 unique words acquired.\n",
      "290000/294299 lyric files processed. 0.29 minutes elapsed. 277706 contents processed. 1090218 unique words acquired.\n",
      "Elapsed Time: 0.29395125309626263 minutes.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "unique_words = defaultdict(lambda: 0)\n",
    "lyricfiles = os.listdir(lyrics_dir_root)\n",
    "num_files = len(lyricfiles)\n",
    "contents_processed = 0\n",
    "for count, lyricfile in enumerate(lyricfiles):\n",
    "    lyricfile = os.path.join(lyrics_dir_root, lyricfile)\n",
    "    if count % 10000 == 0:\n",
    "        print('{0}/{1} lyric files processed. {2:.02f} minutes elapsed. {3} contents processed. {4} unique words acquired.'.format(\n",
    "            count, num_files, (time.time() - start) / 60, contents_processed, len(unique_words)))\n",
    "    contents = read_file_contents(lyricfile)\n",
    "    if contents and contents[0]:\n",
    "        split = contents[0].split()\n",
    "        for word in split:\n",
    "            unique_words[word] += 1\n",
    "        contents_processed += 1\n",
    "            \n",
    "end = time.time()\n",
    "elapsed = (end - start) / 60\n",
    "\n",
    "print('Elapsed Time: {0} minutes.'.format(elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>1888745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>1607271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>1313028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>1119509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>1003483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>me</th>\n",
       "      <td>748343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>704609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my</th>\n",
       "      <td>614638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>613737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>582333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>510154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>And</th>\n",
       "      <td>497584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>your</th>\n",
       "      <td>460351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>409884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I'm</th>\n",
       "      <td>387302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>383197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>372311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be</th>\n",
       "      <td>337014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>326832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>312369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        count\n",
       "the   1888745\n",
       "I     1607271\n",
       "you   1313028\n",
       "to    1119509\n",
       "a     1003483\n",
       "me     748343\n",
       "and    704609\n",
       "my     614638\n",
       "in     613737\n",
       "of     582333\n",
       "it     510154\n",
       "And    497584\n",
       "your   460351\n",
       "on     409884\n",
       "I'm    387302\n",
       "that   383197\n",
       "is     372311\n",
       "be     337014\n",
       "for    326832\n",
       "all    312369"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(unique_words, orient='index', columns=['count'])\n",
    "df = df.sort_values('count', ascending=False)\n",
    "df[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Word2vec\n",
    "Tutorial: http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Build a list containing all words in the dataset\n",
    "2. \"Extract the top V most common words to include in our embedding vector\"\n",
    "3. \"Gather together all the unique words and index them with a unique integer value – this is what is required to create an equivalent one-hot type input for the word.  We’ll use a dictionary to do this\"\n",
    "4. \"Loop through every word in the dataset (vocabulary variable) and assign it to the unique integer word identified, created in Step 2 above.  This will allow easy lookup / processing of the word data stream\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "def lyrics_preprocessing(lyrics):\n",
    "    \"\"\"\n",
    "    Apply this function to any lyric file contents before reading for embeddings\n",
    "    \"\"\"\n",
    "    # https://stackoverflow.com/questions/17390326/getting-rid-of-stop-words-and-document-tokenization-using-nltk\n",
    "    stop = stopwords.words('english') + list(string.punctuation)\n",
    "    tokens = [i for i in word_tokenize(lyrics.lower()) if i not in stop]\n",
    "    return tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_w266_project",
   "language": "python",
   "name": ".venv_w266_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
