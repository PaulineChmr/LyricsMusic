{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lyric Mood Classification - Word Embeddings\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Build vocabulary from _full_ set of song lyrics (including those without labels). Save vocabulary as \n",
    "\n",
    "word2vec\n",
    "* skip-gram\n",
    "* cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'tensforflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-e39a4e052e3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensforflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# NLTK materials - make sure that you have stopwords and punkt for some reason\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'tensforflow'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from index_lyrics import read_file_contents\n",
    "import collections\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import random\n",
    "import tensforflow as tf\n",
    "\n",
    "# NLTK materials - make sure that you have stopwords and punkt for some reason\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "lyrics_dir_root = 'data/lyrics/txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many unique words do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/294299 lyric files processed. 0.00 minutes elapsed. 0 contents processed. 0 unique words acquired.\n",
      "10000/294299 lyric files processed. 0.01 minutes elapsed. 9547 contents processed. 122479 unique words acquired.\n",
      "20000/294299 lyric files processed. 0.02 minutes elapsed. 19145 contents processed. 195130 unique words acquired.\n",
      "30000/294299 lyric files processed. 0.03 minutes elapsed. 28736 contents processed. 257393 unique words acquired.\n",
      "40000/294299 lyric files processed. 0.04 minutes elapsed. 38320 contents processed. 308552 unique words acquired.\n",
      "50000/294299 lyric files processed. 0.05 minutes elapsed. 47911 contents processed. 357693 unique words acquired.\n",
      "60000/294299 lyric files processed. 0.06 minutes elapsed. 57519 contents processed. 402886 unique words acquired.\n",
      "70000/294299 lyric files processed. 0.07 minutes elapsed. 67117 contents processed. 446049 unique words acquired.\n",
      "80000/294299 lyric files processed. 0.08 minutes elapsed. 76664 contents processed. 484761 unique words acquired.\n",
      "90000/294299 lyric files processed. 0.09 minutes elapsed. 86238 contents processed. 523551 unique words acquired.\n",
      "100000/294299 lyric files processed. 0.10 minutes elapsed. 95793 contents processed. 559187 unique words acquired.\n",
      "110000/294299 lyric files processed. 0.11 minutes elapsed. 105391 contents processed. 594497 unique words acquired.\n",
      "120000/294299 lyric files processed. 0.12 minutes elapsed. 114962 contents processed. 627482 unique words acquired.\n",
      "130000/294299 lyric files processed. 0.13 minutes elapsed. 124552 contents processed. 660732 unique words acquired.\n",
      "140000/294299 lyric files processed. 0.14 minutes elapsed. 134145 contents processed. 692278 unique words acquired.\n",
      "150000/294299 lyric files processed. 0.15 minutes elapsed. 143727 contents processed. 724343 unique words acquired.\n",
      "160000/294299 lyric files processed. 0.16 minutes elapsed. 153261 contents processed. 754730 unique words acquired.\n",
      "170000/294299 lyric files processed. 0.17 minutes elapsed. 162819 contents processed. 783273 unique words acquired.\n",
      "180000/294299 lyric files processed. 0.18 minutes elapsed. 172393 contents processed. 809591 unique words acquired.\n",
      "190000/294299 lyric files processed. 0.19 minutes elapsed. 181982 contents processed. 837658 unique words acquired.\n",
      "200000/294299 lyric files processed. 0.20 minutes elapsed. 191524 contents processed. 865905 unique words acquired.\n",
      "210000/294299 lyric files processed. 0.21 minutes elapsed. 201096 contents processed. 892760 unique words acquired.\n",
      "220000/294299 lyric files processed. 0.22 minutes elapsed. 210674 contents processed. 918515 unique words acquired.\n",
      "230000/294299 lyric files processed. 0.23 minutes elapsed. 220261 contents processed. 944771 unique words acquired.\n",
      "240000/294299 lyric files processed. 0.24 minutes elapsed. 229819 contents processed. 969901 unique words acquired.\n",
      "250000/294299 lyric files processed. 0.25 minutes elapsed. 239386 contents processed. 993682 unique words acquired.\n",
      "260000/294299 lyric files processed. 0.26 minutes elapsed. 248957 contents processed. 1017986 unique words acquired.\n",
      "270000/294299 lyric files processed. 0.27 minutes elapsed. 258530 contents processed. 1041076 unique words acquired.\n",
      "280000/294299 lyric files processed. 0.28 minutes elapsed. 268134 contents processed. 1065321 unique words acquired.\n",
      "290000/294299 lyric files processed. 0.29 minutes elapsed. 277706 contents processed. 1090218 unique words acquired.\n",
      "Elapsed Time: 0.29395125309626263 minutes.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "unique_words = collections.defaultdict(lambda: 0)\n",
    "lyricfiles = os.listdir(lyrics_dir_root)\n",
    "num_files = len(lyricfiles)\n",
    "contents_processed = 0\n",
    "for count, lyricfile in enumerate(lyricfiles):\n",
    "    lyricfile = os.path.join(lyrics_dir_root, lyricfile)\n",
    "    if count % 10000 == 0:\n",
    "        print('{0}/{1} lyric files processed. {2:.02f} minutes elapsed. {3} contents processed. {4} unique words acquired.'.format(\n",
    "            count, num_files, (time.time() - start) / 60, contents_processed, len(unique_words)))\n",
    "    contents = read_file_contents(lyricfile)\n",
    "    if contents and contents[0]:\n",
    "        split = contents[0].split()\n",
    "        for word in split:\n",
    "            unique_words[word] += 1\n",
    "        contents_processed += 1\n",
    "            \n",
    "end = time.time()\n",
    "elapsed = (end - start) / 60\n",
    "\n",
    "print('Elapsed Time: {0} minutes.'.format(elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>1888745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>1607271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>1313028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>1119509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>1003483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>me</th>\n",
       "      <td>748343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>704609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my</th>\n",
       "      <td>614638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>613737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>582333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>510154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>And</th>\n",
       "      <td>497584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>your</th>\n",
       "      <td>460351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>409884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I'm</th>\n",
       "      <td>387302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>383197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>372311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be</th>\n",
       "      <td>337014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>326832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>312369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        count\n",
       "the   1888745\n",
       "I     1607271\n",
       "you   1313028\n",
       "to    1119509\n",
       "a     1003483\n",
       "me     748343\n",
       "and    704609\n",
       "my     614638\n",
       "in     613737\n",
       "of     582333\n",
       "it     510154\n",
       "And    497584\n",
       "your   460351\n",
       "on     409884\n",
       "I'm    387302\n",
       "that   383197\n",
       "is     372311\n",
       "be     337014\n",
       "for    326832\n",
       "all    312369"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(unique_words, orient='index', columns=['count'])\n",
    "df = df.sort_values('count', ascending=False)\n",
    "df[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Word2vec\n",
    "Tutorial: http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Build a list containing all words in the dataset\n",
    "2. \"Extract the top V most common words to include in our embedding vector\"\n",
    "3. \"Gather together all the unique words and index them with a unique integer value – this is what is required to create an equivalent one-hot type input for the word.  We’ll use a dictionary to do this\"\n",
    "4. \"Loop through every word in the dataset (vocabulary variable) and assign it to the unique integer word identified, created in Step 2 above.  This will allow easy lookup / processing of the word data stream\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lyrics_preprocessing(lyrics):\n",
    "    \"\"\"\n",
    "    Apply this function to any lyric file contents before reading for embeddings\n",
    "    \"\"\"\n",
    "    # https://stackoverflow.com/questions/17390326/getting-rid-of-stop-words-and-document-tokenization-using-nltk\n",
    "    stop = stopwords.words('english') + list(string.punctuation)\n",
    "    tokens = [i for i in word_tokenize(lyrics.lower()) if i not in stop]\n",
    "    return tokens\n",
    "\n",
    "def extract_words(root_dir, preprocessing_func, verbose=True):\n",
    "    \"\"\"\n",
    "    Iterates over all files in <root_dir>, reads contents, applies\n",
    "    <preprocessing_func> on text, and returns a python list of all words\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "\n",
    "    words = list()\n",
    "    lyricfiles = os.listdir(lyrics_dir_root)\n",
    "    contents_processed = 0\n",
    "    for count, lyricfile in enumerate(lyricfiles):\n",
    "        lyricfile = os.path.join(lyrics_dir_root, lyricfile)\n",
    "        if count % 10000 == 0 and verbose:\n",
    "            print('{0}/{1} lyric files processed. {2:.02f} minutes elapsed. {3} contents processed. {4} words acquired.'.format(\n",
    "                count, num_files, (time.time() - start) / 60, contents_processed, len(words)))\n",
    "        contents = read_file_contents(lyricfile)\n",
    "        if contents and contents[0]:\n",
    "            tokens = preprocessing_func(contents[0])\n",
    "            words += tokens\n",
    "            contents_processed += 1\n",
    "\n",
    "    end = time.time()\n",
    "    elapsed = (end - start) / 60\n",
    "\n",
    "    if verbose:\n",
    "        print('Elapsed Time: {0} minutes.'.format(elapsed))\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/294299 lyric files processed. 0.00 minutes elapsed. 0 contents processed. 0 words acquired.\n",
      "10000/294299 lyric files processed. 0.20 minutes elapsed. 9547 contents processed. 1259791 words acquired.\n",
      "20000/294299 lyric files processed. 0.39 minutes elapsed. 19145 contents processed. 2515120 words acquired.\n",
      "30000/294299 lyric files processed. 0.59 minutes elapsed. 28736 contents processed. 3779741 words acquired.\n",
      "40000/294299 lyric files processed. 0.78 minutes elapsed. 38320 contents processed. 5016148 words acquired.\n",
      "50000/294299 lyric files processed. 0.98 minutes elapsed. 47911 contents processed. 6281925 words acquired.\n",
      "60000/294299 lyric files processed. 1.17 minutes elapsed. 57519 contents processed. 7538992 words acquired.\n",
      "70000/294299 lyric files processed. 1.37 minutes elapsed. 67117 contents processed. 8816560 words acquired.\n",
      "80000/294299 lyric files processed. 1.57 minutes elapsed. 76664 contents processed. 10063517 words acquired.\n",
      "90000/294299 lyric files processed. 1.76 minutes elapsed. 86238 contents processed. 11319728 words acquired.\n",
      "100000/294299 lyric files processed. 1.96 minutes elapsed. 95793 contents processed. 12568244 words acquired.\n",
      "110000/294299 lyric files processed. 2.15 minutes elapsed. 105391 contents processed. 13816340 words acquired.\n",
      "120000/294299 lyric files processed. 2.34 minutes elapsed. 114962 contents processed. 15057168 words acquired.\n",
      "130000/294299 lyric files processed. 2.54 minutes elapsed. 124552 contents processed. 16318834 words acquired.\n",
      "140000/294299 lyric files processed. 2.73 minutes elapsed. 134145 contents processed. 17573724 words acquired.\n",
      "150000/294299 lyric files processed. 2.93 minutes elapsed. 143727 contents processed. 18823535 words acquired.\n",
      "160000/294299 lyric files processed. 3.12 minutes elapsed. 153261 contents processed. 20075379 words acquired.\n",
      "170000/294299 lyric files processed. 3.31 minutes elapsed. 162819 contents processed. 21319576 words acquired.\n",
      "180000/294299 lyric files processed. 3.50 minutes elapsed. 172393 contents processed. 22565685 words acquired.\n",
      "190000/294299 lyric files processed. 3.70 minutes elapsed. 181982 contents processed. 23828593 words acquired.\n",
      "200000/294299 lyric files processed. 3.90 minutes elapsed. 191524 contents processed. 25083660 words acquired.\n",
      "210000/294299 lyric files processed. 4.09 minutes elapsed. 201096 contents processed. 26344763 words acquired.\n",
      "220000/294299 lyric files processed. 4.29 minutes elapsed. 210674 contents processed. 27602407 words acquired.\n",
      "230000/294299 lyric files processed. 4.48 minutes elapsed. 220261 contents processed. 28858963 words acquired.\n",
      "240000/294299 lyric files processed. 4.68 minutes elapsed. 229819 contents processed. 30128777 words acquired.\n",
      "250000/294299 lyric files processed. 4.87 minutes elapsed. 239386 contents processed. 31369940 words acquired.\n",
      "260000/294299 lyric files processed. 5.06 minutes elapsed. 248957 contents processed. 32620557 words acquired.\n",
      "270000/294299 lyric files processed. 5.26 minutes elapsed. 258530 contents processed. 33862252 words acquired.\n",
      "280000/294299 lyric files processed. 5.45 minutes elapsed. 268134 contents processed. 35122438 words acquired.\n",
      "290000/294299 lyric files processed. 5.64 minutes elapsed. 277706 contents processed. 36370991 words acquired.\n",
      "Elapsed Time: 5.727279249827067 minutes.\n"
     ]
    }
   ],
   "source": [
    "words = extract_words(lyrics_dir_root, lyrics_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thank you: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py\n",
    "def build_dataset(words, n_words):\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "data_index = 0\n",
    "# generate batch data\n",
    "def generate_batch(data, batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    context = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # [ skip_window input_word skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # input word at the center of the buffer\n",
    "        targets_to_avoid = [skip_window]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]  # this is the input word\n",
    "            context[i * num_skips + j, 0] = buffer[target]  # these are the context words\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = 50000\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory footprint is probably getting pretty large...\n",
    "# remove unneeded 'words'\n",
    "del words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 294298\n"
     ]
    }
   ],
   "source": [
    "batch_size = len(os.listdir(lyrics_dir_root))  # rough approximation of appropriate number of batches\n",
    "batch_size = batch_size - 1 if batch_size % 2 != 0 else batch_size\n",
    "print('batch_size =', batch_size)\n",
    "num_skips = 2  # How many times to reuse an input to generate a label.\n",
    "skip_window = 4  # take 4 before and 4 after for context window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367 feet -> 17 see\n",
      "367 feet -> 3926 shes\n",
      "38 could -> 3926 shes\n",
      "38 could -> 2 n't\n",
      "2 n't -> 135 hard\n",
      "2 n't -> 629 front\n",
      "17 see -> 38 could\n",
      "17 see -> 3363 obvious\n",
      "294298\n",
      "294298\n"
     ]
    }
   ],
   "source": [
    "batch, labels = generate_batch(data, batch_size=batch_size, num_skips=num_skips, skip_window=skip_window)\n",
    "for i in range(8):\n",
    "    print(batch[i], reverse_dictionary[batch[i]], '->', labels[i, 0],\n",
    "        reverse_dictionary[labels[i, 0]])\n",
    "print(len(batch))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-bbc3f2985662>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mvalid_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_window\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 4: Build and train a skip-gram model.\n",
    "\n",
    "batch_size = 128\n",
    "embedding_size = 128  # Dimension of the embedding vector.\n",
    "skip_window = 1  # How many words to consider left and right.\n",
    "num_skips = 2  # How many times to reuse an input to generate a label.\n",
    "num_sampled = 64  # Number of negative examples to sample.\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent. These 3 variables are used only for\n",
    "# displaying model accuracy, they don't affect calculation.\n",
    "valid_size = 16  # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    with tf.name_scope('inputs'):\n",
    "        train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "        train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "        valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "    # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "    with tf.device('/cpu:0'):\n",
    "        # Look up embeddings for inputs.\n",
    "        with tf.name_scope('embeddings'):\n",
    "            embeddings = tf.Variable(\n",
    "                tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "            embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "    # Construct the variables for the NCE loss\n",
    "    with tf.name_scope('weights'):\n",
    "        nce_weights = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                [vocabulary_size, embedding_size],\n",
    "                stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    with tf.name_scope('biases'):\n",
    "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Compute the average NCE loss for the batch.\n",
    "    # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "    # time we evaluate the loss.\n",
    "    # Explanation of the meaning of NCE loss:\n",
    "    #   http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(\n",
    "                weights=nce_weights,\n",
    "                biases=nce_biases,\n",
    "                labels=train_labels,\n",
    "                inputs=embed,\n",
    "                num_sampled=num_sampled,\n",
    "                num_classes=vocabulary_size))\n",
    "\n",
    "    # Add the loss value as a scalar to summary.\n",
    "    tf.summary.scalar('loss', loss)\n",
    "\n",
    "    # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "    with tf.name_scope('optimizer'):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "    # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings,\n",
    "                                            valid_dataset)\n",
    "    similarity = tf.matmul(\n",
    "        valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "    # Merge all summaries.\n",
    "    merged = tf.summary.merge_all()\n",
    "\n",
    "    # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Create a saver.\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_w266_project",
   "language": "python",
   "name": ".venv_w266_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
