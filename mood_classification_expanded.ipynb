{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lyric Mood Classification - Mood Classification\n",
    "\n",
    "In the [word_embeddings](word_embeddings.ipynb) notebook, we demonstrated our embeddings model based on word2vec. In this notebook, we use those embeddings to produce real classification results with a neural network.\n",
    "\n",
    "First, we split our labeled data into the classic train-dev-test split.\n",
    "\n",
    "Second, we establish a baseline classification with simple classifiers.\n",
    "\n",
    "Third, we demonstrate our neural network architecture and model for mood classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Imports\n",
    "from index_lyrics import read_file_contents\n",
    "from label_lyrics import CSV_LABELED_LYRICS\n",
    "from scrape_lyrics import LYRICS_TXT_DIR\n",
    "from lyrics2vec import lyrics2vec\n",
    "\n",
    "# Python and Package Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lyrics2vec\n",
    "import datetime\n",
    "import json\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset consists of a large number of text files where each file represents a different song. The songs are indexed by a csv file produced by `label_lyrics.py`. We can use the index to retrieve a song's lyrics and observe its matched mood.\n",
    "\n",
    "We drop all songs that are not english, do not have lyrics available, and do not have a matched mood as classifying across languages is out of scope of this project and no classification can be done on a song without lyrics or without a matched mood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jcworkma/jack/w266-group-project_lyric-mood-classification/.venv_w266_project/lib/python3.5/site-packages/IPython/core/interactiveshell.py:3020: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['msd_id', 'msd_artist', 'msd_title', 'is_english', 'lyrics_available',\n",
       "       'wordcount', 'lyrics_filename', 'mood', 'found_tags', 'matched_mood'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we leave out the musixmatch id, artist, and title cols\n",
    "df = pd.read_csv('data/labeled_lyrics_expanded.csv', usecols=['msd_id', 'msd_artist', 'msd_title', 'is_english', 'lyrics_available', 'wordcount', 'lyrics_filename', 'mood', 'found_tags', 'matched_mood'])\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Df shape: (779056, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_english</th>\n",
       "      <th>lyrics_available</th>\n",
       "      <th>wordcount</th>\n",
       "      <th>found_tags</th>\n",
       "      <th>matched_mood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>779056.000000</td>\n",
       "      <td>779056.000000</td>\n",
       "      <td>779056.000000</td>\n",
       "      <td>779056.000000</td>\n",
       "      <td>779056.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.342566</td>\n",
       "      <td>0.420039</td>\n",
       "      <td>87.716389</td>\n",
       "      <td>5.870052</td>\n",
       "      <td>-0.575658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.474571</td>\n",
       "      <td>0.493565</td>\n",
       "      <td>141.151970</td>\n",
       "      <td>19.502538</td>\n",
       "      <td>0.638805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8623.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          is_english  lyrics_available      wordcount     found_tags  \\\n",
       "count  779056.000000     779056.000000  779056.000000  779056.000000   \n",
       "mean        0.342566          0.420039      87.716389       5.870052   \n",
       "std         0.474571          0.493565     141.151970      19.502538   \n",
       "min        -1.000000          0.000000      -1.000000      -1.000000   \n",
       "25%         0.000000          0.000000       0.000000      -1.000000   \n",
       "50%         0.000000          0.000000       0.000000      -1.000000   \n",
       "75%         1.000000          1.000000     161.000000       1.000000   \n",
       "max         1.000000          1.000000    8623.000000     100.000000   \n",
       "\n",
       "        matched_mood  \n",
       "count  779056.000000  \n",
       "mean       -0.575658  \n",
       "std         0.638805  \n",
       "min        -1.000000  \n",
       "25%        -1.000000  \n",
       "50%        -1.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Df shape:', df.shape)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After is_english filter: (266879, 10)\n",
      "After lyrics_available filter: (266783, 10)\n",
      "After matched_mood filter: (63803, 10)\n"
     ]
    }
   ],
   "source": [
    "df = df[df.is_english == 1]\n",
    "print('After is_english filter:', df.shape)\n",
    "df = df[df.lyrics_available == 1]\n",
    "print('After lyrics_available filter:', df.shape)\n",
    "df = df[df.matched_mood == 1]\n",
    "print('After matched_mood filter:', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove no longer needed columns to conserve memory\n",
    "df = df.drop(['is_english', 'lyrics_available', 'matched_mood'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4  7 15  2  1  5 17 16 11  0 13 12  9  3  8  6 10 14]\n",
      "(63803, 8)\n"
     ]
    }
   ],
   "source": [
    "# create a categorical data column for moods\n",
    "# thank you: https://stackoverflow.com/questions/38088652/pandas-convert-categories-to-numbers\n",
    "df.mood = pd.Categorical(df.mood)\n",
    "df['mood_cats'] = df.mood.cat.codes\n",
    "print(df['mood_cats'].unique())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3     And Oceans\\nMiscellaneous\\nNew Model World\\n[i...\n",
       "7     Electro eroticism\\nIntelligence is sexy\\nElect...\n",
       "17    You fight just for the sake of it\\nYou know wh...\n",
       "19    I'm in the dark, I'd like to read his mind\\nBu...\n",
       "25    There was a time\\nYou opened up every doorway\\...\n",
       "Name: lyrics, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the lyrics of each song\n",
    "def extract_lyrics(lyrics_filepath):\n",
    "    lyrics = ''\n",
    "    if os.path.exists(lyrics_filepath):\n",
    "        lyrics = read_file_contents(lyrics_filepath)[0]\n",
    "    return lyrics\n",
    "\n",
    "def make_lyrics_txt_path(lyrics_filename):\n",
    "    return os.path.join(LYRICS_TXT_DIR, lyrics_filename) + '.txt'\n",
    "\n",
    "# here we make use of panda's apply function to parallelize the IO operation\n",
    "df['lyrics'] = df.lyrics_filename.apply(lambda x: extract_lyrics(make_lyrics_txt_path(x)))\n",
    "df.lyrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final number of songs with a matched mood: 63803\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('The final number of songs with a matched mood:', df.shape[0])\n",
    "_ = df.mood.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "calm          24733\n",
       "sad            8986\n",
       "happy          6689\n",
       "depressed      5584\n",
       "upbeat         5193\n",
       "romantic       3849\n",
       "anger          3809\n",
       "angst          1085\n",
       "aggression     1015\n",
       "cheerful        946\n",
       "grief           691\n",
       "desire          267\n",
       "confident       224\n",
       "excitement      218\n",
       "brooding        212\n",
       "earnest         148\n",
       "pessimism        85\n",
       "dreamy           69\n",
       "Name: mood, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.mood.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Dev, & Test\n",
    "\n",
    "With our dataset index in hand, we are prepared to split the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thank you: https://stackoverflow.com/questions/38250710/how-to-split-data-into-3-sets-train-validation-and-test/38251213#38251213\n",
    "# optional random dataframe shuffle\n",
    "#df = df.reindex(np.random.permutation(df.index))\n",
    "np.random.seed(12)\n",
    "def split_data(data):\n",
    "    return np.split(data.sample(frac=1), [int(.6*len(data)), int(.8*len(data))])\n",
    "\n",
    "df_train, df_dev, df_test = split_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (38281, 9)\n",
      "Dev: (12761, 9)\n",
      "Test: (12761, 9)\n"
     ]
    }
   ],
   "source": [
    "print('Train:', df_train.shape)\n",
    "print('Dev:', df_dev.shape)\n",
    "print('Test:', df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# thank you: https://stackoverflow.com/questions/14992644/turn-pandas-dataframe-of-strings-into-histogram\n",
    "_ = df_train.mood.value_counts().plot(kind='bar', title='Train Dataset Mood Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = df_dev.mood.value_counts().plot(kind='bar', title='Dev Dataset Mood Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = df_test.mood.value_counts().plot(kind='bar', title='Test Dataset Mood Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most-Common-Case Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pprint_accuracy(acc):\n",
    "    return '{0:.02f}%'.format(acc * 100)\n",
    "\n",
    "def most_common_case_classification(x, y):\n",
    "    assert len(x) == len(y)\n",
    "    total_count = len(x)\n",
    "    common_count = y.value_counts().max()\n",
    "    accuracy = common_count / total_count\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common case for each dataset split is\n",
      "\tTrain: calm\n",
      "\tDev: calm\n",
      "\tTest: calm\n",
      "The accuracy of the most-common-case classifier for each dataset split is\n",
      "\tTrain: 38.81%\n",
      "\tDev: 39.26%\n",
      "\tTest: 38.15%\n"
     ]
    }
   ],
   "source": [
    "print('The most common case for each dataset split is')\n",
    "print('\\tTrain:', df_train.mood.value_counts().idxmax())\n",
    "print('\\tDev:', df_dev.mood.value_counts().idxmax())\n",
    "print('\\tTest:', df_test.mood.value_counts().idxmax())\n",
    "#df_test.loc[df_test.mood_classes.idxmax()].mood\n",
    "print('The accuracy of the most-common-case classifier for each dataset split is')\n",
    "print('\\tTrain:', pprint_accuracy(most_common_case_classification(df_train.lyrics_filename, df_train.mood)))\n",
    "print('\\tDev:', pprint_accuracy(most_common_case_classification(df_dev.lyrics_filename, df_dev.mood)))\n",
    "print('\\tTest:', pprint_accuracy(most_common_case_classification(df_test.lyrics_filename, df_test.mood)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive-Bayes Classification\n",
    "\n",
    "The Naive-Bayes Classifier will require the actual lyrical text for classification, so we begin by reading into memory the text for each song in our dataset.\n",
    "\n",
    "Once we have the lyrics, we use the python sklearn package to vectorize and process the lyrical text, fit the Naive Bayes Classifier, and compute the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38281, 62247)\n",
      "(12761, 62247)\n",
      "(12761, 62247)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "def prep_lyrics_for_nb(lyrics_series, count_vect=None, tfidf_transformer=None):\n",
    "   # thank you: https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "    if not count_vect:\n",
    "        count_vect = CountVectorizer()\n",
    "        count_vect = count_vect.fit(lyrics_series)\n",
    "    x_train_counts = count_vect.transform(lyrics_series)\n",
    "    if not tfidf_transformer:\n",
    "        tfidf_transformer = TfidfTransformer()\n",
    "        tfidf_transformer = tfidf_transformer.fit(x_train_counts)\n",
    "    x_train_tfidf = tfidf_transformer.fit_transform(x_train_counts)\n",
    "    return count_vect, x_train_counts, tfidf_transformer, x_train_tfidf\n",
    "\n",
    "# convert lyrics to counts and term-frequencies\n",
    "count_vect, x_train_counts, tfidf_transformer, x_train_tfidf = prep_lyrics_for_nb(df_train.lyrics)\n",
    "print(x_train_tfidf.shape)\n",
    "_, x_dev_counts, _, x_dev_tfidf = prep_lyrics_for_nb(df_dev.lyrics, count_vect, tfidf_transformer)\n",
    "print(x_dev_tfidf.shape)\n",
    "_, x_test_counts, _, x_test_tfidf = prep_lyrics_for_nb(df_test.lyrics, count_vect, tfidf_transformer)\n",
    "print(x_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classifier Dev Accuracy: 39.43%\n",
      "Naive Bayes Classifier Test Accuracy: 38.25%\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB().fit(x_train_tfidf, df_train.mood_cats) \n",
    "dev_acc = clf.score(x_dev_tfidf, df_dev.mood_cats)\n",
    "print('Naive Bayes Classifier Dev Accuracy:', pprint_accuracy(dev_acc))\n",
    "test_acc = clf.score(x_test_tfidf, df_test.mood_cats)\n",
    "print('Naive Bayes Classifier Test Accuracy:', pprint_accuracy(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Classification\n",
    "\n",
    "From [Corona & O'Mahony](https://www.researchgate.net/publication/280733696_An_Exploration_of_Mood_Classification_in_the_Million_Songs_Dataset), SVMs have been used with success in this area. Here we see that they do provide a sizeable increase in accuracy over NB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Classifier Dev Accuracy: 43.76%\n",
      "SVM Classifier Test Accuracy: 42.89%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42,\n",
    "            max_iter=5, tol=None).fit(x_train_tfidf, df_train.mood_cats)\n",
    "dev_acc = clf.score(x_dev_tfidf, df_dev.mood_cats)\n",
    "print('SVM Classifier Dev Accuracy:', pprint_accuracy(dev_acc))\n",
    "test_acc = clf.score(x_test_tfidf, df_test.mood_cats)\n",
    "print('SVM Classifier Test Accuracy:', pprint_accuracy(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN\n",
    "\n",
    "http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/\n",
    "\n",
    "CNNs process textual input different than a Naive Bayes or SVM classifier. We perform the following data processing steps on all lyrics:\n",
    "* Truncate/extend all songs to the 75% wordcount percentile\n",
    "* Tokenize lyrics with nltk's word_tokenize function\n",
    "* Remove all stopwords that match from within nltk's stopwords corpus\n",
    "* Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    63803.000000\n",
      "mean       231.658982\n",
      "std        139.035896\n",
      "min          1.000000\n",
      "25%        143.000000\n",
      "50%        204.000000\n",
      "75%        282.000000\n",
      "max       2913.000000\n",
      "Name: wordcount, dtype: float64\n",
      "\n",
      "All songs will be limited to 282 words\n"
     ]
    }
   ],
   "source": [
    "pctiles = df.wordcount.describe()\n",
    "print(pctiles)\n",
    "cutoff = int(pctiles[pctiles.index.str.startswith('75%')][0])\n",
    "print('\\nAll songs will be limited to {0} words'.format(cutoff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(lyrics2vec)\n",
    "lyrics_vectorizer = lyrics2vec.lyrics2vec.InitFromLyrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data normalized (0.8400406042734782 minutes)\n",
      "228247    [526, 43, 99, 5, 15, 77, 234, 179, 19, 86, 383...\n",
      "214654    [37, 93, 868, 3364, 2, 12, 596, 1668, 873, 4, ...\n",
      "175139    [71, 44, 297, 250, 2271, 1511, 127, 1310, 231,...\n",
      "58579     [37, 93, 1, 100, 382, 122, 181, 38, 162, 35, 1...\n",
      "196716    [253, 7, 43, 22, 1, 209, 209, 3, 147, 7, 94, 7...\n",
      "Name: normalized_lyrics, dtype: object\n",
      "dev data normalized (1.1153544306755065 minutes)\n",
      "149604    [315, 26, 8, 95, 280, 7, 136, 201, 5, 37330, 4...\n",
      "71942     [37, 93, 24, 68, 1462, 12, 213, 6044, 43, 231,...\n",
      "91524     [37, 93, 269, 14574, 130, 1032, 1107, 6, 1467,...\n",
      "238415    [37, 93, 60, 245, 800, 2179, 0, 288, 250, 0, 1...\n",
      "162087    [37, 143, 1495, 143, 146, 48086, 14761, 391, 4...\n",
      "Name: normalized_lyrics, dtype: object\n",
      "test data normalized (1.3944415728251138 minutes)\n",
      "248608    [24, 26, 12, 766, 7, 222, 6, 45, 2295, 1402, 2...\n",
      "240285    [99, 212, 99, 99, 315, 124, 1840, 194, 235, 33...\n",
      "200631    [15, 294, 3904, 656, 193, 1133, 193, 4136, 291...\n",
      "187697    [3866, 856, 23149, 334, 3869, 306, 0, 2540, 22...\n",
      "83000     [37, 93, 27, 667, 15, 8, 31, 3065, 27, 12, 330...\n",
      "Name: normalized_lyrics, dtype: object\n",
      "\n",
      "Example of padding:\n",
      "\tFirst 5 tokens: [526, 43, 99, 5, 15]\n",
      "\tLast 5 tokens: [0, 0, 0, 0, 0].\n",
      "\n",
      "Elapsed Time: 1.3946945190429687 minutes\n"
     ]
    }
   ],
   "source": [
    "def normalize_lyrics(lyrics, max_length, lyrics_vectorizer):\n",
    "    \"\"\"\n",
    "    Tokenize, process, shorten/lengthen, and vectorize lyrics\n",
    "    \"\"\"\n",
    "    lyrics = lyrics2vec.lyrics_preprocessing(lyrics)\n",
    "    if len(lyrics) > max_length:\n",
    "        lyrics = lyrics[:max_length]\n",
    "    else:\n",
    "        lyrics += ['<PAD>'] * (int(max_length) - int(len(lyrics)))\n",
    "\n",
    "    lyric_vector = lyrics_vectorizer.transform(lyrics)\n",
    "    return lyric_vector\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# here we make use of panda's apply function to parallelize the IO operation (again)\n",
    "df_train['normalized_lyrics'] = df_train.lyrics.apply(lambda x: normalize_lyrics(x, cutoff, lyrics_vectorizer))\n",
    "print('train data normalized ({0} minutes)'.format((time.time() - start) / 60))\n",
    "print(df_train.normalized_lyrics.head())\n",
    "\n",
    "df_dev['normalized_lyrics'] = df_dev.lyrics.apply(lambda x: normalize_lyrics(x, cutoff, lyrics_vectorizer))\n",
    "print('dev data normalized ({0} minutes)'.format((time.time() - start) / 60))\n",
    "print(df_dev.normalized_lyrics.head())\n",
    "\n",
    "df_test['normalized_lyrics'] = df_test.lyrics.apply(lambda x: normalize_lyrics(x, cutoff, lyrics_vectorizer))\n",
    "print('test data normalized ({0} minutes)'.format((time.time() - start) / 60))\n",
    "print(df_test.normalized_lyrics.head())\n",
    "\n",
    "print('\\nExample of padding:')\n",
    "example = df_train.normalized_lyrics[df_train.normalized_lyrics.str.len() == cutoff].iloc[0]\n",
    "print('\\tFirst 5 tokens: {0}'.format(example[:5]))\n",
    "print('\\tLast 5 tokens: {0}.'.format(example[-5:]))\n",
    "\n",
    "print('\\nElapsed Time: {0} minutes'.format((time.time() - start) / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Following code is borrowed from [this excellent and very helpful blog post](http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/) to PoC the validity of the preprocessed tokens and word embeddings**\n",
    "\n",
    "And here's a walkthrough of that blog post's code: https://agarnitin86.github.io/blog/2016/12/23/text-classification-cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size,\n",
    "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0,\n",
    "      embeddings=None):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        # for loading word2vec: https://stackoverflow.com/questions/35687678/using-a-pre-trained-word-embedding-word2vec-or-glove-in-tensorflow\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "\n",
    "            self.pretrained_embeddings = embeddings is not None\n",
    "            if self.pretrained_embeddings:\n",
    "                self.W = tf.get_variable(\n",
    "                    shape=embeddings.shape,\n",
    "                    initializer=tf.constant_initializer(embeddings),\n",
    "                    trainable=True,\n",
    "                    name=\"W\")\n",
    "            else:\n",
    "                self.W = tf.Variable(\n",
    "                    tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                    name=\"W\")\n",
    "            \n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                Wconv = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"Wconv\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    Wconv,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            Wconv = tf.get_variable(\n",
    "                \"Wconv\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(Wconv)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, Wconv, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # Calculate mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "SAVE_STEP_DATA = True\n",
    "USE_TIME_AS_KEY = False\n",
    "\n",
    "# Model Hyperparameters\n",
    "embedding_dim = 300\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 64\n",
    "dropout_keep_prob = 0.5\n",
    "l2_reg_lambda = 0.01\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "evaluate_every = 100\n",
    "checkpoint_every = 100\n",
    "num_checkpoints = 5\n",
    "\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    print('num_batches_per_epoch = {0}'.format(num_batches_per_epoch))\n",
    "    for epoch in range(num_epochs):\n",
    "        print('***********************************************')\n",
    "        print('Epoch {0}/{1}\\n'.format(epoch, num_epochs))\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            print('-----------------------------------------------')\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            print('Epoch {0}/{1}, Batch {2}/{3} (start={4}, end={5})'.format(\n",
    "                epoch, num_epochs, batch_num, num_batches_per_epoch, start_index, end_index))\n",
    "            yield shuffled_data[start_index:end_index]\n",
    "            \n",
    "def train(vocab_size, x_train, y_train, x_dev, y_dev, x_test, y_test, train_embeddings=False, embeddings=None):\n",
    "    # Training\n",
    "    # ==================================================\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        session_conf = tf.ConfigProto()\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            cnn = TextCNN(\n",
    "                sequence_length=x_train.shape[1],\n",
    "                num_classes=y_train.shape[1],\n",
    "                #vocab_size=len(vocab_processor.vocabulary_),\n",
    "                vocab_size=vocab_size,\n",
    "                embedding_size=embedding_dim,\n",
    "                filter_sizes=filter_sizes,\n",
    "                num_filters=num_filters,\n",
    "                l2_reg_lambda=l2_reg_lambda,\n",
    "                embeddings=embeddings)\n",
    "\n",
    "            # Define Training procedure\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "            grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "            # Keep track of gradient values and sparsity (optional)\n",
    "            grad_summaries = []\n",
    "            for g, v in grads_and_vars:\n",
    "                if g is not None:\n",
    "                    grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                    sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                    grad_summaries.append(grad_hist_summary)\n",
    "                    grad_summaries.append(sparsity_summary)\n",
    "            grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "            # Output directory for models and summaries\n",
    "            if USE_TIME_AS_KEY:\n",
    "                unique_key = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "            else:\n",
    "                unique_key = 'Em-{0}_FS-{1}_NF-{2}_D-{3}_L2-{4}_B-{5}_Ep-{6}_W2V-{7}{8}_V-{9}'.format(\n",
    "                    embedding_dim, '-'.join(map(str, filter_sizes)),\n",
    "                    num_filters,\n",
    "                    dropout_keep_prob,\n",
    "                    l2_reg_lambda,\n",
    "                    batch_size,\n",
    "                    num_epochs,\n",
    "                    1 if cnn.pretrained_embeddings else 0,\n",
    "                    '-Tr' if train_embeddings else '',\n",
    "                    vocab_size)\n",
    "            out_dir = os.path.abspath(os.path.join(lyrics2vec.LOGS_TF_DIR, \"runs\", unique_key))\n",
    "            os.makedirs(out_dir)\n",
    "            print(\"Writing to {}\\n\".format(out_dir))\n",
    "            \n",
    "            # dump params to json in case they need to be referenced later\n",
    "            with open(os.path.join(out_dir, 'model_params.json'), 'w') as outfile:\n",
    "                model_params = {\n",
    "                    'embedding_dim': embedding_dim,\n",
    "                    'filter_sizes': filter_sizes,\n",
    "                    'num_filters': num_filters,\n",
    "                    'dropout_keep_prob': dropout_keep_prob,\n",
    "                    'l2_reg_lambda': l2_reg_lambda,\n",
    "                    'batch_size': batch_size,\n",
    "                    'num_epochs': num_epochs,\n",
    "                    'evaluate_every': evaluate_every,\n",
    "                    'checkpoint_every': checkpoint_every,\n",
    "                    'num_checkpoints': num_checkpoints,\n",
    "                    'train_embeddings': train_embeddings,\n",
    "                    'pretrained_embeddings': cnn.pretrained_embeddings\n",
    "                }\n",
    "                json.dump(model_params, outfile, sort_keys=True)\n",
    "                \n",
    "            # Summaries for loss and accuracy\n",
    "            loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "            acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "            # Train Summaries\n",
    "            summary_dir = os.path.join(out_dir, \"summaries\")\n",
    "            train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "            train_summary_writer = tf.summary.FileWriter(os.path.join(summary_dir, \"train\"), sess.graph)\n",
    "\n",
    "            # Dev summaries\n",
    "            dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "            dev_summary_writer = tf.summary.FileWriter(os.path.join(summary_dir, \"dev\"), sess.graph)\n",
    "\n",
    "            # Test summaries\n",
    "            test_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "            test_summary_writer = tf.summary.FileWriter(os.path.join(summary_dir, \"test\"), sess.graph)\n",
    "            \n",
    "            # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "            checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            saver = tf.train.Saver(tf.global_variables(), max_to_keep=num_checkpoints)\n",
    "\n",
    "            # Write vocabulary\n",
    "            #vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "            # Initialize all variables\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            def train_step(x_batch, y_batch, summary_writer=None, step_writer=None):\n",
    "                \"\"\"\n",
    "                A single training step\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                  cnn.input_x: x_batch,\n",
    "                  cnn.input_y: y_batch,\n",
    "                  cnn.dropout_keep_prob: dropout_keep_prob\n",
    "                }\n",
    "                _, step, summaries, loss, accuracy = sess.run(\n",
    "                    [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                    feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "                if summary_writer:\n",
    "                    summary_writer.add_summary(summaries, step)\n",
    "                if step_writer:\n",
    "                    step_writer.writerow(['train', time_str, step, loss, accuracy])\n",
    "                return time_str, step, loss, accuracy\n",
    "\n",
    "            def dev_step(x_batch, y_batch, summary_writer=None, step_writer=None):\n",
    "                \"\"\"\n",
    "                Evaluates model on a dev set\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                  cnn.input_x: x_batch,\n",
    "                  cnn.input_y: y_batch,\n",
    "                  cnn.dropout_keep_prob: 1.0\n",
    "                }\n",
    "                step, summaries, loss, accuracy = sess.run(\n",
    "                    [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                    feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "                if summary_writer:\n",
    "                    summary_writer.add_summary(summaries, step)\n",
    "                if step_writer:\n",
    "                    step_writer.writerow(['train', time_str, step, loss, accuracy])\n",
    "                return time_str, step, loss, accuracy\n",
    "                   \n",
    "            csvwriter = None\n",
    "            if SAVE_STEP_DATA:\n",
    "                csvfile = open(os.path.join(out_dir, 'step_data.csv'), 'w')\n",
    "                csvwriter = csv.writer(csvfile)\n",
    "                csvwriter.writerow(['dataset', 'time', 'step', 'loss', 'acc'])\n",
    "                \n",
    "            # Generate batches\n",
    "            batches = batch_iter(\n",
    "                list(zip(x_train, y_train)), batch_size, num_epochs)\n",
    "            # Training loop. For each batch...\n",
    "            for batch in batches:\n",
    "                x_batch, y_batch = zip(*batch)\n",
    "                train_step(x_batch, y_batch, summary_writer=train_summary_writer, step_writer=csvwriter)\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "                if current_step % evaluate_every == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    dev_step(x_dev, y_dev, summary_writer=dev_summary_writer, step_writer=csvwriter)\n",
    "                    print(\"\")\n",
    "                if current_step % checkpoint_every == 0:\n",
    "                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                    \n",
    "            print(\"\\nFinal Test Evaluation:\")\n",
    "            dev_step(x_test, y_test, summary_writer=test_summary_writer, step_writer=csvwriter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings shape: (50000, 300)\n"
     ]
    }
   ],
   "source": [
    "# get our pre-trained word2vec embeddings\n",
    "lyrics_vectorizer = lyrics2vec.lyrics2vec()\n",
    "embeddings_loaded = lyrics_vectorizer.load_embeddings()\n",
    "if embeddings_loaded:\n",
    "    print('embeddings shape:', lyrics_vectorizer.final_embeddings.shape)\n",
    "else:\n",
    "    print('failed to load embeddings!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/Wconv:0/grad/hist is illegal; using conv-maxpool-3/Wconv_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/Wconv:0/grad/sparsity is illegal; using conv-maxpool-3/Wconv_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/Wconv:0/grad/hist is illegal; using conv-maxpool-4/Wconv_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/Wconv:0/grad/sparsity is illegal; using conv-maxpool-4/Wconv_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/Wconv:0/grad/hist is illegal; using conv-maxpool-5/Wconv_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/Wconv:0/grad/sparsity is illegal; using conv-maxpool-5/Wconv_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name Wconv:0/grad/hist is illegal; using Wconv_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name Wconv:0/grad/sparsity is illegal; using Wconv_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000\n",
      "\n",
      "num_batches_per_epoch = 599\n",
      "***********************************************\n",
      "Epoch 0/10\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 0/599 (start=0, end=64)\n",
      "2018-11-27T04:12:43.347348: step 1, loss 3.83636, acc 0.078125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 1/599 (start=64, end=128)\n",
      "2018-11-27T04:12:43.688919: step 2, loss 3.5957, acc 0.03125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 2/599 (start=128, end=192)\n",
      "2018-11-27T04:12:44.002383: step 3, loss 3.0945, acc 0.140625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 3/599 (start=192, end=256)\n",
      "2018-11-27T04:12:44.333070: step 4, loss 3.01978, acc 0.140625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 4/599 (start=256, end=320)\n",
      "2018-11-27T04:12:44.665721: step 5, loss 2.91782, acc 0.109375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 5/599 (start=320, end=384)\n",
      "2018-11-27T04:12:44.992122: step 6, loss 2.79916, acc 0.15625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 6/599 (start=384, end=448)\n",
      "2018-11-27T04:12:45.338021: step 7, loss 2.75212, acc 0.171875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 7/599 (start=448, end=512)\n",
      "2018-11-27T04:12:45.662243: step 8, loss 2.59854, acc 0.1875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 8/599 (start=512, end=576)\n",
      "2018-11-27T04:12:45.967733: step 9, loss 2.46535, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 9/599 (start=576, end=640)\n",
      "2018-11-27T04:12:46.312752: step 10, loss 2.39468, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 10/599 (start=640, end=704)\n",
      "2018-11-27T04:12:46.636079: step 11, loss 2.50421, acc 0.28125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 11/599 (start=704, end=768)\n",
      "2018-11-27T04:12:46.973862: step 12, loss 2.46779, acc 0.265625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 12/599 (start=768, end=832)\n",
      "2018-11-27T04:12:47.304358: step 13, loss 2.24795, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 13/599 (start=832, end=896)\n",
      "2018-11-27T04:12:47.631441: step 14, loss 2.24557, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 14/599 (start=896, end=960)\n",
      "2018-11-27T04:12:47.988680: step 15, loss 2.25982, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 15/599 (start=960, end=1024)\n",
      "2018-11-27T04:12:48.327603: step 16, loss 2.21184, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 16/599 (start=1024, end=1088)\n",
      "2018-11-27T04:12:48.658315: step 17, loss 2.50642, acc 0.3125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 17/599 (start=1088, end=1152)\n",
      "2018-11-27T04:12:48.992402: step 18, loss 2.23503, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 18/599 (start=1152, end=1216)\n",
      "2018-11-27T04:12:49.313961: step 19, loss 2.26496, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 19/599 (start=1216, end=1280)\n",
      "2018-11-27T04:12:49.649773: step 20, loss 1.94788, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 20/599 (start=1280, end=1344)\n",
      "2018-11-27T04:12:49.999272: step 21, loss 2.72757, acc 0.25\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 21/599 (start=1344, end=1408)\n",
      "2018-11-27T04:12:50.340809: step 22, loss 2.53935, acc 0.28125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 22/599 (start=1408, end=1472)\n",
      "2018-11-27T04:12:50.653808: step 23, loss 2.37156, acc 0.328125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 23/599 (start=1472, end=1536)\n",
      "2018-11-27T04:12:50.984886: step 24, loss 2.37442, acc 0.234375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 24/599 (start=1536, end=1600)\n",
      "2018-11-27T04:12:51.329990: step 25, loss 2.25672, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 25/599 (start=1600, end=1664)\n",
      "2018-11-27T04:12:51.648185: step 26, loss 2.34415, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 26/599 (start=1664, end=1728)\n",
      "2018-11-27T04:12:51.962209: step 27, loss 2.35914, acc 0.21875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 27/599 (start=1728, end=1792)\n",
      "2018-11-27T04:12:52.304180: step 28, loss 2.40848, acc 0.25\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 28/599 (start=1792, end=1856)\n",
      "2018-11-27T04:12:52.610788: step 29, loss 2.30558, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 29/599 (start=1856, end=1920)\n",
      "2018-11-27T04:12:52.928217: step 30, loss 2.29188, acc 0.3125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 30/599 (start=1920, end=1984)\n",
      "2018-11-27T04:12:53.262791: step 31, loss 2.24229, acc 0.25\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 31/599 (start=1984, end=2048)\n",
      "2018-11-27T04:12:53.601306: step 32, loss 2.29132, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 32/599 (start=2048, end=2112)\n",
      "2018-11-27T04:12:53.940768: step 33, loss 2.11129, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 33/599 (start=2112, end=2176)\n",
      "2018-11-27T04:12:54.251924: step 34, loss 2.21845, acc 0.28125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 34/599 (start=2176, end=2240)\n",
      "2018-11-27T04:12:54.577562: step 35, loss 2.33709, acc 0.265625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 35/599 (start=2240, end=2304)\n",
      "2018-11-27T04:12:54.920409: step 36, loss 2.50841, acc 0.296875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 36/599 (start=2304, end=2368)\n",
      "2018-11-27T04:12:55.265486: step 37, loss 2.38561, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 37/599 (start=2368, end=2432)\n",
      "2018-11-27T04:12:55.596759: step 38, loss 2.15938, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 38/599 (start=2432, end=2496)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:12:55.932225: step 39, loss 2.22284, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 39/599 (start=2496, end=2560)\n",
      "2018-11-27T04:12:56.277699: step 40, loss 2.39326, acc 0.3125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 40/599 (start=2560, end=2624)\n",
      "2018-11-27T04:12:56.591142: step 41, loss 2.36129, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 41/599 (start=2624, end=2688)\n",
      "2018-11-27T04:12:56.902461: step 42, loss 2.15134, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 42/599 (start=2688, end=2752)\n",
      "2018-11-27T04:12:57.237479: step 43, loss 2.42421, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 43/599 (start=2752, end=2816)\n",
      "2018-11-27T04:12:57.557384: step 44, loss 2.29554, acc 0.328125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 44/599 (start=2816, end=2880)\n",
      "2018-11-27T04:12:57.867257: step 45, loss 2.21806, acc 0.328125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 45/599 (start=2880, end=2944)\n",
      "2018-11-27T04:12:58.180823: step 46, loss 2.09668, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 46/599 (start=2944, end=3008)\n",
      "2018-11-27T04:12:58.517623: step 47, loss 2.1451, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 47/599 (start=3008, end=3072)\n",
      "2018-11-27T04:12:58.850128: step 48, loss 2.12272, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 48/599 (start=3072, end=3136)\n",
      "2018-11-27T04:12:59.188044: step 49, loss 2.22364, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 49/599 (start=3136, end=3200)\n",
      "2018-11-27T04:12:59.542089: step 50, loss 2.14817, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 50/599 (start=3200, end=3264)\n",
      "2018-11-27T04:12:59.861824: step 51, loss 2.0341, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 51/599 (start=3264, end=3328)\n",
      "2018-11-27T04:13:00.203965: step 52, loss 2.51347, acc 0.3125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 52/599 (start=3328, end=3392)\n",
      "2018-11-27T04:13:00.514381: step 53, loss 2.27391, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 53/599 (start=3392, end=3456)\n",
      "2018-11-27T04:13:00.855413: step 54, loss 2.18697, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 54/599 (start=3456, end=3520)\n",
      "2018-11-27T04:13:01.202469: step 55, loss 2.19012, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 55/599 (start=3520, end=3584)\n",
      "2018-11-27T04:13:01.542069: step 56, loss 2.11132, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 56/599 (start=3584, end=3648)\n",
      "2018-11-27T04:13:01.881106: step 57, loss 2.15027, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 57/599 (start=3648, end=3712)\n",
      "2018-11-27T04:13:02.205517: step 58, loss 2.42522, acc 0.3125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 58/599 (start=3712, end=3776)\n",
      "2018-11-27T04:13:02.565252: step 59, loss 2.48817, acc 0.296875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 59/599 (start=3776, end=3840)\n",
      "2018-11-27T04:13:02.875357: step 60, loss 2.20421, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 60/599 (start=3840, end=3904)\n",
      "2018-11-27T04:13:03.208181: step 61, loss 2.39951, acc 0.328125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 61/599 (start=3904, end=3968)\n",
      "2018-11-27T04:13:03.532416: step 62, loss 2.15719, acc 0.3125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 62/599 (start=3968, end=4032)\n",
      "2018-11-27T04:13:03.860867: step 63, loss 2.38253, acc 0.328125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 63/599 (start=4032, end=4096)\n",
      "2018-11-27T04:13:04.186410: step 64, loss 2.11098, acc 0.328125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 64/599 (start=4096, end=4160)\n",
      "2018-11-27T04:13:04.501915: step 65, loss 2.07228, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 65/599 (start=4160, end=4224)\n",
      "2018-11-27T04:13:04.837604: step 66, loss 2.3625, acc 0.296875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 66/599 (start=4224, end=4288)\n",
      "2018-11-27T04:13:05.174573: step 67, loss 2.46901, acc 0.21875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 67/599 (start=4288, end=4352)\n",
      "2018-11-27T04:13:05.497153: step 68, loss 2.26708, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 68/599 (start=4352, end=4416)\n",
      "2018-11-27T04:13:05.836818: step 69, loss 2.32409, acc 0.328125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 69/599 (start=4416, end=4480)\n",
      "2018-11-27T04:13:06.182423: step 70, loss 2.3974, acc 0.25\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 70/599 (start=4480, end=4544)\n",
      "2018-11-27T04:13:06.515707: step 71, loss 2.30757, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 71/599 (start=4544, end=4608)\n",
      "2018-11-27T04:13:06.845423: step 72, loss 2.35159, acc 0.3125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 72/599 (start=4608, end=4672)\n",
      "2018-11-27T04:13:07.169552: step 73, loss 2.2657, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 73/599 (start=4672, end=4736)\n",
      "2018-11-27T04:13:07.488237: step 74, loss 2.26137, acc 0.3125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 74/599 (start=4736, end=4800)\n",
      "2018-11-27T04:13:07.827142: step 75, loss 2.13237, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 75/599 (start=4800, end=4864)\n",
      "2018-11-27T04:13:08.161895: step 76, loss 2.48422, acc 0.25\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 76/599 (start=4864, end=4928)\n",
      "2018-11-27T04:13:08.480422: step 77, loss 2.01934, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 77/599 (start=4928, end=4992)\n",
      "2018-11-27T04:13:08.809874: step 78, loss 2.16381, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 78/599 (start=4992, end=5056)\n",
      "2018-11-27T04:13:09.140956: step 79, loss 2.14827, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 79/599 (start=5056, end=5120)\n",
      "2018-11-27T04:13:09.478737: step 80, loss 2.1374, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 80/599 (start=5120, end=5184)\n",
      "2018-11-27T04:13:09.786599: step 81, loss 2.49067, acc 0.296875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 81/599 (start=5184, end=5248)\n",
      "2018-11-27T04:13:10.109826: step 82, loss 2.15026, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 82/599 (start=5248, end=5312)\n",
      "2018-11-27T04:13:10.446063: step 83, loss 2.20748, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 83/599 (start=5312, end=5376)\n",
      "2018-11-27T04:13:10.765484: step 84, loss 2.07814, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 84/599 (start=5376, end=5440)\n",
      "2018-11-27T04:13:11.101573: step 85, loss 2.32265, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 85/599 (start=5440, end=5504)\n",
      "2018-11-27T04:13:11.416832: step 86, loss 2.31903, acc 0.328125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 86/599 (start=5504, end=5568)\n",
      "2018-11-27T04:13:11.757298: step 87, loss 2.1761, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 87/599 (start=5568, end=5632)\n",
      "2018-11-27T04:13:12.103799: step 88, loss 2.33591, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 88/599 (start=5632, end=5696)\n",
      "2018-11-27T04:13:12.432233: step 89, loss 2.22618, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 89/599 (start=5696, end=5760)\n",
      "2018-11-27T04:13:12.789092: step 90, loss 1.90264, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 90/599 (start=5760, end=5824)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:13:13.095532: step 91, loss 2.09431, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 91/599 (start=5824, end=5888)\n",
      "2018-11-27T04:13:13.427075: step 92, loss 2.32528, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 92/599 (start=5888, end=5952)\n",
      "2018-11-27T04:13:13.760802: step 93, loss 2.35512, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 93/599 (start=5952, end=6016)\n",
      "2018-11-27T04:13:14.100338: step 94, loss 2.11952, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 94/599 (start=6016, end=6080)\n",
      "2018-11-27T04:13:14.446943: step 95, loss 2.20089, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 95/599 (start=6080, end=6144)\n",
      "2018-11-27T04:13:14.781951: step 96, loss 2.03351, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 96/599 (start=6144, end=6208)\n",
      "2018-11-27T04:13:15.111775: step 97, loss 2.0846, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 97/599 (start=6208, end=6272)\n",
      "2018-11-27T04:13:15.433100: step 98, loss 2.2294, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 98/599 (start=6272, end=6336)\n",
      "2018-11-27T04:13:15.784876: step 99, loss 2.24994, acc 0.328125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 99/599 (start=6336, end=6400)\n",
      "2018-11-27T04:13:16.118216: step 100, loss 2.26761, acc 0.34375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:13:22.447256: step 100, loss 2.09702, acc 0.392681\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-100\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 100/599 (start=6400, end=6464)\n",
      "2018-11-27T04:13:23.171704: step 101, loss 1.97889, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 101/599 (start=6464, end=6528)\n",
      "2018-11-27T04:13:23.508198: step 102, loss 2.12677, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 102/599 (start=6528, end=6592)\n",
      "2018-11-27T04:13:23.852370: step 103, loss 2.27459, acc 0.328125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 103/599 (start=6592, end=6656)\n",
      "2018-11-27T04:13:24.199078: step 104, loss 2.2406, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 104/599 (start=6656, end=6720)\n",
      "2018-11-27T04:13:24.536602: step 105, loss 2.23085, acc 0.328125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 105/599 (start=6720, end=6784)\n",
      "2018-11-27T04:13:24.876169: step 106, loss 2.23542, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 106/599 (start=6784, end=6848)\n",
      "2018-11-27T04:13:25.225615: step 107, loss 2.18381, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 107/599 (start=6848, end=6912)\n",
      "2018-11-27T04:13:25.568466: step 108, loss 2.18711, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 108/599 (start=6912, end=6976)\n",
      "2018-11-27T04:13:25.905409: step 109, loss 2.06373, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 109/599 (start=6976, end=7040)\n",
      "2018-11-27T04:13:26.246000: step 110, loss 1.86211, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 110/599 (start=7040, end=7104)\n",
      "2018-11-27T04:13:26.592590: step 111, loss 2.43607, acc 0.3125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 111/599 (start=7104, end=7168)\n",
      "2018-11-27T04:13:26.907125: step 112, loss 2.1131, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 112/599 (start=7168, end=7232)\n",
      "2018-11-27T04:13:27.257338: step 113, loss 2.00826, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 113/599 (start=7232, end=7296)\n",
      "2018-11-27T04:13:27.574359: step 114, loss 2.21359, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 114/599 (start=7296, end=7360)\n",
      "2018-11-27T04:13:27.902168: step 115, loss 2.30171, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 115/599 (start=7360, end=7424)\n",
      "2018-11-27T04:13:28.231770: step 116, loss 2.21071, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 116/599 (start=7424, end=7488)\n",
      "2018-11-27T04:13:28.547182: step 117, loss 2.32802, acc 0.21875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 117/599 (start=7488, end=7552)\n",
      "2018-11-27T04:13:28.888547: step 118, loss 2.25409, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 118/599 (start=7552, end=7616)\n",
      "2018-11-27T04:13:29.208539: step 119, loss 1.91702, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 119/599 (start=7616, end=7680)\n",
      "2018-11-27T04:13:29.539545: step 120, loss 2.09784, acc 0.3125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 120/599 (start=7680, end=7744)\n",
      "2018-11-27T04:13:29.856174: step 121, loss 2.10617, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 121/599 (start=7744, end=7808)\n",
      "2018-11-27T04:13:30.175537: step 122, loss 2.28027, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 122/599 (start=7808, end=7872)\n",
      "2018-11-27T04:13:30.505088: step 123, loss 2.09639, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 123/599 (start=7872, end=7936)\n",
      "2018-11-27T04:13:30.847312: step 124, loss 2.26297, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 124/599 (start=7936, end=8000)\n",
      "2018-11-27T04:13:31.167941: step 125, loss 2.19819, acc 0.328125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 125/599 (start=8000, end=8064)\n",
      "2018-11-27T04:13:31.497446: step 126, loss 2.06267, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 126/599 (start=8064, end=8128)\n",
      "2018-11-27T04:13:31.808244: step 127, loss 2.06455, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 127/599 (start=8128, end=8192)\n",
      "2018-11-27T04:13:32.158554: step 128, loss 1.96235, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 128/599 (start=8192, end=8256)\n",
      "2018-11-27T04:13:32.483383: step 129, loss 2.33608, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 129/599 (start=8256, end=8320)\n",
      "2018-11-27T04:13:32.797335: step 130, loss 2.1403, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 130/599 (start=8320, end=8384)\n",
      "2018-11-27T04:13:33.115455: step 131, loss 2.13727, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 131/599 (start=8384, end=8448)\n",
      "2018-11-27T04:13:33.465501: step 132, loss 2.17024, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 132/599 (start=8448, end=8512)\n",
      "2018-11-27T04:13:33.785495: step 133, loss 2.3366, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 133/599 (start=8512, end=8576)\n",
      "2018-11-27T04:13:34.098491: step 134, loss 2.27657, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 134/599 (start=8576, end=8640)\n",
      "2018-11-27T04:13:34.419324: step 135, loss 2.16034, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 135/599 (start=8640, end=8704)\n",
      "2018-11-27T04:13:34.764227: step 136, loss 2.17015, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 136/599 (start=8704, end=8768)\n",
      "2018-11-27T04:13:35.085227: step 137, loss 2.36602, acc 0.296875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 137/599 (start=8768, end=8832)\n",
      "2018-11-27T04:13:35.410902: step 138, loss 2.19378, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 138/599 (start=8832, end=8896)\n",
      "2018-11-27T04:13:35.749650: step 139, loss 2.16384, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 139/599 (start=8896, end=8960)\n",
      "2018-11-27T04:13:36.091331: step 140, loss 2.35426, acc 0.328125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 140/599 (start=8960, end=9024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:13:36.397551: step 141, loss 2.37847, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 141/599 (start=9024, end=9088)\n",
      "2018-11-27T04:13:36.703384: step 142, loss 2.19992, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 142/599 (start=9088, end=9152)\n",
      "2018-11-27T04:13:37.043988: step 143, loss 2.14494, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 143/599 (start=9152, end=9216)\n",
      "2018-11-27T04:13:37.387397: step 144, loss 2.22835, acc 0.296875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 144/599 (start=9216, end=9280)\n",
      "2018-11-27T04:13:37.730793: step 145, loss 2.18554, acc 0.265625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 145/599 (start=9280, end=9344)\n",
      "2018-11-27T04:13:38.079155: step 146, loss 2.00409, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 146/599 (start=9344, end=9408)\n",
      "2018-11-27T04:13:38.397528: step 147, loss 2.20286, acc 0.3125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 147/599 (start=9408, end=9472)\n",
      "2018-11-27T04:13:38.712411: step 148, loss 1.98806, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 148/599 (start=9472, end=9536)\n",
      "2018-11-27T04:13:39.043863: step 149, loss 2.21351, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 149/599 (start=9536, end=9600)\n",
      "2018-11-27T04:13:39.366711: step 150, loss 2.0734, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 150/599 (start=9600, end=9664)\n",
      "2018-11-27T04:13:39.692938: step 151, loss 1.93915, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 151/599 (start=9664, end=9728)\n",
      "2018-11-27T04:13:40.023585: step 152, loss 1.95192, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 152/599 (start=9728, end=9792)\n",
      "2018-11-27T04:13:40.354356: step 153, loss 2.04207, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 153/599 (start=9792, end=9856)\n",
      "2018-11-27T04:13:40.695983: step 154, loss 2.09945, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 154/599 (start=9856, end=9920)\n",
      "2018-11-27T04:13:41.033635: step 155, loss 2.36049, acc 0.296875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 155/599 (start=9920, end=9984)\n",
      "2018-11-27T04:13:41.366029: step 156, loss 2.14373, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 156/599 (start=9984, end=10048)\n",
      "2018-11-27T04:13:41.704588: step 157, loss 1.95362, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 157/599 (start=10048, end=10112)\n",
      "2018-11-27T04:13:42.050247: step 158, loss 2.1519, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 158/599 (start=10112, end=10176)\n",
      "2018-11-27T04:13:42.388102: step 159, loss 2.03475, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 159/599 (start=10176, end=10240)\n",
      "2018-11-27T04:13:42.723634: step 160, loss 2.02234, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 160/599 (start=10240, end=10304)\n",
      "2018-11-27T04:13:43.057153: step 161, loss 2.19042, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 161/599 (start=10304, end=10368)\n",
      "2018-11-27T04:13:43.385736: step 162, loss 2.31798, acc 0.3125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 162/599 (start=10368, end=10432)\n",
      "2018-11-27T04:13:43.717399: step 163, loss 2.0681, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 163/599 (start=10432, end=10496)\n",
      "2018-11-27T04:13:44.053067: step 164, loss 2.25593, acc 0.328125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 164/599 (start=10496, end=10560)\n",
      "2018-11-27T04:13:44.387077: step 165, loss 2.08827, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 165/599 (start=10560, end=10624)\n",
      "2018-11-27T04:13:44.716438: step 166, loss 2.16981, acc 0.3125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 166/599 (start=10624, end=10688)\n",
      "2018-11-27T04:13:45.049361: step 167, loss 2.47876, acc 0.28125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 167/599 (start=10688, end=10752)\n",
      "2018-11-27T04:13:45.391956: step 168, loss 2.18397, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 168/599 (start=10752, end=10816)\n",
      "2018-11-27T04:13:45.727291: step 169, loss 2.1111, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 169/599 (start=10816, end=10880)\n",
      "2018-11-27T04:13:46.039789: step 170, loss 1.98404, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 170/599 (start=10880, end=10944)\n",
      "2018-11-27T04:13:46.351012: step 171, loss 2.25428, acc 0.28125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 171/599 (start=10944, end=11008)\n",
      "2018-11-27T04:13:46.689835: step 172, loss 2.23793, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 172/599 (start=11008, end=11072)\n",
      "2018-11-27T04:13:47.011993: step 173, loss 2.12868, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 173/599 (start=11072, end=11136)\n",
      "2018-11-27T04:13:47.338592: step 174, loss 2.05485, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 174/599 (start=11136, end=11200)\n",
      "2018-11-27T04:13:47.656942: step 175, loss 2.02175, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 175/599 (start=11200, end=11264)\n",
      "2018-11-27T04:13:47.961513: step 176, loss 2.1336, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 176/599 (start=11264, end=11328)\n",
      "2018-11-27T04:13:48.275100: step 177, loss 1.95187, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 177/599 (start=11328, end=11392)\n",
      "2018-11-27T04:13:48.603003: step 178, loss 2.1617, acc 0.21875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 178/599 (start=11392, end=11456)\n",
      "2018-11-27T04:13:48.942743: step 179, loss 2.07963, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 179/599 (start=11456, end=11520)\n",
      "2018-11-27T04:13:49.279109: step 180, loss 2.15382, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 180/599 (start=11520, end=11584)\n",
      "2018-11-27T04:13:49.607362: step 181, loss 2.2743, acc 0.3125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 181/599 (start=11584, end=11648)\n",
      "2018-11-27T04:13:49.941535: step 182, loss 2.24573, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 182/599 (start=11648, end=11712)\n",
      "2018-11-27T04:13:50.283272: step 183, loss 2.23628, acc 0.328125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 183/599 (start=11712, end=11776)\n",
      "2018-11-27T04:13:50.625304: step 184, loss 2.11697, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 184/599 (start=11776, end=11840)\n",
      "2018-11-27T04:13:50.965491: step 185, loss 2.1204, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 185/599 (start=11840, end=11904)\n",
      "2018-11-27T04:13:51.319522: step 186, loss 1.87663, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 186/599 (start=11904, end=11968)\n",
      "2018-11-27T04:13:51.654504: step 187, loss 1.78993, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 187/599 (start=11968, end=12032)\n",
      "2018-11-27T04:13:51.991854: step 188, loss 2.03057, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 188/599 (start=12032, end=12096)\n",
      "2018-11-27T04:13:52.303703: step 189, loss 2.14582, acc 0.328125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 189/599 (start=12096, end=12160)\n",
      "2018-11-27T04:13:52.624644: step 190, loss 1.98873, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 190/599 (start=12160, end=12224)\n",
      "2018-11-27T04:13:52.963929: step 191, loss 2.19094, acc 0.328125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 191/599 (start=12224, end=12288)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:13:53.286058: step 192, loss 2.04412, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 192/599 (start=12288, end=12352)\n",
      "2018-11-27T04:13:53.624516: step 193, loss 2.23438, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 193/599 (start=12352, end=12416)\n",
      "2018-11-27T04:13:53.965535: step 194, loss 1.99257, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 194/599 (start=12416, end=12480)\n",
      "2018-11-27T04:13:54.286985: step 195, loss 2.07998, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 195/599 (start=12480, end=12544)\n",
      "2018-11-27T04:13:54.628354: step 196, loss 2.36547, acc 0.3125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 196/599 (start=12544, end=12608)\n",
      "2018-11-27T04:13:54.970780: step 197, loss 2.13416, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 197/599 (start=12608, end=12672)\n",
      "2018-11-27T04:13:55.314839: step 198, loss 1.95176, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 198/599 (start=12672, end=12736)\n",
      "2018-11-27T04:13:55.631047: step 199, loss 2.06518, acc 0.328125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 199/599 (start=12736, end=12800)\n",
      "2018-11-27T04:13:55.956431: step 200, loss 2.19118, acc 0.40625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:14:02.328497: step 200, loss 2.02794, acc 0.395267\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-200\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 200/599 (start=12800, end=12864)\n",
      "2018-11-27T04:14:03.050399: step 201, loss 2.19427, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 201/599 (start=12864, end=12928)\n",
      "2018-11-27T04:14:03.383180: step 202, loss 1.87997, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 202/599 (start=12928, end=12992)\n",
      "2018-11-27T04:14:03.728685: step 203, loss 1.81621, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 203/599 (start=12992, end=13056)\n",
      "2018-11-27T04:14:04.043212: step 204, loss 2.24562, acc 0.328125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 204/599 (start=13056, end=13120)\n",
      "2018-11-27T04:14:04.359030: step 205, loss 2.11838, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 205/599 (start=13120, end=13184)\n",
      "2018-11-27T04:14:04.706956: step 206, loss 1.96883, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 206/599 (start=13184, end=13248)\n",
      "2018-11-27T04:14:05.024582: step 207, loss 1.97115, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 207/599 (start=13248, end=13312)\n",
      "2018-11-27T04:14:05.361325: step 208, loss 2.11122, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 208/599 (start=13312, end=13376)\n",
      "2018-11-27T04:14:05.704185: step 209, loss 2.34751, acc 0.328125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 209/599 (start=13376, end=13440)\n",
      "2018-11-27T04:14:06.025176: step 210, loss 2.06323, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 210/599 (start=13440, end=13504)\n",
      "2018-11-27T04:14:06.348604: step 211, loss 1.82476, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 211/599 (start=13504, end=13568)\n",
      "2018-11-27T04:14:06.680194: step 212, loss 2.01575, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 212/599 (start=13568, end=13632)\n",
      "2018-11-27T04:14:07.009020: step 213, loss 2.07712, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 213/599 (start=13632, end=13696)\n",
      "2018-11-27T04:14:07.329913: step 214, loss 2.30012, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 214/599 (start=13696, end=13760)\n",
      "2018-11-27T04:14:07.677041: step 215, loss 2.05856, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 215/599 (start=13760, end=13824)\n",
      "2018-11-27T04:14:08.020159: step 216, loss 2.11709, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 216/599 (start=13824, end=13888)\n",
      "2018-11-27T04:14:08.331040: step 217, loss 2.03528, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 217/599 (start=13888, end=13952)\n",
      "2018-11-27T04:14:08.651674: step 218, loss 1.86188, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 218/599 (start=13952, end=14016)\n",
      "2018-11-27T04:14:08.990828: step 219, loss 2.18407, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 219/599 (start=14016, end=14080)\n",
      "2018-11-27T04:14:09.311055: step 220, loss 2.1448, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 220/599 (start=14080, end=14144)\n",
      "2018-11-27T04:14:09.659336: step 221, loss 1.9697, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 221/599 (start=14144, end=14208)\n",
      "2018-11-27T04:14:09.995072: step 222, loss 2.08005, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 222/599 (start=14208, end=14272)\n",
      "2018-11-27T04:14:10.307764: step 223, loss 2.04852, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 223/599 (start=14272, end=14336)\n",
      "2018-11-27T04:14:10.657040: step 224, loss 1.96437, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 224/599 (start=14336, end=14400)\n",
      "2018-11-27T04:14:11.001684: step 225, loss 2.1158, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 225/599 (start=14400, end=14464)\n",
      "2018-11-27T04:14:11.334619: step 226, loss 1.93108, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 226/599 (start=14464, end=14528)\n",
      "2018-11-27T04:14:11.671727: step 227, loss 1.80918, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 227/599 (start=14528, end=14592)\n",
      "2018-11-27T04:14:12.004839: step 228, loss 2.12465, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 228/599 (start=14592, end=14656)\n",
      "2018-11-27T04:14:12.340186: step 229, loss 2.07119, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 229/599 (start=14656, end=14720)\n",
      "2018-11-27T04:14:12.657012: step 230, loss 2.0556, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 230/599 (start=14720, end=14784)\n",
      "2018-11-27T04:14:12.990071: step 231, loss 2.14399, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 231/599 (start=14784, end=14848)\n",
      "2018-11-27T04:14:13.320731: step 232, loss 2.1405, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 232/599 (start=14848, end=14912)\n",
      "2018-11-27T04:14:13.670116: step 233, loss 2.15694, acc 0.28125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 233/599 (start=14912, end=14976)\n",
      "2018-11-27T04:14:13.988618: step 234, loss 1.96259, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 234/599 (start=14976, end=15040)\n",
      "2018-11-27T04:14:14.308069: step 235, loss 1.95567, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 235/599 (start=15040, end=15104)\n",
      "2018-11-27T04:14:14.624640: step 236, loss 2.2244, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 236/599 (start=15104, end=15168)\n",
      "2018-11-27T04:14:14.956581: step 237, loss 1.87208, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 237/599 (start=15168, end=15232)\n",
      "2018-11-27T04:14:15.298216: step 238, loss 2.04585, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 238/599 (start=15232, end=15296)\n",
      "2018-11-27T04:14:15.628666: step 239, loss 2.13166, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 239/599 (start=15296, end=15360)\n",
      "2018-11-27T04:14:15.946610: step 240, loss 2.02042, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 240/599 (start=15360, end=15424)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:14:16.289249: step 241, loss 2.17375, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 241/599 (start=15424, end=15488)\n",
      "2018-11-27T04:14:16.604120: step 242, loss 2.00935, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 242/599 (start=15488, end=15552)\n",
      "2018-11-27T04:14:16.950322: step 243, loss 1.98036, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 243/599 (start=15552, end=15616)\n",
      "2018-11-27T04:14:17.282389: step 244, loss 2.12057, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 244/599 (start=15616, end=15680)\n",
      "2018-11-27T04:14:17.604744: step 245, loss 1.97522, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 245/599 (start=15680, end=15744)\n",
      "2018-11-27T04:14:17.925890: step 246, loss 1.97238, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 246/599 (start=15744, end=15808)\n",
      "2018-11-27T04:14:18.267108: step 247, loss 2.04081, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 247/599 (start=15808, end=15872)\n",
      "2018-11-27T04:14:18.606823: step 248, loss 2.16733, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 248/599 (start=15872, end=15936)\n",
      "2018-11-27T04:14:18.955446: step 249, loss 2.15187, acc 0.328125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 249/599 (start=15936, end=16000)\n",
      "2018-11-27T04:14:19.300275: step 250, loss 1.98129, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 250/599 (start=16000, end=16064)\n",
      "2018-11-27T04:14:19.629962: step 251, loss 2.08145, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 251/599 (start=16064, end=16128)\n",
      "2018-11-27T04:14:19.951766: step 252, loss 2.17565, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 252/599 (start=16128, end=16192)\n",
      "2018-11-27T04:14:20.282165: step 253, loss 1.88111, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 253/599 (start=16192, end=16256)\n",
      "2018-11-27T04:14:20.593473: step 254, loss 2.37956, acc 0.296875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 254/599 (start=16256, end=16320)\n",
      "2018-11-27T04:14:20.918355: step 255, loss 1.8101, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 255/599 (start=16320, end=16384)\n",
      "2018-11-27T04:14:21.251750: step 256, loss 2.01945, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 256/599 (start=16384, end=16448)\n",
      "2018-11-27T04:14:21.583353: step 257, loss 2.01464, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 257/599 (start=16448, end=16512)\n",
      "2018-11-27T04:14:21.906645: step 258, loss 2.01237, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 258/599 (start=16512, end=16576)\n",
      "2018-11-27T04:14:22.237506: step 259, loss 2.27351, acc 0.265625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 259/599 (start=16576, end=16640)\n",
      "2018-11-27T04:14:22.583859: step 260, loss 2.11003, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 260/599 (start=16640, end=16704)\n",
      "2018-11-27T04:14:22.921611: step 261, loss 2.17494, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 261/599 (start=16704, end=16768)\n",
      "2018-11-27T04:14:23.232423: step 262, loss 2.10201, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 262/599 (start=16768, end=16832)\n",
      "2018-11-27T04:14:23.578570: step 263, loss 2.21681, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 263/599 (start=16832, end=16896)\n",
      "2018-11-27T04:14:23.921569: step 264, loss 2.03835, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 264/599 (start=16896, end=16960)\n",
      "2018-11-27T04:14:24.258551: step 265, loss 2.21387, acc 0.328125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 265/599 (start=16960, end=17024)\n",
      "2018-11-27T04:14:24.565271: step 266, loss 2.33532, acc 0.28125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 266/599 (start=17024, end=17088)\n",
      "2018-11-27T04:14:24.907229: step 267, loss 2.07396, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 267/599 (start=17088, end=17152)\n",
      "2018-11-27T04:14:25.244310: step 268, loss 2.03126, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 268/599 (start=17152, end=17216)\n",
      "2018-11-27T04:14:25.572537: step 269, loss 2.14444, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 269/599 (start=17216, end=17280)\n",
      "2018-11-27T04:14:25.904901: step 270, loss 2.22844, acc 0.3125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 270/599 (start=17280, end=17344)\n",
      "2018-11-27T04:14:26.249107: step 271, loss 1.95596, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 271/599 (start=17344, end=17408)\n",
      "2018-11-27T04:14:26.588797: step 272, loss 1.92651, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 272/599 (start=17408, end=17472)\n",
      "2018-11-27T04:14:26.901703: step 273, loss 2.12358, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 273/599 (start=17472, end=17536)\n",
      "2018-11-27T04:14:27.235529: step 274, loss 1.9765, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 274/599 (start=17536, end=17600)\n",
      "2018-11-27T04:14:27.557235: step 275, loss 2.05983, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 275/599 (start=17600, end=17664)\n",
      "2018-11-27T04:14:27.888432: step 276, loss 2.08912, acc 0.328125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 276/599 (start=17664, end=17728)\n",
      "2018-11-27T04:14:28.201148: step 277, loss 2.00991, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 277/599 (start=17728, end=17792)\n",
      "2018-11-27T04:14:28.515758: step 278, loss 2.11635, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 278/599 (start=17792, end=17856)\n",
      "2018-11-27T04:14:28.840706: step 279, loss 2.2044, acc 0.296875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 279/599 (start=17856, end=17920)\n",
      "2018-11-27T04:14:29.177780: step 280, loss 2.50517, acc 0.28125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 280/599 (start=17920, end=17984)\n",
      "2018-11-27T04:14:29.498951: step 281, loss 2.06956, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 281/599 (start=17984, end=18048)\n",
      "2018-11-27T04:14:29.835423: step 282, loss 2.13623, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 282/599 (start=18048, end=18112)\n",
      "2018-11-27T04:14:30.155511: step 283, loss 2.10372, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 283/599 (start=18112, end=18176)\n",
      "2018-11-27T04:14:30.466314: step 284, loss 2.28631, acc 0.28125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 284/599 (start=18176, end=18240)\n",
      "2018-11-27T04:14:30.783995: step 285, loss 1.98684, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 285/599 (start=18240, end=18304)\n",
      "2018-11-27T04:14:31.116716: step 286, loss 1.9665, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 286/599 (start=18304, end=18368)\n",
      "2018-11-27T04:14:31.453157: step 287, loss 1.96939, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 287/599 (start=18368, end=18432)\n",
      "2018-11-27T04:14:31.792163: step 288, loss 2.25897, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 288/599 (start=18432, end=18496)\n",
      "2018-11-27T04:14:32.136740: step 289, loss 2.02823, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 289/599 (start=18496, end=18560)\n",
      "2018-11-27T04:14:32.472716: step 290, loss 2.16204, acc 0.296875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 290/599 (start=18560, end=18624)\n",
      "2018-11-27T04:14:32.815742: step 291, loss 1.94325, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 291/599 (start=18624, end=18688)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:14:33.148003: step 292, loss 1.89591, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 292/599 (start=18688, end=18752)\n",
      "2018-11-27T04:14:33.469878: step 293, loss 2.01068, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 293/599 (start=18752, end=18816)\n",
      "2018-11-27T04:14:33.792274: step 294, loss 1.94765, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 294/599 (start=18816, end=18880)\n",
      "2018-11-27T04:14:34.130759: step 295, loss 2.14342, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 295/599 (start=18880, end=18944)\n",
      "2018-11-27T04:14:34.446418: step 296, loss 1.94404, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 296/599 (start=18944, end=19008)\n",
      "2018-11-27T04:14:34.785989: step 297, loss 2.12747, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 297/599 (start=19008, end=19072)\n",
      "2018-11-27T04:14:35.128620: step 298, loss 2.04652, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 298/599 (start=19072, end=19136)\n",
      "2018-11-27T04:14:35.447946: step 299, loss 1.81018, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 299/599 (start=19136, end=19200)\n",
      "2018-11-27T04:14:35.797317: step 300, loss 1.96738, acc 0.390625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:14:42.283328: step 300, loss 1.97139, acc 0.393856\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-300\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 300/599 (start=19200, end=19264)\n",
      "2018-11-27T04:14:42.991011: step 301, loss 2.04454, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 301/599 (start=19264, end=19328)\n",
      "2018-11-27T04:14:43.341627: step 302, loss 2.31006, acc 0.328125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 302/599 (start=19328, end=19392)\n",
      "2018-11-27T04:14:43.666598: step 303, loss 1.88694, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 303/599 (start=19392, end=19456)\n",
      "2018-11-27T04:14:43.997681: step 304, loss 1.99617, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 304/599 (start=19456, end=19520)\n",
      "2018-11-27T04:14:44.344879: step 305, loss 2.14474, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 305/599 (start=19520, end=19584)\n",
      "2018-11-27T04:14:44.689745: step 306, loss 1.96059, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 306/599 (start=19584, end=19648)\n",
      "2018-11-27T04:14:45.019522: step 307, loss 1.90131, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 307/599 (start=19648, end=19712)\n",
      "2018-11-27T04:14:45.359745: step 308, loss 1.94964, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 308/599 (start=19712, end=19776)\n",
      "2018-11-27T04:14:45.685832: step 309, loss 1.82926, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 309/599 (start=19776, end=19840)\n",
      "2018-11-27T04:14:45.998345: step 310, loss 1.92954, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 310/599 (start=19840, end=19904)\n",
      "2018-11-27T04:14:46.314988: step 311, loss 1.94397, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 311/599 (start=19904, end=19968)\n",
      "2018-11-27T04:14:46.654344: step 312, loss 2.20874, acc 0.328125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 312/599 (start=19968, end=20032)\n",
      "2018-11-27T04:14:46.994956: step 313, loss 1.84243, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 313/599 (start=20032, end=20096)\n",
      "2018-11-27T04:14:47.336983: step 314, loss 2.15161, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 314/599 (start=20096, end=20160)\n",
      "2018-11-27T04:14:47.657174: step 315, loss 1.8678, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 315/599 (start=20160, end=20224)\n",
      "2018-11-27T04:14:47.970606: step 316, loss 2.07003, acc 0.3125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 316/599 (start=20224, end=20288)\n",
      "2018-11-27T04:14:48.311892: step 317, loss 2.14363, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 317/599 (start=20288, end=20352)\n",
      "2018-11-27T04:14:48.638930: step 318, loss 1.93669, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 318/599 (start=20352, end=20416)\n",
      "2018-11-27T04:14:48.967835: step 319, loss 1.7935, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 319/599 (start=20416, end=20480)\n",
      "2018-11-27T04:14:49.293988: step 320, loss 2.01332, acc 0.296875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 320/599 (start=20480, end=20544)\n",
      "2018-11-27T04:14:49.626027: step 321, loss 2.05926, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 321/599 (start=20544, end=20608)\n",
      "2018-11-27T04:14:49.935624: step 322, loss 1.77489, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 322/599 (start=20608, end=20672)\n",
      "2018-11-27T04:14:50.278482: step 323, loss 2.18743, acc 0.28125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 323/599 (start=20672, end=20736)\n",
      "2018-11-27T04:14:50.618142: step 324, loss 2.01027, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 324/599 (start=20736, end=20800)\n",
      "2018-11-27T04:14:50.934675: step 325, loss 1.87035, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 325/599 (start=20800, end=20864)\n",
      "2018-11-27T04:14:51.246738: step 326, loss 2.12879, acc 0.328125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 326/599 (start=20864, end=20928)\n",
      "2018-11-27T04:14:51.573795: step 327, loss 2.09512, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 327/599 (start=20928, end=20992)\n",
      "2018-11-27T04:14:51.894465: step 328, loss 2.02708, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 328/599 (start=20992, end=21056)\n",
      "2018-11-27T04:14:52.200839: step 329, loss 1.92936, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 329/599 (start=21056, end=21120)\n",
      "2018-11-27T04:14:52.518794: step 330, loss 2.06935, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 330/599 (start=21120, end=21184)\n",
      "2018-11-27T04:14:52.866638: step 331, loss 2.04257, acc 0.328125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 331/599 (start=21184, end=21248)\n",
      "2018-11-27T04:14:53.205436: step 332, loss 1.89142, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 332/599 (start=21248, end=21312)\n",
      "2018-11-27T04:14:53.545339: step 333, loss 2.13061, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 333/599 (start=21312, end=21376)\n",
      "2018-11-27T04:14:53.878737: step 334, loss 1.83571, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 334/599 (start=21376, end=21440)\n",
      "2018-11-27T04:14:54.190565: step 335, loss 2.00371, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 335/599 (start=21440, end=21504)\n",
      "2018-11-27T04:14:54.497410: step 336, loss 1.69376, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 336/599 (start=21504, end=21568)\n",
      "2018-11-27T04:14:54.828088: step 337, loss 1.8792, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 337/599 (start=21568, end=21632)\n",
      "2018-11-27T04:14:55.161969: step 338, loss 1.79041, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 338/599 (start=21632, end=21696)\n",
      "2018-11-27T04:14:55.475397: step 339, loss 2.12041, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 339/599 (start=21696, end=21760)\n",
      "2018-11-27T04:14:55.795589: step 340, loss 1.94433, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 340/599 (start=21760, end=21824)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:14:56.138027: step 341, loss 2.15968, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 341/599 (start=21824, end=21888)\n",
      "2018-11-27T04:14:56.472331: step 342, loss 1.91682, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 342/599 (start=21888, end=21952)\n",
      "2018-11-27T04:14:56.838083: step 343, loss 1.98495, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 343/599 (start=21952, end=22016)\n",
      "2018-11-27T04:14:57.164802: step 344, loss 1.76338, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 344/599 (start=22016, end=22080)\n",
      "2018-11-27T04:14:57.500263: step 345, loss 2.07129, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 345/599 (start=22080, end=22144)\n",
      "2018-11-27T04:14:57.827157: step 346, loss 2.11501, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 346/599 (start=22144, end=22208)\n",
      "2018-11-27T04:14:58.160240: step 347, loss 2.12358, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 347/599 (start=22208, end=22272)\n",
      "2018-11-27T04:14:58.480759: step 348, loss 2.04087, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 348/599 (start=22272, end=22336)\n",
      "2018-11-27T04:14:58.831639: step 349, loss 1.99833, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 349/599 (start=22336, end=22400)\n",
      "2018-11-27T04:14:59.151082: step 350, loss 1.78034, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 350/599 (start=22400, end=22464)\n",
      "2018-11-27T04:14:59.483376: step 351, loss 2.2697, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 351/599 (start=22464, end=22528)\n",
      "2018-11-27T04:14:59.819780: step 352, loss 1.92912, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 352/599 (start=22528, end=22592)\n",
      "2018-11-27T04:15:00.167401: step 353, loss 1.92327, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 353/599 (start=22592, end=22656)\n",
      "2018-11-27T04:15:00.504784: step 354, loss 2.36427, acc 0.3125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 354/599 (start=22656, end=22720)\n",
      "2018-11-27T04:15:00.823389: step 355, loss 2.16388, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 355/599 (start=22720, end=22784)\n",
      "2018-11-27T04:15:01.158971: step 356, loss 1.94702, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 356/599 (start=22784, end=22848)\n",
      "2018-11-27T04:15:01.503197: step 357, loss 2.01282, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 357/599 (start=22848, end=22912)\n",
      "2018-11-27T04:15:01.829912: step 358, loss 1.92145, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 358/599 (start=22912, end=22976)\n",
      "2018-11-27T04:15:02.171237: step 359, loss 1.98463, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 359/599 (start=22976, end=23040)\n",
      "2018-11-27T04:15:02.499406: step 360, loss 1.83159, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 360/599 (start=23040, end=23104)\n",
      "2018-11-27T04:15:02.849563: step 361, loss 1.9214, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 361/599 (start=23104, end=23168)\n",
      "2018-11-27T04:15:03.181913: step 362, loss 1.82183, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 362/599 (start=23168, end=23232)\n",
      "2018-11-27T04:15:03.520431: step 363, loss 1.94499, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 363/599 (start=23232, end=23296)\n",
      "2018-11-27T04:15:03.865077: step 364, loss 1.87597, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 364/599 (start=23296, end=23360)\n",
      "2018-11-27T04:15:04.172565: step 365, loss 1.94464, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 365/599 (start=23360, end=23424)\n",
      "2018-11-27T04:15:04.484381: step 366, loss 1.93182, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 366/599 (start=23424, end=23488)\n",
      "2018-11-27T04:15:04.816875: step 367, loss 2.23102, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 367/599 (start=23488, end=23552)\n",
      "2018-11-27T04:15:05.158873: step 368, loss 1.88315, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 368/599 (start=23552, end=23616)\n",
      "2018-11-27T04:15:05.469915: step 369, loss 1.94956, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 369/599 (start=23616, end=23680)\n",
      "2018-11-27T04:15:05.779783: step 370, loss 1.94545, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 370/599 (start=23680, end=23744)\n",
      "2018-11-27T04:15:06.113000: step 371, loss 1.8861, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 371/599 (start=23744, end=23808)\n",
      "2018-11-27T04:15:06.459575: step 372, loss 1.89562, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 372/599 (start=23808, end=23872)\n",
      "2018-11-27T04:15:06.795330: step 373, loss 1.85325, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 373/599 (start=23872, end=23936)\n",
      "2018-11-27T04:15:07.125802: step 374, loss 1.70668, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 374/599 (start=23936, end=24000)\n",
      "2018-11-27T04:15:07.466939: step 375, loss 2.02664, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 375/599 (start=24000, end=24064)\n",
      "2018-11-27T04:15:07.801652: step 376, loss 2.11087, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 376/599 (start=24064, end=24128)\n",
      "2018-11-27T04:15:08.109667: step 377, loss 1.92702, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 377/599 (start=24128, end=24192)\n",
      "2018-11-27T04:15:08.436754: step 378, loss 2.01942, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 378/599 (start=24192, end=24256)\n",
      "2018-11-27T04:15:08.754061: step 379, loss 2.08235, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 379/599 (start=24256, end=24320)\n",
      "2018-11-27T04:15:09.058885: step 380, loss 1.90833, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 380/599 (start=24320, end=24384)\n",
      "2018-11-27T04:15:09.390700: step 381, loss 1.84803, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 381/599 (start=24384, end=24448)\n",
      "2018-11-27T04:15:09.727404: step 382, loss 2.15993, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 382/599 (start=24448, end=24512)\n",
      "2018-11-27T04:15:10.069010: step 383, loss 2.21282, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 383/599 (start=24512, end=24576)\n",
      "2018-11-27T04:15:10.394601: step 384, loss 2.24855, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 384/599 (start=24576, end=24640)\n",
      "2018-11-27T04:15:10.729616: step 385, loss 2.14673, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 385/599 (start=24640, end=24704)\n",
      "2018-11-27T04:15:11.050311: step 386, loss 1.99792, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 386/599 (start=24704, end=24768)\n",
      "2018-11-27T04:15:11.389849: step 387, loss 2.00461, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 387/599 (start=24768, end=24832)\n",
      "2018-11-27T04:15:11.700231: step 388, loss 2.02254, acc 0.296875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 388/599 (start=24832, end=24896)\n",
      "2018-11-27T04:15:12.044883: step 389, loss 1.85169, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 389/599 (start=24896, end=24960)\n",
      "2018-11-27T04:15:12.366314: step 390, loss 2.14214, acc 0.28125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 390/599 (start=24960, end=25024)\n",
      "2018-11-27T04:15:12.698559: step 391, loss 2.09782, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 391/599 (start=25024, end=25088)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:15:13.012665: step 392, loss 1.91687, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 392/599 (start=25088, end=25152)\n",
      "2018-11-27T04:15:13.380104: step 393, loss 1.87175, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 393/599 (start=25152, end=25216)\n",
      "2018-11-27T04:15:13.685955: step 394, loss 2.13525, acc 0.3125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 394/599 (start=25216, end=25280)\n",
      "2018-11-27T04:15:14.010481: step 395, loss 1.90162, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 395/599 (start=25280, end=25344)\n",
      "2018-11-27T04:15:14.342997: step 396, loss 1.96031, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 396/599 (start=25344, end=25408)\n",
      "2018-11-27T04:15:14.686683: step 397, loss 1.81969, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 397/599 (start=25408, end=25472)\n",
      "2018-11-27T04:15:15.014074: step 398, loss 2.28534, acc 0.296875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 398/599 (start=25472, end=25536)\n",
      "2018-11-27T04:15:15.344053: step 399, loss 2.19083, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 399/599 (start=25536, end=25600)\n",
      "2018-11-27T04:15:15.687105: step 400, loss 2.0359, acc 0.375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:15:21.792541: step 400, loss 1.9106, acc 0.403182\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-400\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 400/599 (start=25600, end=25664)\n",
      "2018-11-27T04:15:22.500964: step 401, loss 1.82878, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 401/599 (start=25664, end=25728)\n",
      "2018-11-27T04:15:22.831737: step 402, loss 2.03138, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 402/599 (start=25728, end=25792)\n",
      "2018-11-27T04:15:23.146689: step 403, loss 2.046, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 403/599 (start=25792, end=25856)\n",
      "2018-11-27T04:15:23.464338: step 404, loss 1.97872, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 404/599 (start=25856, end=25920)\n",
      "2018-11-27T04:15:23.797961: step 405, loss 1.85101, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 405/599 (start=25920, end=25984)\n",
      "2018-11-27T04:15:24.142753: step 406, loss 2.07589, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 406/599 (start=25984, end=26048)\n",
      "2018-11-27T04:15:24.474017: step 407, loss 2.34179, acc 0.328125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 407/599 (start=26048, end=26112)\n",
      "2018-11-27T04:15:24.819649: step 408, loss 1.97437, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 408/599 (start=26112, end=26176)\n",
      "2018-11-27T04:15:25.151333: step 409, loss 1.74529, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 409/599 (start=26176, end=26240)\n",
      "2018-11-27T04:15:25.501894: step 410, loss 1.76482, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 410/599 (start=26240, end=26304)\n",
      "2018-11-27T04:15:25.833418: step 411, loss 1.84234, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 411/599 (start=26304, end=26368)\n",
      "2018-11-27T04:15:26.137465: step 412, loss 2.10458, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 412/599 (start=26368, end=26432)\n",
      "2018-11-27T04:15:26.452332: step 413, loss 1.91567, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 413/599 (start=26432, end=26496)\n",
      "2018-11-27T04:15:26.791157: step 414, loss 1.91135, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 414/599 (start=26496, end=26560)\n",
      "2018-11-27T04:15:27.123411: step 415, loss 2.20971, acc 0.328125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 415/599 (start=26560, end=26624)\n",
      "2018-11-27T04:15:27.431887: step 416, loss 1.8286, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 416/599 (start=26624, end=26688)\n",
      "2018-11-27T04:15:27.745675: step 417, loss 1.78829, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 417/599 (start=26688, end=26752)\n",
      "2018-11-27T04:15:28.061080: step 418, loss 1.89101, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 418/599 (start=26752, end=26816)\n",
      "2018-11-27T04:15:28.392466: step 419, loss 2.09964, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 419/599 (start=26816, end=26880)\n",
      "2018-11-27T04:15:28.733518: step 420, loss 1.68695, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 420/599 (start=26880, end=26944)\n",
      "2018-11-27T04:15:29.047269: step 421, loss 2.07191, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 421/599 (start=26944, end=27008)\n",
      "2018-11-27T04:15:29.349931: step 422, loss 1.82794, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 422/599 (start=27008, end=27072)\n",
      "2018-11-27T04:15:29.687394: step 423, loss 2.10241, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 423/599 (start=27072, end=27136)\n",
      "2018-11-27T04:15:30.007859: step 424, loss 1.72784, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 424/599 (start=27136, end=27200)\n",
      "2018-11-27T04:15:30.329699: step 425, loss 2.00627, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 425/599 (start=27200, end=27264)\n",
      "2018-11-27T04:15:30.642305: step 426, loss 1.84659, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 426/599 (start=27264, end=27328)\n",
      "2018-11-27T04:15:30.972957: step 427, loss 1.90662, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 427/599 (start=27328, end=27392)\n",
      "2018-11-27T04:15:31.320211: step 428, loss 1.98718, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 428/599 (start=27392, end=27456)\n",
      "2018-11-27T04:15:31.640834: step 429, loss 1.96839, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 429/599 (start=27456, end=27520)\n",
      "2018-11-27T04:15:31.958728: step 430, loss 2.2612, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 430/599 (start=27520, end=27584)\n",
      "2018-11-27T04:15:32.291369: step 431, loss 1.95308, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 431/599 (start=27584, end=27648)\n",
      "2018-11-27T04:15:32.604799: step 432, loss 2.11502, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 432/599 (start=27648, end=27712)\n",
      "2018-11-27T04:15:32.931673: step 433, loss 1.98772, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 433/599 (start=27712, end=27776)\n",
      "2018-11-27T04:15:33.261217: step 434, loss 2.02707, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 434/599 (start=27776, end=27840)\n",
      "2018-11-27T04:15:33.596529: step 435, loss 1.86265, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 435/599 (start=27840, end=27904)\n",
      "2018-11-27T04:15:33.942542: step 436, loss 2.06662, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 436/599 (start=27904, end=27968)\n",
      "2018-11-27T04:15:34.249802: step 437, loss 1.79731, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 437/599 (start=27968, end=28032)\n",
      "2018-11-27T04:15:34.587738: step 438, loss 2.06444, acc 0.3125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 438/599 (start=28032, end=28096)\n",
      "2018-11-27T04:15:34.920509: step 439, loss 2.00676, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 439/599 (start=28096, end=28160)\n",
      "2018-11-27T04:15:35.240283: step 440, loss 1.97151, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 440/599 (start=28160, end=28224)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:15:35.550959: step 441, loss 1.89508, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 441/599 (start=28224, end=28288)\n",
      "2018-11-27T04:15:35.891006: step 442, loss 1.76108, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 442/599 (start=28288, end=28352)\n",
      "2018-11-27T04:15:36.251786: step 443, loss 1.98201, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 443/599 (start=28352, end=28416)\n",
      "2018-11-27T04:15:36.561971: step 444, loss 2.05068, acc 0.328125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 444/599 (start=28416, end=28480)\n",
      "2018-11-27T04:15:36.895967: step 445, loss 1.954, acc 0.296875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 445/599 (start=28480, end=28544)\n",
      "2018-11-27T04:15:37.210784: step 446, loss 1.7815, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 446/599 (start=28544, end=28608)\n",
      "2018-11-27T04:15:37.530604: step 447, loss 1.87209, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 447/599 (start=28608, end=28672)\n",
      "2018-11-27T04:15:37.865271: step 448, loss 1.88817, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 448/599 (start=28672, end=28736)\n",
      "2018-11-27T04:15:38.176103: step 449, loss 1.94031, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 449/599 (start=28736, end=28800)\n",
      "2018-11-27T04:15:38.513667: step 450, loss 1.98151, acc 0.296875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 450/599 (start=28800, end=28864)\n",
      "2018-11-27T04:15:38.868376: step 451, loss 2.2874, acc 0.265625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 451/599 (start=28864, end=28928)\n",
      "2018-11-27T04:15:39.176142: step 452, loss 1.95667, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 452/599 (start=28928, end=28992)\n",
      "2018-11-27T04:15:39.501120: step 453, loss 2.06056, acc 0.328125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 453/599 (start=28992, end=29056)\n",
      "2018-11-27T04:15:39.842581: step 454, loss 2.05754, acc 0.296875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 454/599 (start=29056, end=29120)\n",
      "2018-11-27T04:15:40.175319: step 455, loss 1.95843, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 455/599 (start=29120, end=29184)\n",
      "2018-11-27T04:15:40.481406: step 456, loss 2.05101, acc 0.296875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 456/599 (start=29184, end=29248)\n",
      "2018-11-27T04:15:40.830210: step 457, loss 1.95189, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 457/599 (start=29248, end=29312)\n",
      "2018-11-27T04:15:41.167514: step 458, loss 2.0743, acc 0.25\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 458/599 (start=29312, end=29376)\n",
      "2018-11-27T04:15:41.475873: step 459, loss 2.0531, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 459/599 (start=29376, end=29440)\n",
      "2018-11-27T04:15:41.815417: step 460, loss 1.90432, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 460/599 (start=29440, end=29504)\n",
      "2018-11-27T04:15:42.146040: step 461, loss 2.20133, acc 0.328125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 461/599 (start=29504, end=29568)\n",
      "2018-11-27T04:15:42.458556: step 462, loss 1.95264, acc 0.3125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 462/599 (start=29568, end=29632)\n",
      "2018-11-27T04:15:42.792356: step 463, loss 2.09278, acc 0.265625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 463/599 (start=29632, end=29696)\n",
      "2018-11-27T04:15:43.137139: step 464, loss 1.98197, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 464/599 (start=29696, end=29760)\n",
      "2018-11-27T04:15:43.477974: step 465, loss 2.11318, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 465/599 (start=29760, end=29824)\n",
      "2018-11-27T04:15:43.811462: step 466, loss 2.07791, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 466/599 (start=29824, end=29888)\n",
      "2018-11-27T04:15:44.140429: step 467, loss 1.68967, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 467/599 (start=29888, end=29952)\n",
      "2018-11-27T04:15:44.465458: step 468, loss 1.80641, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 468/599 (start=29952, end=30016)\n",
      "2018-11-27T04:15:44.806920: step 469, loss 1.88925, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 469/599 (start=30016, end=30080)\n",
      "2018-11-27T04:15:45.121516: step 470, loss 2.06658, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 470/599 (start=30080, end=30144)\n",
      "2018-11-27T04:15:45.458376: step 471, loss 1.80396, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 471/599 (start=30144, end=30208)\n",
      "2018-11-27T04:15:45.778011: step 472, loss 2.13346, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 472/599 (start=30208, end=30272)\n",
      "2018-11-27T04:15:46.125972: step 473, loss 1.84216, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 473/599 (start=30272, end=30336)\n",
      "2018-11-27T04:15:46.457964: step 474, loss 2.16964, acc 0.28125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 474/599 (start=30336, end=30400)\n",
      "2018-11-27T04:15:46.779465: step 475, loss 1.99355, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 475/599 (start=30400, end=30464)\n",
      "2018-11-27T04:15:47.118719: step 476, loss 2.1826, acc 0.28125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 476/599 (start=30464, end=30528)\n",
      "2018-11-27T04:15:47.448402: step 477, loss 1.85936, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 477/599 (start=30528, end=30592)\n",
      "2018-11-27T04:15:47.751619: step 478, loss 1.92916, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 478/599 (start=30592, end=30656)\n",
      "2018-11-27T04:15:48.080312: step 479, loss 1.97479, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 479/599 (start=30656, end=30720)\n",
      "2018-11-27T04:15:48.410728: step 480, loss 1.88773, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 480/599 (start=30720, end=30784)\n",
      "2018-11-27T04:15:48.751202: step 481, loss 2.04162, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 481/599 (start=30784, end=30848)\n",
      "2018-11-27T04:15:49.085564: step 482, loss 1.67888, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 482/599 (start=30848, end=30912)\n",
      "2018-11-27T04:15:49.415380: step 483, loss 1.90211, acc 0.328125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 483/599 (start=30912, end=30976)\n",
      "2018-11-27T04:15:49.735486: step 484, loss 1.71244, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 484/599 (start=30976, end=31040)\n",
      "2018-11-27T04:15:50.040809: step 485, loss 1.94598, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 485/599 (start=31040, end=31104)\n",
      "2018-11-27T04:15:50.382584: step 486, loss 2.08916, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 486/599 (start=31104, end=31168)\n",
      "2018-11-27T04:15:50.727800: step 487, loss 1.93742, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 487/599 (start=31168, end=31232)\n",
      "2018-11-27T04:15:51.068134: step 488, loss 2.16606, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 488/599 (start=31232, end=31296)\n",
      "2018-11-27T04:15:51.403255: step 489, loss 1.79907, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 489/599 (start=31296, end=31360)\n",
      "2018-11-27T04:15:51.746577: step 490, loss 1.97992, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 490/599 (start=31360, end=31424)\n",
      "2018-11-27T04:15:52.088162: step 491, loss 2.04497, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 491/599 (start=31424, end=31488)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:15:52.424202: step 492, loss 1.76243, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 492/599 (start=31488, end=31552)\n",
      "2018-11-27T04:15:52.769091: step 493, loss 1.83976, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 493/599 (start=31552, end=31616)\n",
      "2018-11-27T04:15:53.087821: step 494, loss 1.85465, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 494/599 (start=31616, end=31680)\n",
      "2018-11-27T04:15:53.415653: step 495, loss 1.8017, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 495/599 (start=31680, end=31744)\n",
      "2018-11-27T04:15:53.745531: step 496, loss 2.03583, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 496/599 (start=31744, end=31808)\n",
      "2018-11-27T04:15:54.056494: step 497, loss 2.06542, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 497/599 (start=31808, end=31872)\n",
      "2018-11-27T04:15:54.405200: step 498, loss 2.08908, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 498/599 (start=31872, end=31936)\n",
      "2018-11-27T04:15:54.739897: step 499, loss 1.92048, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 499/599 (start=31936, end=32000)\n",
      "2018-11-27T04:15:55.079689: step 500, loss 1.88647, acc 0.46875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:16:01.424184: step 500, loss 1.87453, acc 0.412272\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-500\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 500/599 (start=32000, end=32064)\n",
      "2018-11-27T04:16:02.122172: step 501, loss 1.89157, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 501/599 (start=32064, end=32128)\n",
      "2018-11-27T04:16:02.454749: step 502, loss 1.94199, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 502/599 (start=32128, end=32192)\n",
      "2018-11-27T04:16:02.801223: step 503, loss 1.81861, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 503/599 (start=32192, end=32256)\n",
      "2018-11-27T04:16:03.140049: step 504, loss 2.07438, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 504/599 (start=32256, end=32320)\n",
      "2018-11-27T04:16:03.480647: step 505, loss 1.99021, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 505/599 (start=32320, end=32384)\n",
      "2018-11-27T04:16:03.816951: step 506, loss 1.77474, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 506/599 (start=32384, end=32448)\n",
      "2018-11-27T04:16:04.155416: step 507, loss 1.91207, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 507/599 (start=32448, end=32512)\n",
      "2018-11-27T04:16:04.465089: step 508, loss 1.89148, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 508/599 (start=32512, end=32576)\n",
      "2018-11-27T04:16:04.812323: step 509, loss 1.80413, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 509/599 (start=32576, end=32640)\n",
      "2018-11-27T04:16:05.141333: step 510, loss 2.01985, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 510/599 (start=32640, end=32704)\n",
      "2018-11-27T04:16:05.473044: step 511, loss 1.76819, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 511/599 (start=32704, end=32768)\n",
      "2018-11-27T04:16:05.780802: step 512, loss 2.02782, acc 0.265625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 512/599 (start=32768, end=32832)\n",
      "2018-11-27T04:16:06.112031: step 513, loss 1.91617, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 513/599 (start=32832, end=32896)\n",
      "2018-11-27T04:16:06.448523: step 514, loss 2.02233, acc 0.328125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 514/599 (start=32896, end=32960)\n",
      "2018-11-27T04:16:06.764407: step 515, loss 2.05381, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 515/599 (start=32960, end=33024)\n",
      "2018-11-27T04:16:07.084825: step 516, loss 1.8333, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 516/599 (start=33024, end=33088)\n",
      "2018-11-27T04:16:07.416342: step 517, loss 2.12289, acc 0.234375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 517/599 (start=33088, end=33152)\n",
      "2018-11-27T04:16:07.752584: step 518, loss 1.90816, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 518/599 (start=33152, end=33216)\n",
      "2018-11-27T04:16:08.056086: step 519, loss 1.92423, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 519/599 (start=33216, end=33280)\n",
      "2018-11-27T04:16:08.374865: step 520, loss 1.97041, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 520/599 (start=33280, end=33344)\n",
      "2018-11-27T04:16:08.696600: step 521, loss 1.95219, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 521/599 (start=33344, end=33408)\n",
      "2018-11-27T04:16:09.012159: step 522, loss 1.94227, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 522/599 (start=33408, end=33472)\n",
      "2018-11-27T04:16:09.323415: step 523, loss 2.35413, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 523/599 (start=33472, end=33536)\n",
      "2018-11-27T04:16:09.637584: step 524, loss 2.1867, acc 0.296875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 524/599 (start=33536, end=33600)\n",
      "2018-11-27T04:16:09.981786: step 525, loss 1.87388, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 525/599 (start=33600, end=33664)\n",
      "2018-11-27T04:16:10.312581: step 526, loss 2.08653, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 526/599 (start=33664, end=33728)\n",
      "2018-11-27T04:16:10.650868: step 527, loss 1.89747, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 527/599 (start=33728, end=33792)\n",
      "2018-11-27T04:16:10.999746: step 528, loss 2.00612, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 528/599 (start=33792, end=33856)\n",
      "2018-11-27T04:16:11.331965: step 529, loss 1.81099, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 529/599 (start=33856, end=33920)\n",
      "2018-11-27T04:16:11.681639: step 530, loss 1.96967, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 530/599 (start=33920, end=33984)\n",
      "2018-11-27T04:16:12.002603: step 531, loss 1.83114, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 531/599 (start=33984, end=34048)\n",
      "2018-11-27T04:16:12.349059: step 532, loss 1.77935, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 532/599 (start=34048, end=34112)\n",
      "2018-11-27T04:16:12.685970: step 533, loss 1.86975, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 533/599 (start=34112, end=34176)\n",
      "2018-11-27T04:16:12.999773: step 534, loss 1.8347, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 534/599 (start=34176, end=34240)\n",
      "2018-11-27T04:16:13.340267: step 535, loss 2.02354, acc 0.328125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 535/599 (start=34240, end=34304)\n",
      "2018-11-27T04:16:13.679896: step 536, loss 1.72818, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 536/599 (start=34304, end=34368)\n",
      "2018-11-27T04:16:13.989066: step 537, loss 1.73746, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 537/599 (start=34368, end=34432)\n",
      "2018-11-27T04:16:14.314342: step 538, loss 2.01726, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 538/599 (start=34432, end=34496)\n",
      "2018-11-27T04:16:14.647617: step 539, loss 1.71519, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 539/599 (start=34496, end=34560)\n",
      "2018-11-27T04:16:14.985884: step 540, loss 1.68303, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 540/599 (start=34560, end=34624)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:16:15.325636: step 541, loss 2.25739, acc 0.328125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 541/599 (start=34624, end=34688)\n",
      "2018-11-27T04:16:15.659529: step 542, loss 2.01644, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 542/599 (start=34688, end=34752)\n",
      "2018-11-27T04:16:15.998564: step 543, loss 1.94436, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 543/599 (start=34752, end=34816)\n",
      "2018-11-27T04:16:16.320622: step 544, loss 1.72613, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 544/599 (start=34816, end=34880)\n",
      "2018-11-27T04:16:16.632011: step 545, loss 2.02568, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 545/599 (start=34880, end=34944)\n",
      "2018-11-27T04:16:16.969328: step 546, loss 2.14737, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 546/599 (start=34944, end=35008)\n",
      "2018-11-27T04:16:17.319674: step 547, loss 2.04212, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 547/599 (start=35008, end=35072)\n",
      "2018-11-27T04:16:17.627882: step 548, loss 1.88425, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 548/599 (start=35072, end=35136)\n",
      "2018-11-27T04:16:17.972077: step 549, loss 1.94283, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 549/599 (start=35136, end=35200)\n",
      "2018-11-27T04:16:18.280015: step 550, loss 1.63775, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 550/599 (start=35200, end=35264)\n",
      "2018-11-27T04:16:18.617558: step 551, loss 2.16128, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 551/599 (start=35264, end=35328)\n",
      "2018-11-27T04:16:18.956527: step 552, loss 1.98773, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 552/599 (start=35328, end=35392)\n",
      "2018-11-27T04:16:19.293875: step 553, loss 1.80162, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 553/599 (start=35392, end=35456)\n",
      "2018-11-27T04:16:19.619069: step 554, loss 1.94893, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 554/599 (start=35456, end=35520)\n",
      "2018-11-27T04:16:19.959436: step 555, loss 1.60865, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 555/599 (start=35520, end=35584)\n",
      "2018-11-27T04:16:20.273463: step 556, loss 2.06342, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 556/599 (start=35584, end=35648)\n",
      "2018-11-27T04:16:20.588428: step 557, loss 1.79729, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 557/599 (start=35648, end=35712)\n",
      "2018-11-27T04:16:20.911632: step 558, loss 2.12721, acc 0.28125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 558/599 (start=35712, end=35776)\n",
      "2018-11-27T04:16:21.213033: step 559, loss 2.05018, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 559/599 (start=35776, end=35840)\n",
      "2018-11-27T04:16:21.531523: step 560, loss 1.90706, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 560/599 (start=35840, end=35904)\n",
      "2018-11-27T04:16:21.864243: step 561, loss 2.01992, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 561/599 (start=35904, end=35968)\n",
      "2018-11-27T04:16:22.200332: step 562, loss 2.18527, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 562/599 (start=35968, end=36032)\n",
      "2018-11-27T04:16:22.540261: step 563, loss 1.86009, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 563/599 (start=36032, end=36096)\n",
      "2018-11-27T04:16:22.882457: step 564, loss 1.94393, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 564/599 (start=36096, end=36160)\n",
      "2018-11-27T04:16:23.220651: step 565, loss 1.86173, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 565/599 (start=36160, end=36224)\n",
      "2018-11-27T04:16:23.561835: step 566, loss 1.88579, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 566/599 (start=36224, end=36288)\n",
      "2018-11-27T04:16:23.891406: step 567, loss 1.9015, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 567/599 (start=36288, end=36352)\n",
      "2018-11-27T04:16:24.223842: step 568, loss 1.95084, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 568/599 (start=36352, end=36416)\n",
      "2018-11-27T04:16:24.542714: step 569, loss 2.05707, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 569/599 (start=36416, end=36480)\n",
      "2018-11-27T04:16:24.870281: step 570, loss 1.93989, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 570/599 (start=36480, end=36544)\n",
      "2018-11-27T04:16:25.210150: step 571, loss 2.19037, acc 0.328125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 571/599 (start=36544, end=36608)\n",
      "2018-11-27T04:16:25.550400: step 572, loss 1.82505, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 572/599 (start=36608, end=36672)\n",
      "2018-11-27T04:16:25.864452: step 573, loss 1.74171, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 573/599 (start=36672, end=36736)\n",
      "2018-11-27T04:16:26.189512: step 574, loss 1.81206, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 574/599 (start=36736, end=36800)\n",
      "2018-11-27T04:16:26.510686: step 575, loss 1.83938, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 575/599 (start=36800, end=36864)\n",
      "2018-11-27T04:16:26.836830: step 576, loss 1.80802, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 576/599 (start=36864, end=36928)\n",
      "2018-11-27T04:16:27.172422: step 577, loss 1.88158, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 577/599 (start=36928, end=36992)\n",
      "2018-11-27T04:16:27.487381: step 578, loss 2.02132, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 578/599 (start=36992, end=37056)\n",
      "2018-11-27T04:16:27.803707: step 579, loss 1.99444, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 579/599 (start=37056, end=37120)\n",
      "2018-11-27T04:16:28.135660: step 580, loss 2.20395, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 580/599 (start=37120, end=37184)\n",
      "2018-11-27T04:16:28.451899: step 581, loss 1.99456, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 581/599 (start=37184, end=37248)\n",
      "2018-11-27T04:16:28.800129: step 582, loss 1.97772, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 582/599 (start=37248, end=37312)\n",
      "2018-11-27T04:16:29.133701: step 583, loss 1.75482, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 583/599 (start=37312, end=37376)\n",
      "2018-11-27T04:16:29.448352: step 584, loss 1.95945, acc 0.296875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 584/599 (start=37376, end=37440)\n",
      "2018-11-27T04:16:29.788272: step 585, loss 1.98553, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 585/599 (start=37440, end=37504)\n",
      "2018-11-27T04:16:30.109728: step 586, loss 2.00705, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 586/599 (start=37504, end=37568)\n",
      "2018-11-27T04:16:30.446398: step 587, loss 1.92735, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 587/599 (start=37568, end=37632)\n",
      "2018-11-27T04:16:30.775448: step 588, loss 1.69232, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 588/599 (start=37632, end=37696)\n",
      "2018-11-27T04:16:31.103080: step 589, loss 1.81724, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 589/599 (start=37696, end=37760)\n",
      "2018-11-27T04:16:31.447785: step 590, loss 2.00734, acc 0.25\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 590/599 (start=37760, end=37824)\n",
      "2018-11-27T04:16:31.763836: step 591, loss 1.88224, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 591/599 (start=37824, end=37888)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:16:32.097587: step 592, loss 1.59793, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 592/599 (start=37888, end=37952)\n",
      "2018-11-27T04:16:32.439392: step 593, loss 1.86181, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 593/599 (start=37952, end=38016)\n",
      "2018-11-27T04:16:32.783949: step 594, loss 1.81999, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 594/599 (start=38016, end=38080)\n",
      "2018-11-27T04:16:33.111901: step 595, loss 1.75643, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 595/599 (start=38080, end=38144)\n",
      "2018-11-27T04:16:33.437021: step 596, loss 1.8474, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 596/599 (start=38144, end=38208)\n",
      "2018-11-27T04:16:33.757912: step 597, loss 2.03262, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 597/599 (start=38208, end=38272)\n",
      "2018-11-27T04:16:34.092249: step 598, loss 1.88082, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 0/10, Batch 598/599 (start=38272, end=38281)\n",
      "2018-11-27T04:16:34.325722: step 599, loss 2.29148, acc 0\n",
      "***********************************************\n",
      "Epoch 1/10\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 0/599 (start=0, end=64)\n",
      "2018-11-27T04:16:34.659388: step 600, loss 1.79356, acc 0.390625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:16:41.047530: step 600, loss 1.84607, acc 0.430609\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-600\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 1/599 (start=64, end=128)\n",
      "2018-11-27T04:16:41.791083: step 601, loss 1.76757, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 2/599 (start=128, end=192)\n",
      "2018-11-27T04:16:42.118956: step 602, loss 1.54197, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 3/599 (start=192, end=256)\n",
      "2018-11-27T04:16:42.467630: step 603, loss 1.92629, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 4/599 (start=256, end=320)\n",
      "2018-11-27T04:16:42.793631: step 604, loss 1.75636, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 5/599 (start=320, end=384)\n",
      "2018-11-27T04:16:43.116587: step 605, loss 1.70255, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 6/599 (start=384, end=448)\n",
      "2018-11-27T04:16:43.457092: step 606, loss 1.86424, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 7/599 (start=448, end=512)\n",
      "2018-11-27T04:16:43.808596: step 607, loss 1.83278, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 8/599 (start=512, end=576)\n",
      "2018-11-27T04:16:44.121591: step 608, loss 1.86977, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 9/599 (start=576, end=640)\n",
      "2018-11-27T04:16:44.453379: step 609, loss 1.59665, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 10/599 (start=640, end=704)\n",
      "2018-11-27T04:16:44.771457: step 610, loss 1.60468, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 11/599 (start=704, end=768)\n",
      "2018-11-27T04:16:45.079811: step 611, loss 1.79247, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 12/599 (start=768, end=832)\n",
      "2018-11-27T04:16:45.406507: step 612, loss 1.77637, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 13/599 (start=832, end=896)\n",
      "2018-11-27T04:16:45.732320: step 613, loss 1.73436, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 14/599 (start=896, end=960)\n",
      "2018-11-27T04:16:46.083975: step 614, loss 1.55239, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 15/599 (start=960, end=1024)\n",
      "2018-11-27T04:16:46.419257: step 615, loss 1.80097, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 16/599 (start=1024, end=1088)\n",
      "2018-11-27T04:16:46.757950: step 616, loss 1.87714, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 17/599 (start=1088, end=1152)\n",
      "2018-11-27T04:16:47.065336: step 617, loss 1.5261, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 18/599 (start=1152, end=1216)\n",
      "2018-11-27T04:16:47.402929: step 618, loss 1.60864, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 19/599 (start=1216, end=1280)\n",
      "2018-11-27T04:16:47.727254: step 619, loss 1.92455, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 20/599 (start=1280, end=1344)\n",
      "2018-11-27T04:16:48.051660: step 620, loss 1.7729, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 21/599 (start=1344, end=1408)\n",
      "2018-11-27T04:16:48.389058: step 621, loss 1.80376, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 22/599 (start=1408, end=1472)\n",
      "2018-11-27T04:16:48.699338: step 622, loss 1.58301, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 23/599 (start=1472, end=1536)\n",
      "2018-11-27T04:16:49.028736: step 623, loss 1.84735, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 24/599 (start=1536, end=1600)\n",
      "2018-11-27T04:16:49.379994: step 624, loss 1.8292, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 25/599 (start=1600, end=1664)\n",
      "2018-11-27T04:16:49.721961: step 625, loss 1.7563, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 26/599 (start=1664, end=1728)\n",
      "2018-11-27T04:16:50.060867: step 626, loss 1.81701, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 27/599 (start=1728, end=1792)\n",
      "2018-11-27T04:16:50.400417: step 627, loss 1.73353, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 28/599 (start=1792, end=1856)\n",
      "2018-11-27T04:16:50.742708: step 628, loss 1.78944, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 29/599 (start=1856, end=1920)\n",
      "2018-11-27T04:16:51.085608: step 629, loss 1.964, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 30/599 (start=1920, end=1984)\n",
      "2018-11-27T04:16:51.389309: step 630, loss 1.90382, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 31/599 (start=1984, end=2048)\n",
      "2018-11-27T04:16:51.722137: step 631, loss 1.762, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 32/599 (start=2048, end=2112)\n",
      "2018-11-27T04:16:52.044136: step 632, loss 1.63785, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 33/599 (start=2112, end=2176)\n",
      "2018-11-27T04:16:52.375619: step 633, loss 1.71882, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 34/599 (start=2176, end=2240)\n",
      "2018-11-27T04:16:52.682309: step 634, loss 1.76721, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 35/599 (start=2240, end=2304)\n",
      "2018-11-27T04:16:53.025392: step 635, loss 1.55909, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 36/599 (start=2304, end=2368)\n",
      "2018-11-27T04:16:53.353300: step 636, loss 1.69153, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 37/599 (start=2368, end=2432)\n",
      "2018-11-27T04:16:53.683551: step 637, loss 1.74403, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 38/599 (start=2432, end=2496)\n",
      "2018-11-27T04:16:54.032415: step 638, loss 1.79384, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 39/599 (start=2496, end=2560)\n",
      "2018-11-27T04:16:54.370556: step 639, loss 1.63913, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 40/599 (start=2560, end=2624)\n",
      "2018-11-27T04:16:54.697492: step 640, loss 1.8141, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 41/599 (start=2624, end=2688)\n",
      "2018-11-27T04:16:55.007415: step 641, loss 1.728, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 42/599 (start=2688, end=2752)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:16:55.335964: step 642, loss 1.78979, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 43/599 (start=2752, end=2816)\n",
      "2018-11-27T04:16:55.679682: step 643, loss 1.56876, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 44/599 (start=2816, end=2880)\n",
      "2018-11-27T04:16:55.989794: step 644, loss 1.67984, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 45/599 (start=2880, end=2944)\n",
      "2018-11-27T04:16:56.306096: step 645, loss 1.92228, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 46/599 (start=2944, end=3008)\n",
      "2018-11-27T04:16:56.650248: step 646, loss 1.66043, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 47/599 (start=3008, end=3072)\n",
      "2018-11-27T04:16:56.990992: step 647, loss 1.72786, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 48/599 (start=3072, end=3136)\n",
      "2018-11-27T04:16:57.323959: step 648, loss 1.56974, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 49/599 (start=3136, end=3200)\n",
      "2018-11-27T04:16:57.672148: step 649, loss 1.68496, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 50/599 (start=3200, end=3264)\n",
      "2018-11-27T04:16:58.015110: step 650, loss 1.65207, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 51/599 (start=3264, end=3328)\n",
      "2018-11-27T04:16:58.333506: step 651, loss 1.75466, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 52/599 (start=3328, end=3392)\n",
      "2018-11-27T04:16:58.667015: step 652, loss 1.60826, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 53/599 (start=3392, end=3456)\n",
      "2018-11-27T04:16:59.015734: step 653, loss 1.81496, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 54/599 (start=3456, end=3520)\n",
      "2018-11-27T04:16:59.346425: step 654, loss 1.61858, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 55/599 (start=3520, end=3584)\n",
      "2018-11-27T04:16:59.685934: step 655, loss 1.55382, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 56/599 (start=3584, end=3648)\n",
      "2018-11-27T04:17:00.028018: step 656, loss 1.53474, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 57/599 (start=3648, end=3712)\n",
      "2018-11-27T04:17:00.390133: step 657, loss 1.58444, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 58/599 (start=3712, end=3776)\n",
      "2018-11-27T04:17:00.721130: step 658, loss 1.63952, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 59/599 (start=3776, end=3840)\n",
      "2018-11-27T04:17:01.056311: step 659, loss 1.5409, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 60/599 (start=3840, end=3904)\n",
      "2018-11-27T04:17:01.395458: step 660, loss 2.03218, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 61/599 (start=3904, end=3968)\n",
      "2018-11-27T04:17:01.736083: step 661, loss 1.78631, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 62/599 (start=3968, end=4032)\n",
      "2018-11-27T04:17:02.065285: step 662, loss 1.79915, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 63/599 (start=4032, end=4096)\n",
      "2018-11-27T04:17:02.400232: step 663, loss 1.62824, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 64/599 (start=4096, end=4160)\n",
      "2018-11-27T04:17:02.728676: step 664, loss 1.59615, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 65/599 (start=4160, end=4224)\n",
      "2018-11-27T04:17:03.065600: step 665, loss 1.56988, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 66/599 (start=4224, end=4288)\n",
      "2018-11-27T04:17:03.379773: step 666, loss 1.85353, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 67/599 (start=4288, end=4352)\n",
      "2018-11-27T04:17:03.736247: step 667, loss 1.79838, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 68/599 (start=4352, end=4416)\n",
      "2018-11-27T04:17:04.058173: step 668, loss 1.75497, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 69/599 (start=4416, end=4480)\n",
      "2018-11-27T04:17:04.385296: step 669, loss 1.78164, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 70/599 (start=4480, end=4544)\n",
      "2018-11-27T04:17:04.694928: step 670, loss 1.69792, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 71/599 (start=4544, end=4608)\n",
      "2018-11-27T04:17:05.016209: step 671, loss 1.85704, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 72/599 (start=4608, end=4672)\n",
      "2018-11-27T04:17:05.332031: step 672, loss 1.52781, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 73/599 (start=4672, end=4736)\n",
      "2018-11-27T04:17:05.644875: step 673, loss 1.56247, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 74/599 (start=4736, end=4800)\n",
      "2018-11-27T04:17:05.980443: step 674, loss 1.84339, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 75/599 (start=4800, end=4864)\n",
      "2018-11-27T04:17:06.314796: step 675, loss 1.72059, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 76/599 (start=4864, end=4928)\n",
      "2018-11-27T04:17:06.631620: step 676, loss 1.82649, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 77/599 (start=4928, end=4992)\n",
      "2018-11-27T04:17:06.965304: step 677, loss 1.84613, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 78/599 (start=4992, end=5056)\n",
      "2018-11-27T04:17:07.308711: step 678, loss 1.49561, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 79/599 (start=5056, end=5120)\n",
      "2018-11-27T04:17:07.655632: step 679, loss 1.55261, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 80/599 (start=5120, end=5184)\n",
      "2018-11-27T04:17:07.984411: step 680, loss 1.6975, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 81/599 (start=5184, end=5248)\n",
      "2018-11-27T04:17:08.324414: step 681, loss 1.8591, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 82/599 (start=5248, end=5312)\n",
      "2018-11-27T04:17:08.671149: step 682, loss 1.90062, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 83/599 (start=5312, end=5376)\n",
      "2018-11-27T04:17:08.992266: step 683, loss 1.79275, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 84/599 (start=5376, end=5440)\n",
      "2018-11-27T04:17:09.338662: step 684, loss 1.88068, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 85/599 (start=5440, end=5504)\n",
      "2018-11-27T04:17:09.683087: step 685, loss 1.56235, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 86/599 (start=5504, end=5568)\n",
      "2018-11-27T04:17:10.007512: step 686, loss 1.84577, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 87/599 (start=5568, end=5632)\n",
      "2018-11-27T04:17:10.327916: step 687, loss 1.62791, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 88/599 (start=5632, end=5696)\n",
      "2018-11-27T04:17:10.672798: step 688, loss 1.67125, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 89/599 (start=5696, end=5760)\n",
      "2018-11-27T04:17:10.988676: step 689, loss 1.54971, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 90/599 (start=5760, end=5824)\n",
      "2018-11-27T04:17:11.329146: step 690, loss 1.64113, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 91/599 (start=5824, end=5888)\n",
      "2018-11-27T04:17:11.650694: step 691, loss 1.66785, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 92/599 (start=5888, end=5952)\n",
      "2018-11-27T04:17:11.988843: step 692, loss 1.78019, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 93/599 (start=5952, end=6016)\n",
      "2018-11-27T04:17:12.325924: step 693, loss 1.70163, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 94/599 (start=6016, end=6080)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:17:12.663415: step 694, loss 1.67378, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 95/599 (start=6080, end=6144)\n",
      "2018-11-27T04:17:13.008799: step 695, loss 1.73505, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 96/599 (start=6144, end=6208)\n",
      "2018-11-27T04:17:13.356789: step 696, loss 1.67333, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 97/599 (start=6208, end=6272)\n",
      "2018-11-27T04:17:13.675118: step 697, loss 1.70264, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 98/599 (start=6272, end=6336)\n",
      "2018-11-27T04:17:14.033884: step 698, loss 1.77353, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 99/599 (start=6336, end=6400)\n",
      "2018-11-27T04:17:14.370924: step 699, loss 1.98508, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 100/599 (start=6400, end=6464)\n",
      "2018-11-27T04:17:14.705063: step 700, loss 1.44931, acc 0.578125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:17:20.869291: step 700, loss 1.81905, acc 0.438053\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-700\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 101/599 (start=6464, end=6528)\n",
      "2018-11-27T04:17:21.599969: step 701, loss 1.90425, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 102/599 (start=6528, end=6592)\n",
      "2018-11-27T04:17:21.922423: step 702, loss 1.61464, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 103/599 (start=6592, end=6656)\n",
      "2018-11-27T04:17:22.268512: step 703, loss 1.9077, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 104/599 (start=6656, end=6720)\n",
      "2018-11-27T04:17:22.603933: step 704, loss 1.5763, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 105/599 (start=6720, end=6784)\n",
      "2018-11-27T04:17:22.937046: step 705, loss 1.73682, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 106/599 (start=6784, end=6848)\n",
      "2018-11-27T04:17:23.274829: step 706, loss 1.63658, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 107/599 (start=6848, end=6912)\n",
      "2018-11-27T04:17:23.587679: step 707, loss 1.45979, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 108/599 (start=6912, end=6976)\n",
      "2018-11-27T04:17:23.893328: step 708, loss 1.44894, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 109/599 (start=6976, end=7040)\n",
      "2018-11-27T04:17:24.209195: step 709, loss 1.56435, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 110/599 (start=7040, end=7104)\n",
      "2018-11-27T04:17:24.591918: step 710, loss 1.6595, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 111/599 (start=7104, end=7168)\n",
      "2018-11-27T04:17:24.936673: step 711, loss 1.49996, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 112/599 (start=7168, end=7232)\n",
      "2018-11-27T04:17:25.278955: step 712, loss 1.87074, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 113/599 (start=7232, end=7296)\n",
      "2018-11-27T04:17:25.591601: step 713, loss 1.70188, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 114/599 (start=7296, end=7360)\n",
      "2018-11-27T04:17:25.920152: step 714, loss 1.64706, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 115/599 (start=7360, end=7424)\n",
      "2018-11-27T04:17:26.240052: step 715, loss 1.59985, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 116/599 (start=7424, end=7488)\n",
      "2018-11-27T04:17:26.585842: step 716, loss 1.91466, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 117/599 (start=7488, end=7552)\n",
      "2018-11-27T04:17:26.941151: step 717, loss 1.96502, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 118/599 (start=7552, end=7616)\n",
      "2018-11-27T04:17:27.253093: step 718, loss 1.88973, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 119/599 (start=7616, end=7680)\n",
      "2018-11-27T04:17:27.585303: step 719, loss 1.7478, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 120/599 (start=7680, end=7744)\n",
      "2018-11-27T04:17:27.934809: step 720, loss 1.39374, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 121/599 (start=7744, end=7808)\n",
      "2018-11-27T04:17:28.290693: step 721, loss 1.53828, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 122/599 (start=7808, end=7872)\n",
      "2018-11-27T04:17:28.626910: step 722, loss 1.57299, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 123/599 (start=7872, end=7936)\n",
      "2018-11-27T04:17:28.950327: step 723, loss 1.76971, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 124/599 (start=7936, end=8000)\n",
      "2018-11-27T04:17:29.267180: step 724, loss 2.09547, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 125/599 (start=8000, end=8064)\n",
      "2018-11-27T04:17:29.589206: step 725, loss 1.72984, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 126/599 (start=8064, end=8128)\n",
      "2018-11-27T04:17:29.927244: step 726, loss 1.816, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 127/599 (start=8128, end=8192)\n",
      "2018-11-27T04:17:30.283423: step 727, loss 1.66101, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 128/599 (start=8192, end=8256)\n",
      "2018-11-27T04:17:30.617423: step 728, loss 1.82496, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 129/599 (start=8256, end=8320)\n",
      "2018-11-27T04:17:30.958750: step 729, loss 1.4267, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 130/599 (start=8320, end=8384)\n",
      "2018-11-27T04:17:31.305514: step 730, loss 1.67138, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 131/599 (start=8384, end=8448)\n",
      "2018-11-27T04:17:31.635429: step 731, loss 1.88309, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 132/599 (start=8448, end=8512)\n",
      "2018-11-27T04:17:31.947941: step 732, loss 1.94218, acc 0.328125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 133/599 (start=8512, end=8576)\n",
      "2018-11-27T04:17:32.270424: step 733, loss 2.31467, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 134/599 (start=8576, end=8640)\n",
      "2018-11-27T04:17:32.600951: step 734, loss 1.67453, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 135/599 (start=8640, end=8704)\n",
      "2018-11-27T04:17:32.924097: step 735, loss 1.66901, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 136/599 (start=8704, end=8768)\n",
      "2018-11-27T04:17:33.264906: step 736, loss 1.67237, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 137/599 (start=8768, end=8832)\n",
      "2018-11-27T04:17:33.598834: step 737, loss 1.73091, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 138/599 (start=8832, end=8896)\n",
      "2018-11-27T04:17:33.941936: step 738, loss 1.91451, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 139/599 (start=8896, end=8960)\n",
      "2018-11-27T04:17:34.281742: step 739, loss 1.69075, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 140/599 (start=8960, end=9024)\n",
      "2018-11-27T04:17:34.587861: step 740, loss 1.7113, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 141/599 (start=9024, end=9088)\n",
      "2018-11-27T04:17:34.932611: step 741, loss 1.48311, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 142/599 (start=9088, end=9152)\n",
      "2018-11-27T04:17:35.270128: step 742, loss 1.6081, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 143/599 (start=9152, end=9216)\n",
      "2018-11-27T04:17:35.603790: step 743, loss 1.52469, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 144/599 (start=9216, end=9280)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:17:35.930647: step 744, loss 2.01807, acc 0.328125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 145/599 (start=9280, end=9344)\n",
      "2018-11-27T04:17:36.272184: step 745, loss 1.64304, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 146/599 (start=9344, end=9408)\n",
      "2018-11-27T04:17:36.623453: step 746, loss 1.58667, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 147/599 (start=9408, end=9472)\n",
      "2018-11-27T04:17:36.939407: step 747, loss 1.57978, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 148/599 (start=9472, end=9536)\n",
      "2018-11-27T04:17:37.262047: step 748, loss 1.73537, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 149/599 (start=9536, end=9600)\n",
      "2018-11-27T04:17:37.586888: step 749, loss 1.41901, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 150/599 (start=9600, end=9664)\n",
      "2018-11-27T04:17:37.900840: step 750, loss 1.75955, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 151/599 (start=9664, end=9728)\n",
      "2018-11-27T04:17:38.239225: step 751, loss 1.6784, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 152/599 (start=9728, end=9792)\n",
      "2018-11-27T04:17:38.607679: step 752, loss 1.60516, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 153/599 (start=9792, end=9856)\n",
      "2018-11-27T04:17:38.921602: step 753, loss 1.71319, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 154/599 (start=9856, end=9920)\n",
      "2018-11-27T04:17:39.242416: step 754, loss 1.64031, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 155/599 (start=9920, end=9984)\n",
      "2018-11-27T04:17:39.571548: step 755, loss 1.57638, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 156/599 (start=9984, end=10048)\n",
      "2018-11-27T04:17:39.924726: step 756, loss 1.68745, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 157/599 (start=10048, end=10112)\n",
      "2018-11-27T04:17:40.259187: step 757, loss 1.68132, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 158/599 (start=10112, end=10176)\n",
      "2018-11-27T04:17:40.591330: step 758, loss 1.69974, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 159/599 (start=10176, end=10240)\n",
      "2018-11-27T04:17:40.934935: step 759, loss 2.01368, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 160/599 (start=10240, end=10304)\n",
      "2018-11-27T04:17:41.262699: step 760, loss 1.85755, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 161/599 (start=10304, end=10368)\n",
      "2018-11-27T04:17:41.579906: step 761, loss 1.67726, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 162/599 (start=10368, end=10432)\n",
      "2018-11-27T04:17:41.915083: step 762, loss 1.54913, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 163/599 (start=10432, end=10496)\n",
      "2018-11-27T04:17:42.254822: step 763, loss 1.86236, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 164/599 (start=10496, end=10560)\n",
      "2018-11-27T04:17:42.600410: step 764, loss 1.74092, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 165/599 (start=10560, end=10624)\n",
      "2018-11-27T04:17:42.948456: step 765, loss 1.86432, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 166/599 (start=10624, end=10688)\n",
      "2018-11-27T04:17:43.269556: step 766, loss 1.5834, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 167/599 (start=10688, end=10752)\n",
      "2018-11-27T04:17:43.598657: step 767, loss 1.89317, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 168/599 (start=10752, end=10816)\n",
      "2018-11-27T04:17:43.912998: step 768, loss 1.57072, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 169/599 (start=10816, end=10880)\n",
      "2018-11-27T04:17:44.246145: step 769, loss 1.58827, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 170/599 (start=10880, end=10944)\n",
      "2018-11-27T04:17:44.569191: step 770, loss 1.75121, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 171/599 (start=10944, end=11008)\n",
      "2018-11-27T04:17:44.888738: step 771, loss 1.69127, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 172/599 (start=11008, end=11072)\n",
      "2018-11-27T04:17:45.196072: step 772, loss 1.65951, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 173/599 (start=11072, end=11136)\n",
      "2018-11-27T04:17:45.545735: step 773, loss 1.69652, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 174/599 (start=11136, end=11200)\n",
      "2018-11-27T04:17:45.885783: step 774, loss 1.80512, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 175/599 (start=11200, end=11264)\n",
      "2018-11-27T04:17:46.227710: step 775, loss 1.51556, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 176/599 (start=11264, end=11328)\n",
      "2018-11-27T04:17:46.563658: step 776, loss 1.93109, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 177/599 (start=11328, end=11392)\n",
      "2018-11-27T04:17:46.903495: step 777, loss 1.7077, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 178/599 (start=11392, end=11456)\n",
      "2018-11-27T04:17:47.246927: step 778, loss 1.72682, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 179/599 (start=11456, end=11520)\n",
      "2018-11-27T04:17:47.569948: step 779, loss 1.41876, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 180/599 (start=11520, end=11584)\n",
      "2018-11-27T04:17:47.880510: step 780, loss 1.86444, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 181/599 (start=11584, end=11648)\n",
      "2018-11-27T04:17:48.224625: step 781, loss 1.50405, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 182/599 (start=11648, end=11712)\n",
      "2018-11-27T04:17:48.571330: step 782, loss 1.69377, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 183/599 (start=11712, end=11776)\n",
      "2018-11-27T04:17:48.878917: step 783, loss 1.58631, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 184/599 (start=11776, end=11840)\n",
      "2018-11-27T04:17:49.219037: step 784, loss 1.66603, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 185/599 (start=11840, end=11904)\n",
      "2018-11-27T04:17:49.564702: step 785, loss 1.51071, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 186/599 (start=11904, end=11968)\n",
      "2018-11-27T04:17:49.904522: step 786, loss 2.03002, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 187/599 (start=11968, end=12032)\n",
      "2018-11-27T04:17:50.247708: step 787, loss 1.58581, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 188/599 (start=12032, end=12096)\n",
      "2018-11-27T04:17:50.591156: step 788, loss 1.42582, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 189/599 (start=12096, end=12160)\n",
      "2018-11-27T04:17:50.943468: step 789, loss 1.61695, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 190/599 (start=12160, end=12224)\n",
      "2018-11-27T04:17:51.266644: step 790, loss 1.65159, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 191/599 (start=12224, end=12288)\n",
      "2018-11-27T04:17:51.578269: step 791, loss 1.80152, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 192/599 (start=12288, end=12352)\n",
      "2018-11-27T04:17:51.920524: step 792, loss 1.83103, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 193/599 (start=12352, end=12416)\n",
      "2018-11-27T04:17:52.256517: step 793, loss 1.6488, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 194/599 (start=12416, end=12480)\n",
      "2018-11-27T04:17:52.569749: step 794, loss 1.77116, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 195/599 (start=12480, end=12544)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:17:52.907451: step 795, loss 1.80181, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 196/599 (start=12544, end=12608)\n",
      "2018-11-27T04:17:53.241091: step 796, loss 1.75473, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 197/599 (start=12608, end=12672)\n",
      "2018-11-27T04:17:53.583122: step 797, loss 1.72192, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 198/599 (start=12672, end=12736)\n",
      "2018-11-27T04:17:53.914429: step 798, loss 1.83373, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 199/599 (start=12736, end=12800)\n",
      "2018-11-27T04:17:54.254858: step 799, loss 1.69989, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 200/599 (start=12800, end=12864)\n",
      "2018-11-27T04:17:54.612814: step 800, loss 1.78046, acc 0.515625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:18:00.930291: step 800, loss 1.79541, acc 0.451297\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-800\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 201/599 (start=12864, end=12928)\n",
      "2018-11-27T04:18:01.684113: step 801, loss 1.85703, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 202/599 (start=12928, end=12992)\n",
      "2018-11-27T04:18:02.022560: step 802, loss 1.54712, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 203/599 (start=12992, end=13056)\n",
      "2018-11-27T04:18:02.346694: step 803, loss 1.77083, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 204/599 (start=13056, end=13120)\n",
      "2018-11-27T04:18:02.677975: step 804, loss 1.75137, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 205/599 (start=13120, end=13184)\n",
      "2018-11-27T04:18:03.017719: step 805, loss 1.71114, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 206/599 (start=13184, end=13248)\n",
      "2018-11-27T04:18:03.334233: step 806, loss 1.74235, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 207/599 (start=13248, end=13312)\n",
      "2018-11-27T04:18:03.643534: step 807, loss 1.70808, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 208/599 (start=13312, end=13376)\n",
      "2018-11-27T04:18:03.994650: step 808, loss 1.80509, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 209/599 (start=13376, end=13440)\n",
      "2018-11-27T04:18:04.341854: step 809, loss 1.82841, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 210/599 (start=13440, end=13504)\n",
      "2018-11-27T04:18:04.665156: step 810, loss 1.76728, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 211/599 (start=13504, end=13568)\n",
      "2018-11-27T04:18:04.994999: step 811, loss 1.68321, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 212/599 (start=13568, end=13632)\n",
      "2018-11-27T04:18:05.332590: step 812, loss 1.54247, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 213/599 (start=13632, end=13696)\n",
      "2018-11-27T04:18:05.658893: step 813, loss 1.68001, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 214/599 (start=13696, end=13760)\n",
      "2018-11-27T04:18:05.995389: step 814, loss 1.51225, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 215/599 (start=13760, end=13824)\n",
      "2018-11-27T04:18:06.321592: step 815, loss 1.86858, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 216/599 (start=13824, end=13888)\n",
      "2018-11-27T04:18:06.638373: step 816, loss 1.92285, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 217/599 (start=13888, end=13952)\n",
      "2018-11-27T04:18:06.951439: step 817, loss 1.77923, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 218/599 (start=13952, end=14016)\n",
      "2018-11-27T04:18:07.265074: step 818, loss 1.49287, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 219/599 (start=14016, end=14080)\n",
      "2018-11-27T04:18:07.591818: step 819, loss 1.68378, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 220/599 (start=14080, end=14144)\n",
      "2018-11-27T04:18:07.918269: step 820, loss 1.6214, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 221/599 (start=14144, end=14208)\n",
      "2018-11-27T04:18:08.255593: step 821, loss 1.53014, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 222/599 (start=14208, end=14272)\n",
      "2018-11-27T04:18:08.606115: step 822, loss 1.81811, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 223/599 (start=14272, end=14336)\n",
      "2018-11-27T04:18:08.953803: step 823, loss 1.56979, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 224/599 (start=14336, end=14400)\n",
      "2018-11-27T04:18:09.287653: step 824, loss 1.7611, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 225/599 (start=14400, end=14464)\n",
      "2018-11-27T04:18:09.611643: step 825, loss 1.69065, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 226/599 (start=14464, end=14528)\n",
      "2018-11-27T04:18:09.931325: step 826, loss 1.76295, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 227/599 (start=14528, end=14592)\n",
      "2018-11-27T04:18:10.259738: step 827, loss 1.69101, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 228/599 (start=14592, end=14656)\n",
      "2018-11-27T04:18:10.573337: step 828, loss 1.63613, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 229/599 (start=14656, end=14720)\n",
      "2018-11-27T04:18:10.883507: step 829, loss 1.5801, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 230/599 (start=14720, end=14784)\n",
      "2018-11-27T04:18:11.230169: step 830, loss 1.8204, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 231/599 (start=14784, end=14848)\n",
      "2018-11-27T04:18:11.573424: step 831, loss 1.69099, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 232/599 (start=14848, end=14912)\n",
      "2018-11-27T04:18:11.904116: step 832, loss 1.5465, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 233/599 (start=14912, end=14976)\n",
      "2018-11-27T04:18:12.243839: step 833, loss 1.67051, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 234/599 (start=14976, end=15040)\n",
      "2018-11-27T04:18:12.593644: step 834, loss 1.60797, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 235/599 (start=15040, end=15104)\n",
      "2018-11-27T04:18:12.927270: step 835, loss 1.63673, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 236/599 (start=15104, end=15168)\n",
      "2018-11-27T04:18:13.253164: step 836, loss 1.68014, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 237/599 (start=15168, end=15232)\n",
      "2018-11-27T04:18:13.592984: step 837, loss 1.63692, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 238/599 (start=15232, end=15296)\n",
      "2018-11-27T04:18:13.907604: step 838, loss 1.74969, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 239/599 (start=15296, end=15360)\n",
      "2018-11-27T04:18:14.222260: step 839, loss 1.59188, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 240/599 (start=15360, end=15424)\n",
      "2018-11-27T04:18:14.557422: step 840, loss 1.74504, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 241/599 (start=15424, end=15488)\n",
      "2018-11-27T04:18:14.874822: step 841, loss 1.71146, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 242/599 (start=15488, end=15552)\n",
      "2018-11-27T04:18:15.220305: step 842, loss 1.81089, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 243/599 (start=15552, end=15616)\n",
      "2018-11-27T04:18:15.534109: step 843, loss 1.69313, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 244/599 (start=15616, end=15680)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:18:15.854126: step 844, loss 1.69072, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 245/599 (start=15680, end=15744)\n",
      "2018-11-27T04:18:16.189183: step 845, loss 1.74287, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 246/599 (start=15744, end=15808)\n",
      "2018-11-27T04:18:16.512910: step 846, loss 1.74878, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 247/599 (start=15808, end=15872)\n",
      "2018-11-27T04:18:16.831176: step 847, loss 1.79419, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 248/599 (start=15872, end=15936)\n",
      "2018-11-27T04:18:17.166729: step 848, loss 1.85933, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 249/599 (start=15936, end=16000)\n",
      "2018-11-27T04:18:17.499376: step 849, loss 1.72299, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 250/599 (start=16000, end=16064)\n",
      "2018-11-27T04:18:17.817675: step 850, loss 1.50598, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 251/599 (start=16064, end=16128)\n",
      "2018-11-27T04:18:18.159083: step 851, loss 1.80156, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 252/599 (start=16128, end=16192)\n",
      "2018-11-27T04:18:18.484265: step 852, loss 1.77063, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 253/599 (start=16192, end=16256)\n",
      "2018-11-27T04:18:18.833064: step 853, loss 1.82514, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 254/599 (start=16256, end=16320)\n",
      "2018-11-27T04:18:19.167624: step 854, loss 1.37066, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 255/599 (start=16320, end=16384)\n",
      "2018-11-27T04:18:19.496288: step 855, loss 1.67287, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 256/599 (start=16384, end=16448)\n",
      "2018-11-27T04:18:19.813460: step 856, loss 1.71793, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 257/599 (start=16448, end=16512)\n",
      "2018-11-27T04:18:20.147699: step 857, loss 1.55747, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 258/599 (start=16512, end=16576)\n",
      "2018-11-27T04:18:20.479784: step 858, loss 1.69292, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 259/599 (start=16576, end=16640)\n",
      "2018-11-27T04:18:20.829205: step 859, loss 1.88834, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 260/599 (start=16640, end=16704)\n",
      "2018-11-27T04:18:21.171824: step 860, loss 1.61987, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 261/599 (start=16704, end=16768)\n",
      "2018-11-27T04:18:21.506418: step 861, loss 1.4395, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 262/599 (start=16768, end=16832)\n",
      "2018-11-27T04:18:21.842338: step 862, loss 1.55062, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 263/599 (start=16832, end=16896)\n",
      "2018-11-27T04:18:22.194994: step 863, loss 1.65715, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 264/599 (start=16896, end=16960)\n",
      "2018-11-27T04:18:22.540211: step 864, loss 1.64496, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 265/599 (start=16960, end=17024)\n",
      "2018-11-27T04:18:22.857612: step 865, loss 1.96723, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 266/599 (start=17024, end=17088)\n",
      "2018-11-27T04:18:23.182630: step 866, loss 1.69468, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 267/599 (start=17088, end=17152)\n",
      "2018-11-27T04:18:23.515295: step 867, loss 1.71373, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 268/599 (start=17152, end=17216)\n",
      "2018-11-27T04:18:23.851380: step 868, loss 1.77468, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 269/599 (start=17216, end=17280)\n",
      "2018-11-27T04:18:24.170102: step 869, loss 1.80079, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 270/599 (start=17280, end=17344)\n",
      "2018-11-27T04:18:24.496899: step 870, loss 1.87206, acc 0.3125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 271/599 (start=17344, end=17408)\n",
      "2018-11-27T04:18:24.844018: step 871, loss 1.56393, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 272/599 (start=17408, end=17472)\n",
      "2018-11-27T04:18:25.186956: step 872, loss 1.56773, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 273/599 (start=17472, end=17536)\n",
      "2018-11-27T04:18:25.531064: step 873, loss 1.65268, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 274/599 (start=17536, end=17600)\n",
      "2018-11-27T04:18:25.859150: step 874, loss 1.71527, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 275/599 (start=17600, end=17664)\n",
      "2018-11-27T04:18:26.191480: step 875, loss 1.4767, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 276/599 (start=17664, end=17728)\n",
      "2018-11-27T04:18:26.543249: step 876, loss 1.73359, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 277/599 (start=17728, end=17792)\n",
      "2018-11-27T04:18:26.871099: step 877, loss 1.59183, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 278/599 (start=17792, end=17856)\n",
      "2018-11-27T04:18:27.188904: step 878, loss 1.6381, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 279/599 (start=17856, end=17920)\n",
      "2018-11-27T04:18:27.512110: step 879, loss 1.58319, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 280/599 (start=17920, end=17984)\n",
      "2018-11-27T04:18:27.854552: step 880, loss 1.83398, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 281/599 (start=17984, end=18048)\n",
      "2018-11-27T04:18:28.209126: step 881, loss 1.54389, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 282/599 (start=18048, end=18112)\n",
      "2018-11-27T04:18:28.525947: step 882, loss 1.78052, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 283/599 (start=18112, end=18176)\n",
      "2018-11-27T04:18:28.863580: step 883, loss 1.75487, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 284/599 (start=18176, end=18240)\n",
      "2018-11-27T04:18:29.183624: step 884, loss 1.76236, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 285/599 (start=18240, end=18304)\n",
      "2018-11-27T04:18:29.527030: step 885, loss 1.73108, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 286/599 (start=18304, end=18368)\n",
      "2018-11-27T04:18:29.841173: step 886, loss 1.64252, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 287/599 (start=18368, end=18432)\n",
      "2018-11-27T04:18:30.169040: step 887, loss 1.43512, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 288/599 (start=18432, end=18496)\n",
      "2018-11-27T04:18:30.474688: step 888, loss 1.77062, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 289/599 (start=18496, end=18560)\n",
      "2018-11-27T04:18:30.824062: step 889, loss 1.96536, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 290/599 (start=18560, end=18624)\n",
      "2018-11-27T04:18:31.131402: step 890, loss 1.62816, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 291/599 (start=18624, end=18688)\n",
      "2018-11-27T04:18:31.455355: step 891, loss 1.88337, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 292/599 (start=18688, end=18752)\n",
      "2018-11-27T04:18:31.803539: step 892, loss 1.53627, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 293/599 (start=18752, end=18816)\n",
      "2018-11-27T04:18:32.138018: step 893, loss 1.60857, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 294/599 (start=18816, end=18880)\n",
      "2018-11-27T04:18:32.461816: step 894, loss 1.57637, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 295/599 (start=18880, end=18944)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:18:32.784547: step 895, loss 1.72557, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 296/599 (start=18944, end=19008)\n",
      "2018-11-27T04:18:33.107565: step 896, loss 1.57477, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 297/599 (start=19008, end=19072)\n",
      "2018-11-27T04:18:33.421553: step 897, loss 1.69018, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 298/599 (start=19072, end=19136)\n",
      "2018-11-27T04:18:33.734495: step 898, loss 1.94794, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 299/599 (start=19136, end=19200)\n",
      "2018-11-27T04:18:34.079476: step 899, loss 1.70575, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 300/599 (start=19200, end=19264)\n",
      "2018-11-27T04:18:34.423107: step 900, loss 1.6971, acc 0.609375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:18:40.633597: step 900, loss 1.78266, acc 0.455685\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-900\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 301/599 (start=19264, end=19328)\n",
      "2018-11-27T04:18:41.381311: step 901, loss 1.61674, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 302/599 (start=19328, end=19392)\n",
      "2018-11-27T04:18:41.715804: step 902, loss 1.65476, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 303/599 (start=19392, end=19456)\n",
      "2018-11-27T04:18:42.047288: step 903, loss 1.98192, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 304/599 (start=19456, end=19520)\n",
      "2018-11-27T04:18:42.375814: step 904, loss 1.92259, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 305/599 (start=19520, end=19584)\n",
      "2018-11-27T04:18:42.699022: step 905, loss 1.6593, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 306/599 (start=19584, end=19648)\n",
      "2018-11-27T04:18:43.032014: step 906, loss 1.64685, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 307/599 (start=19648, end=19712)\n",
      "2018-11-27T04:18:43.369731: step 907, loss 1.87468, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 308/599 (start=19712, end=19776)\n",
      "2018-11-27T04:18:43.704843: step 908, loss 1.82378, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 309/599 (start=19776, end=19840)\n",
      "2018-11-27T04:18:44.054727: step 909, loss 1.69061, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 310/599 (start=19840, end=19904)\n",
      "2018-11-27T04:18:44.418456: step 910, loss 1.71792, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 311/599 (start=19904, end=19968)\n",
      "2018-11-27T04:18:44.765323: step 911, loss 1.72389, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 312/599 (start=19968, end=20032)\n",
      "2018-11-27T04:18:45.106135: step 912, loss 1.58914, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 313/599 (start=20032, end=20096)\n",
      "2018-11-27T04:18:45.421286: step 913, loss 1.87156, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 314/599 (start=20096, end=20160)\n",
      "2018-11-27T04:18:45.727643: step 914, loss 1.42418, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 315/599 (start=20160, end=20224)\n",
      "2018-11-27T04:18:46.048310: step 915, loss 1.58963, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 316/599 (start=20224, end=20288)\n",
      "2018-11-27T04:18:46.366393: step 916, loss 1.79043, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 317/599 (start=20288, end=20352)\n",
      "2018-11-27T04:18:46.694660: step 917, loss 1.63073, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 318/599 (start=20352, end=20416)\n",
      "2018-11-27T04:18:47.027902: step 918, loss 1.69913, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 319/599 (start=20416, end=20480)\n",
      "2018-11-27T04:18:47.335939: step 919, loss 1.49311, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 320/599 (start=20480, end=20544)\n",
      "2018-11-27T04:18:47.673894: step 920, loss 1.59645, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 321/599 (start=20544, end=20608)\n",
      "2018-11-27T04:18:48.013947: step 921, loss 1.42155, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 322/599 (start=20608, end=20672)\n",
      "2018-11-27T04:18:48.358345: step 922, loss 1.61029, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 323/599 (start=20672, end=20736)\n",
      "2018-11-27T04:18:48.697448: step 923, loss 1.66577, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 324/599 (start=20736, end=20800)\n",
      "2018-11-27T04:18:49.011887: step 924, loss 1.60182, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 325/599 (start=20800, end=20864)\n",
      "2018-11-27T04:18:49.321228: step 925, loss 1.80463, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 326/599 (start=20864, end=20928)\n",
      "2018-11-27T04:18:49.669848: step 926, loss 1.71106, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 327/599 (start=20928, end=20992)\n",
      "2018-11-27T04:18:50.017374: step 927, loss 1.70812, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 328/599 (start=20992, end=21056)\n",
      "2018-11-27T04:18:50.347590: step 928, loss 1.42008, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 329/599 (start=21056, end=21120)\n",
      "2018-11-27T04:18:50.691602: step 929, loss 1.76662, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 330/599 (start=21120, end=21184)\n",
      "2018-11-27T04:18:51.012626: step 930, loss 1.71558, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 331/599 (start=21184, end=21248)\n",
      "2018-11-27T04:18:51.319854: step 931, loss 1.90509, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 332/599 (start=21248, end=21312)\n",
      "2018-11-27T04:18:51.651484: step 932, loss 1.5727, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 333/599 (start=21312, end=21376)\n",
      "2018-11-27T04:18:51.971797: step 933, loss 1.68361, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 334/599 (start=21376, end=21440)\n",
      "2018-11-27T04:18:52.317929: step 934, loss 1.75268, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 335/599 (start=21440, end=21504)\n",
      "2018-11-27T04:18:52.651199: step 935, loss 1.79581, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 336/599 (start=21504, end=21568)\n",
      "2018-11-27T04:18:52.987245: step 936, loss 1.66327, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 337/599 (start=21568, end=21632)\n",
      "2018-11-27T04:18:53.335275: step 937, loss 1.70813, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 338/599 (start=21632, end=21696)\n",
      "2018-11-27T04:18:53.653605: step 938, loss 1.62122, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 339/599 (start=21696, end=21760)\n",
      "2018-11-27T04:18:53.987552: step 939, loss 1.80649, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 340/599 (start=21760, end=21824)\n",
      "2018-11-27T04:18:54.335763: step 940, loss 1.64254, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 341/599 (start=21824, end=21888)\n",
      "2018-11-27T04:18:54.655092: step 941, loss 1.72716, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 342/599 (start=21888, end=21952)\n",
      "2018-11-27T04:18:54.986203: step 942, loss 1.71276, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 343/599 (start=21952, end=22016)\n",
      "2018-11-27T04:18:55.291765: step 943, loss 1.52803, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 344/599 (start=22016, end=22080)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:18:55.619272: step 944, loss 1.60167, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 345/599 (start=22080, end=22144)\n",
      "2018-11-27T04:18:55.964754: step 945, loss 1.93672, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 346/599 (start=22144, end=22208)\n",
      "2018-11-27T04:18:56.298607: step 946, loss 1.73736, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 347/599 (start=22208, end=22272)\n",
      "2018-11-27T04:18:56.630851: step 947, loss 1.73852, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 348/599 (start=22272, end=22336)\n",
      "2018-11-27T04:18:56.934248: step 948, loss 1.55713, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 349/599 (start=22336, end=22400)\n",
      "2018-11-27T04:18:57.257962: step 949, loss 1.5287, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 350/599 (start=22400, end=22464)\n",
      "2018-11-27T04:18:57.568163: step 950, loss 1.55318, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 351/599 (start=22464, end=22528)\n",
      "2018-11-27T04:18:57.889468: step 951, loss 1.67148, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 352/599 (start=22528, end=22592)\n",
      "2018-11-27T04:18:58.198974: step 952, loss 1.64044, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 353/599 (start=22592, end=22656)\n",
      "2018-11-27T04:18:58.516431: step 953, loss 1.66408, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 354/599 (start=22656, end=22720)\n",
      "2018-11-27T04:18:58.860595: step 954, loss 1.71506, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 355/599 (start=22720, end=22784)\n",
      "2018-11-27T04:18:59.189273: step 955, loss 1.82394, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 356/599 (start=22784, end=22848)\n",
      "2018-11-27T04:18:59.527205: step 956, loss 1.64063, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 357/599 (start=22848, end=22912)\n",
      "2018-11-27T04:18:59.861692: step 957, loss 1.76291, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 358/599 (start=22912, end=22976)\n",
      "2018-11-27T04:19:00.213355: step 958, loss 1.83845, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 359/599 (start=22976, end=23040)\n",
      "2018-11-27T04:19:00.544437: step 959, loss 1.67105, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 360/599 (start=23040, end=23104)\n",
      "2018-11-27T04:19:00.877918: step 960, loss 1.69113, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 361/599 (start=23104, end=23168)\n",
      "2018-11-27T04:19:01.206033: step 961, loss 1.78156, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 362/599 (start=23168, end=23232)\n",
      "2018-11-27T04:19:01.550379: step 962, loss 1.59731, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 363/599 (start=23232, end=23296)\n",
      "2018-11-27T04:19:01.881017: step 963, loss 1.94475, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 364/599 (start=23296, end=23360)\n",
      "2018-11-27T04:19:02.183681: step 964, loss 1.63647, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 365/599 (start=23360, end=23424)\n",
      "2018-11-27T04:19:02.527266: step 965, loss 1.52077, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 366/599 (start=23424, end=23488)\n",
      "2018-11-27T04:19:02.859527: step 966, loss 1.73683, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 367/599 (start=23488, end=23552)\n",
      "2018-11-27T04:19:03.183143: step 967, loss 1.80217, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 368/599 (start=23552, end=23616)\n",
      "2018-11-27T04:19:03.496833: step 968, loss 1.77119, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 369/599 (start=23616, end=23680)\n",
      "2018-11-27T04:19:03.809074: step 969, loss 1.55646, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 370/599 (start=23680, end=23744)\n",
      "2018-11-27T04:19:04.159128: step 970, loss 1.73607, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 371/599 (start=23744, end=23808)\n",
      "2018-11-27T04:19:04.511713: step 971, loss 1.7099, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 372/599 (start=23808, end=23872)\n",
      "2018-11-27T04:19:04.857336: step 972, loss 1.82417, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 373/599 (start=23872, end=23936)\n",
      "2018-11-27T04:19:05.180532: step 973, loss 1.66923, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 374/599 (start=23936, end=24000)\n",
      "2018-11-27T04:19:05.525461: step 974, loss 1.44348, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 375/599 (start=24000, end=24064)\n",
      "2018-11-27T04:19:05.840661: step 975, loss 1.68798, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 376/599 (start=24064, end=24128)\n",
      "2018-11-27T04:19:06.176088: step 976, loss 1.60363, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 377/599 (start=24128, end=24192)\n",
      "2018-11-27T04:19:06.510158: step 977, loss 1.74509, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 378/599 (start=24192, end=24256)\n",
      "2018-11-27T04:19:06.848230: step 978, loss 1.65709, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 379/599 (start=24256, end=24320)\n",
      "2018-11-27T04:19:07.184623: step 979, loss 1.55584, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 380/599 (start=24320, end=24384)\n",
      "2018-11-27T04:19:07.499870: step 980, loss 1.76049, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 381/599 (start=24384, end=24448)\n",
      "2018-11-27T04:19:07.832270: step 981, loss 2.12195, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 382/599 (start=24448, end=24512)\n",
      "2018-11-27T04:19:08.156661: step 982, loss 1.87226, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 383/599 (start=24512, end=24576)\n",
      "2018-11-27T04:19:08.471349: step 983, loss 1.72394, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 384/599 (start=24576, end=24640)\n",
      "2018-11-27T04:19:08.815073: step 984, loss 1.50344, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 385/599 (start=24640, end=24704)\n",
      "2018-11-27T04:19:09.159149: step 985, loss 1.73134, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 386/599 (start=24704, end=24768)\n",
      "2018-11-27T04:19:09.466997: step 986, loss 1.50398, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 387/599 (start=24768, end=24832)\n",
      "2018-11-27T04:19:09.798414: step 987, loss 1.58992, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 388/599 (start=24832, end=24896)\n",
      "2018-11-27T04:19:10.118615: step 988, loss 1.86367, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 389/599 (start=24896, end=24960)\n",
      "2018-11-27T04:19:10.445072: step 989, loss 1.75836, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 390/599 (start=24960, end=25024)\n",
      "2018-11-27T04:19:10.773262: step 990, loss 1.80574, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 391/599 (start=25024, end=25088)\n",
      "2018-11-27T04:19:11.117739: step 991, loss 1.54572, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 392/599 (start=25088, end=25152)\n",
      "2018-11-27T04:19:11.440426: step 992, loss 1.61789, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 393/599 (start=25152, end=25216)\n",
      "2018-11-27T04:19:11.786905: step 993, loss 1.72108, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 394/599 (start=25216, end=25280)\n",
      "2018-11-27T04:19:12.102052: step 994, loss 1.49785, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 395/599 (start=25280, end=25344)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:19:12.439457: step 995, loss 1.73645, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 396/599 (start=25344, end=25408)\n",
      "2018-11-27T04:19:12.764920: step 996, loss 1.92288, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 397/599 (start=25408, end=25472)\n",
      "2018-11-27T04:19:13.098805: step 997, loss 1.45297, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 398/599 (start=25472, end=25536)\n",
      "2018-11-27T04:19:13.439592: step 998, loss 1.52525, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 399/599 (start=25536, end=25600)\n",
      "2018-11-27T04:19:13.761711: step 999, loss 1.51547, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 400/599 (start=25600, end=25664)\n",
      "2018-11-27T04:19:14.107568: step 1000, loss 1.61604, acc 0.515625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:19:20.240400: step 1000, loss 1.76299, acc 0.457879\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-1000\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 401/599 (start=25664, end=25728)\n",
      "2018-11-27T04:19:20.991966: step 1001, loss 1.39078, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 402/599 (start=25728, end=25792)\n",
      "2018-11-27T04:19:21.316870: step 1002, loss 1.5486, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 403/599 (start=25792, end=25856)\n",
      "2018-11-27T04:19:21.651644: step 1003, loss 1.53106, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 404/599 (start=25856, end=25920)\n",
      "2018-11-27T04:19:21.972706: step 1004, loss 1.5815, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 405/599 (start=25920, end=25984)\n",
      "2018-11-27T04:19:22.309824: step 1005, loss 1.97739, acc 0.34375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 406/599 (start=25984, end=26048)\n",
      "2018-11-27T04:19:22.640695: step 1006, loss 1.72501, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 407/599 (start=26048, end=26112)\n",
      "2018-11-27T04:19:22.976562: step 1007, loss 1.73207, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 408/599 (start=26112, end=26176)\n",
      "2018-11-27T04:19:23.315528: step 1008, loss 1.50318, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 409/599 (start=26176, end=26240)\n",
      "2018-11-27T04:19:23.654627: step 1009, loss 1.82095, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 410/599 (start=26240, end=26304)\n",
      "2018-11-27T04:19:23.965907: step 1010, loss 1.72757, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 411/599 (start=26304, end=26368)\n",
      "2018-11-27T04:19:24.300513: step 1011, loss 1.58929, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 412/599 (start=26368, end=26432)\n",
      "2018-11-27T04:19:24.617363: step 1012, loss 1.70733, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 413/599 (start=26432, end=26496)\n",
      "2018-11-27T04:19:24.949966: step 1013, loss 1.81802, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 414/599 (start=26496, end=26560)\n",
      "2018-11-27T04:19:25.304186: step 1014, loss 1.67242, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 415/599 (start=26560, end=26624)\n",
      "2018-11-27T04:19:25.637757: step 1015, loss 1.78872, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 416/599 (start=26624, end=26688)\n",
      "2018-11-27T04:19:25.964864: step 1016, loss 1.42001, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 417/599 (start=26688, end=26752)\n",
      "2018-11-27T04:19:26.279298: step 1017, loss 1.74787, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 418/599 (start=26752, end=26816)\n",
      "2018-11-27T04:19:26.588263: step 1018, loss 1.57857, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 419/599 (start=26816, end=26880)\n",
      "2018-11-27T04:19:26.940162: step 1019, loss 1.4145, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 420/599 (start=26880, end=26944)\n",
      "2018-11-27T04:19:27.262175: step 1020, loss 1.7716, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 421/599 (start=26944, end=27008)\n",
      "2018-11-27T04:19:27.588388: step 1021, loss 1.65343, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 422/599 (start=27008, end=27072)\n",
      "2018-11-27T04:19:27.914486: step 1022, loss 1.68202, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 423/599 (start=27072, end=27136)\n",
      "2018-11-27T04:19:28.253702: step 1023, loss 1.61253, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 424/599 (start=27136, end=27200)\n",
      "2018-11-27T04:19:28.589563: step 1024, loss 1.54004, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 425/599 (start=27200, end=27264)\n",
      "2018-11-27T04:19:28.915141: step 1025, loss 1.76207, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 426/599 (start=27264, end=27328)\n",
      "2018-11-27T04:19:29.237419: step 1026, loss 1.73801, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 427/599 (start=27328, end=27392)\n",
      "2018-11-27T04:19:29.558348: step 1027, loss 1.50164, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 428/599 (start=27392, end=27456)\n",
      "2018-11-27T04:19:29.900125: step 1028, loss 1.77645, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 429/599 (start=27456, end=27520)\n",
      "2018-11-27T04:19:30.216975: step 1029, loss 1.8145, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 430/599 (start=27520, end=27584)\n",
      "2018-11-27T04:19:30.557132: step 1030, loss 1.86384, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 431/599 (start=27584, end=27648)\n",
      "2018-11-27T04:19:30.866275: step 1031, loss 1.72441, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 432/599 (start=27648, end=27712)\n",
      "2018-11-27T04:19:31.198626: step 1032, loss 1.76263, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 433/599 (start=27712, end=27776)\n",
      "2018-11-27T04:19:31.529584: step 1033, loss 1.82972, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 434/599 (start=27776, end=27840)\n",
      "2018-11-27T04:19:31.872303: step 1034, loss 1.5312, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 435/599 (start=27840, end=27904)\n",
      "2018-11-27T04:19:32.199617: step 1035, loss 1.59595, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 436/599 (start=27904, end=27968)\n",
      "2018-11-27T04:19:32.518639: step 1036, loss 1.58959, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 437/599 (start=27968, end=28032)\n",
      "2018-11-27T04:19:32.868020: step 1037, loss 1.68625, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 438/599 (start=28032, end=28096)\n",
      "2018-11-27T04:19:33.206041: step 1038, loss 1.62964, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 439/599 (start=28096, end=28160)\n",
      "2018-11-27T04:19:33.535776: step 1039, loss 1.46349, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 440/599 (start=28160, end=28224)\n",
      "2018-11-27T04:19:33.848272: step 1040, loss 1.919, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 441/599 (start=28224, end=28288)\n",
      "2018-11-27T04:19:34.167723: step 1041, loss 1.8755, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 442/599 (start=28288, end=28352)\n",
      "2018-11-27T04:19:34.476346: step 1042, loss 1.62653, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 443/599 (start=28352, end=28416)\n",
      "2018-11-27T04:19:34.783889: step 1043, loss 1.70219, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 444/599 (start=28416, end=28480)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:19:35.098938: step 1044, loss 1.87004, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 445/599 (start=28480, end=28544)\n",
      "2018-11-27T04:19:35.417621: step 1045, loss 1.50724, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 446/599 (start=28544, end=28608)\n",
      "2018-11-27T04:19:35.738477: step 1046, loss 1.84396, acc 0.359375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 447/599 (start=28608, end=28672)\n",
      "2018-11-27T04:19:36.072918: step 1047, loss 1.62231, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 448/599 (start=28672, end=28736)\n",
      "2018-11-27T04:19:36.420411: step 1048, loss 1.67424, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 449/599 (start=28736, end=28800)\n",
      "2018-11-27T04:19:36.769183: step 1049, loss 1.57291, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 450/599 (start=28800, end=28864)\n",
      "2018-11-27T04:19:37.085655: step 1050, loss 1.63334, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 451/599 (start=28864, end=28928)\n",
      "2018-11-27T04:19:37.414976: step 1051, loss 1.45889, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 452/599 (start=28928, end=28992)\n",
      "2018-11-27T04:19:37.749241: step 1052, loss 1.36348, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 453/599 (start=28992, end=29056)\n",
      "2018-11-27T04:19:38.097313: step 1053, loss 1.78976, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 454/599 (start=29056, end=29120)\n",
      "2018-11-27T04:19:38.423146: step 1054, loss 1.75495, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 455/599 (start=29120, end=29184)\n",
      "2018-11-27T04:19:38.772847: step 1055, loss 1.67989, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 456/599 (start=29184, end=29248)\n",
      "2018-11-27T04:19:39.104296: step 1056, loss 1.69445, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 457/599 (start=29248, end=29312)\n",
      "2018-11-27T04:19:39.440795: step 1057, loss 1.76563, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 458/599 (start=29312, end=29376)\n",
      "2018-11-27T04:19:39.750268: step 1058, loss 1.63795, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 459/599 (start=29376, end=29440)\n",
      "2018-11-27T04:19:40.064466: step 1059, loss 1.44255, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 460/599 (start=29440, end=29504)\n",
      "2018-11-27T04:19:40.384589: step 1060, loss 1.58592, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 461/599 (start=29504, end=29568)\n",
      "2018-11-27T04:19:40.699597: step 1061, loss 1.7713, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 462/599 (start=29568, end=29632)\n",
      "2018-11-27T04:19:41.045881: step 1062, loss 1.83696, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 463/599 (start=29632, end=29696)\n",
      "2018-11-27T04:19:41.376405: step 1063, loss 1.47052, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 464/599 (start=29696, end=29760)\n",
      "2018-11-27T04:19:41.707480: step 1064, loss 1.68218, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 465/599 (start=29760, end=29824)\n",
      "2018-11-27T04:19:42.036651: step 1065, loss 1.66371, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 466/599 (start=29824, end=29888)\n",
      "2018-11-27T04:19:42.354615: step 1066, loss 1.66546, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 467/599 (start=29888, end=29952)\n",
      "2018-11-27T04:19:42.678217: step 1067, loss 1.77823, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 468/599 (start=29952, end=30016)\n",
      "2018-11-27T04:19:42.994873: step 1068, loss 1.56421, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 469/599 (start=30016, end=30080)\n",
      "2018-11-27T04:19:43.297768: step 1069, loss 1.41278, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 470/599 (start=30080, end=30144)\n",
      "2018-11-27T04:19:43.613360: step 1070, loss 1.5552, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 471/599 (start=30144, end=30208)\n",
      "2018-11-27T04:19:43.928501: step 1071, loss 1.61692, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 472/599 (start=30208, end=30272)\n",
      "2018-11-27T04:19:44.241411: step 1072, loss 1.71936, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 473/599 (start=30272, end=30336)\n",
      "2018-11-27T04:19:44.568284: step 1073, loss 1.77512, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 474/599 (start=30336, end=30400)\n",
      "2018-11-27T04:19:44.906094: step 1074, loss 1.61088, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 475/599 (start=30400, end=30464)\n",
      "2018-11-27T04:19:45.242448: step 1075, loss 1.84356, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 476/599 (start=30464, end=30528)\n",
      "2018-11-27T04:19:45.574069: step 1076, loss 1.77472, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 477/599 (start=30528, end=30592)\n",
      "2018-11-27T04:19:45.912594: step 1077, loss 1.76178, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 478/599 (start=30592, end=30656)\n",
      "2018-11-27T04:19:46.232528: step 1078, loss 2.05448, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 479/599 (start=30656, end=30720)\n",
      "2018-11-27T04:19:46.568283: step 1079, loss 1.52386, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 480/599 (start=30720, end=30784)\n",
      "2018-11-27T04:19:46.921929: step 1080, loss 1.71514, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 481/599 (start=30784, end=30848)\n",
      "2018-11-27T04:19:47.269262: step 1081, loss 1.53377, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 482/599 (start=30848, end=30912)\n",
      "2018-11-27T04:19:47.611535: step 1082, loss 1.54322, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 483/599 (start=30912, end=30976)\n",
      "2018-11-27T04:19:47.943138: step 1083, loss 1.72395, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 484/599 (start=30976, end=31040)\n",
      "2018-11-27T04:19:48.255842: step 1084, loss 1.64489, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 485/599 (start=31040, end=31104)\n",
      "2018-11-27T04:19:48.571865: step 1085, loss 1.90809, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 486/599 (start=31104, end=31168)\n",
      "2018-11-27T04:19:48.917138: step 1086, loss 1.57839, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 487/599 (start=31168, end=31232)\n",
      "2018-11-27T04:19:49.245969: step 1087, loss 1.61783, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 488/599 (start=31232, end=31296)\n",
      "2018-11-27T04:19:49.592231: step 1088, loss 1.74145, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 489/599 (start=31296, end=31360)\n",
      "2018-11-27T04:19:49.912333: step 1089, loss 1.5508, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 490/599 (start=31360, end=31424)\n",
      "2018-11-27T04:19:50.252237: step 1090, loss 1.65613, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 491/599 (start=31424, end=31488)\n",
      "2018-11-27T04:19:50.587159: step 1091, loss 1.66023, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 492/599 (start=31488, end=31552)\n",
      "2018-11-27T04:19:50.903471: step 1092, loss 1.64852, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 493/599 (start=31552, end=31616)\n",
      "2018-11-27T04:19:51.255387: step 1093, loss 1.86786, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 494/599 (start=31616, end=31680)\n",
      "2018-11-27T04:19:51.590436: step 1094, loss 1.63534, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 495/599 (start=31680, end=31744)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:19:51.917959: step 1095, loss 1.70674, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 496/599 (start=31744, end=31808)\n",
      "2018-11-27T04:19:52.237509: step 1096, loss 1.56451, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 497/599 (start=31808, end=31872)\n",
      "2018-11-27T04:19:52.573849: step 1097, loss 1.70396, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 498/599 (start=31872, end=31936)\n",
      "2018-11-27T04:19:52.911253: step 1098, loss 1.5627, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 499/599 (start=31936, end=32000)\n",
      "2018-11-27T04:19:53.234102: step 1099, loss 1.69237, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 500/599 (start=32000, end=32064)\n",
      "2018-11-27T04:19:53.565556: step 1100, loss 1.60021, acc 0.46875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:19:59.760743: step 1100, loss 1.74172, acc 0.475982\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-1100\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 501/599 (start=32064, end=32128)\n",
      "2018-11-27T04:20:00.516189: step 1101, loss 1.33305, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 502/599 (start=32128, end=32192)\n",
      "2018-11-27T04:20:00.839337: step 1102, loss 1.68732, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 503/599 (start=32192, end=32256)\n",
      "2018-11-27T04:20:01.173555: step 1103, loss 1.80144, acc 0.375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 504/599 (start=32256, end=32320)\n",
      "2018-11-27T04:20:01.475898: step 1104, loss 1.65788, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 505/599 (start=32320, end=32384)\n",
      "2018-11-27T04:20:01.817606: step 1105, loss 1.62464, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 506/599 (start=32384, end=32448)\n",
      "2018-11-27T04:20:02.137481: step 1106, loss 1.78121, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 507/599 (start=32448, end=32512)\n",
      "2018-11-27T04:20:02.472184: step 1107, loss 1.86056, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 508/599 (start=32512, end=32576)\n",
      "2018-11-27T04:20:02.796220: step 1108, loss 1.66406, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 509/599 (start=32576, end=32640)\n",
      "2018-11-27T04:20:03.136119: step 1109, loss 1.66458, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 510/599 (start=32640, end=32704)\n",
      "2018-11-27T04:20:03.482904: step 1110, loss 1.57043, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 511/599 (start=32704, end=32768)\n",
      "2018-11-27T04:20:03.792711: step 1111, loss 1.74382, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 512/599 (start=32768, end=32832)\n",
      "2018-11-27T04:20:04.138004: step 1112, loss 1.64824, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 513/599 (start=32832, end=32896)\n",
      "2018-11-27T04:20:04.480154: step 1113, loss 1.56596, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 514/599 (start=32896, end=32960)\n",
      "2018-11-27T04:20:04.811689: step 1114, loss 1.41322, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 515/599 (start=32960, end=33024)\n",
      "2018-11-27T04:20:05.142780: step 1115, loss 1.86103, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 516/599 (start=33024, end=33088)\n",
      "2018-11-27T04:20:05.483979: step 1116, loss 1.83315, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 517/599 (start=33088, end=33152)\n",
      "2018-11-27T04:20:05.815339: step 1117, loss 1.57819, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 518/599 (start=33152, end=33216)\n",
      "2018-11-27T04:20:06.126400: step 1118, loss 1.78399, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 519/599 (start=33216, end=33280)\n",
      "2018-11-27T04:20:06.464439: step 1119, loss 1.99879, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 520/599 (start=33280, end=33344)\n",
      "2018-11-27T04:20:06.775888: step 1120, loss 1.4455, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 521/599 (start=33344, end=33408)\n",
      "2018-11-27T04:20:07.119249: step 1121, loss 1.72723, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 522/599 (start=33408, end=33472)\n",
      "2018-11-27T04:20:07.452551: step 1122, loss 1.81026, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 523/599 (start=33472, end=33536)\n",
      "2018-11-27T04:20:07.777402: step 1123, loss 1.75581, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 524/599 (start=33536, end=33600)\n",
      "2018-11-27T04:20:08.118254: step 1124, loss 1.64073, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 525/599 (start=33600, end=33664)\n",
      "2018-11-27T04:20:08.423293: step 1125, loss 1.67396, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 526/599 (start=33664, end=33728)\n",
      "2018-11-27T04:20:08.752502: step 1126, loss 1.74775, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 527/599 (start=33728, end=33792)\n",
      "2018-11-27T04:20:09.090510: step 1127, loss 1.67852, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 528/599 (start=33792, end=33856)\n",
      "2018-11-27T04:20:09.434498: step 1128, loss 1.566, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 529/599 (start=33856, end=33920)\n",
      "2018-11-27T04:20:09.767596: step 1129, loss 1.50984, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 530/599 (start=33920, end=33984)\n",
      "2018-11-27T04:20:10.110952: step 1130, loss 1.54105, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 531/599 (start=33984, end=34048)\n",
      "2018-11-27T04:20:10.470292: step 1131, loss 1.63863, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 532/599 (start=34048, end=34112)\n",
      "2018-11-27T04:20:10.783439: step 1132, loss 1.91992, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 533/599 (start=34112, end=34176)\n",
      "2018-11-27T04:20:11.131280: step 1133, loss 1.85529, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 534/599 (start=34176, end=34240)\n",
      "2018-11-27T04:20:11.469422: step 1134, loss 1.50246, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 535/599 (start=34240, end=34304)\n",
      "2018-11-27T04:20:11.813081: step 1135, loss 1.85787, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 536/599 (start=34304, end=34368)\n",
      "2018-11-27T04:20:12.142351: step 1136, loss 1.53321, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 537/599 (start=34368, end=34432)\n",
      "2018-11-27T04:20:12.487882: step 1137, loss 1.85645, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 538/599 (start=34432, end=34496)\n",
      "2018-11-27T04:20:12.805044: step 1138, loss 1.36431, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 539/599 (start=34496, end=34560)\n",
      "2018-11-27T04:20:13.124859: step 1139, loss 1.61697, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 540/599 (start=34560, end=34624)\n",
      "2018-11-27T04:20:13.458020: step 1140, loss 1.6528, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 541/599 (start=34624, end=34688)\n",
      "2018-11-27T04:20:13.800470: step 1141, loss 1.50395, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 542/599 (start=34688, end=34752)\n",
      "2018-11-27T04:20:14.121623: step 1142, loss 1.74495, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 543/599 (start=34752, end=34816)\n",
      "2018-11-27T04:20:14.460825: step 1143, loss 1.55614, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 544/599 (start=34816, end=34880)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:20:14.766262: step 1144, loss 1.48509, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 545/599 (start=34880, end=34944)\n",
      "2018-11-27T04:20:15.108751: step 1145, loss 1.47666, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 546/599 (start=34944, end=35008)\n",
      "2018-11-27T04:20:15.432575: step 1146, loss 1.31579, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 547/599 (start=35008, end=35072)\n",
      "2018-11-27T04:20:15.762364: step 1147, loss 1.53759, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 548/599 (start=35072, end=35136)\n",
      "2018-11-27T04:20:16.104870: step 1148, loss 1.59976, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 549/599 (start=35136, end=35200)\n",
      "2018-11-27T04:20:16.421378: step 1149, loss 1.50513, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 550/599 (start=35200, end=35264)\n",
      "2018-11-27T04:20:16.752654: step 1150, loss 1.4186, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 551/599 (start=35264, end=35328)\n",
      "2018-11-27T04:20:17.057206: step 1151, loss 1.51503, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 552/599 (start=35328, end=35392)\n",
      "2018-11-27T04:20:17.388675: step 1152, loss 1.56628, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 553/599 (start=35392, end=35456)\n",
      "2018-11-27T04:20:17.705095: step 1153, loss 1.70642, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 554/599 (start=35456, end=35520)\n",
      "2018-11-27T04:20:18.034510: step 1154, loss 1.82525, acc 0.40625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 555/599 (start=35520, end=35584)\n",
      "2018-11-27T04:20:18.370318: step 1155, loss 1.65439, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 556/599 (start=35584, end=35648)\n",
      "2018-11-27T04:20:18.690017: step 1156, loss 1.48483, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 557/599 (start=35648, end=35712)\n",
      "2018-11-27T04:20:19.012161: step 1157, loss 1.83155, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 558/599 (start=35712, end=35776)\n",
      "2018-11-27T04:20:19.350432: step 1158, loss 1.49406, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 559/599 (start=35776, end=35840)\n",
      "2018-11-27T04:20:19.693016: step 1159, loss 1.46617, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 560/599 (start=35840, end=35904)\n",
      "2018-11-27T04:20:20.006859: step 1160, loss 1.78043, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 561/599 (start=35904, end=35968)\n",
      "2018-11-27T04:20:20.342647: step 1161, loss 1.78087, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 562/599 (start=35968, end=36032)\n",
      "2018-11-27T04:20:20.675373: step 1162, loss 1.80043, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 563/599 (start=36032, end=36096)\n",
      "2018-11-27T04:20:21.003170: step 1163, loss 1.56848, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 564/599 (start=36096, end=36160)\n",
      "2018-11-27T04:20:21.342810: step 1164, loss 1.74087, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 565/599 (start=36160, end=36224)\n",
      "2018-11-27T04:20:21.666431: step 1165, loss 1.66776, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 566/599 (start=36224, end=36288)\n",
      "2018-11-27T04:20:22.012367: step 1166, loss 1.81979, acc 0.421875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 567/599 (start=36288, end=36352)\n",
      "2018-11-27T04:20:22.324051: step 1167, loss 1.55761, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 568/599 (start=36352, end=36416)\n",
      "2018-11-27T04:20:22.668252: step 1168, loss 1.65704, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 569/599 (start=36416, end=36480)\n",
      "2018-11-27T04:20:23.011911: step 1169, loss 1.59542, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 570/599 (start=36480, end=36544)\n",
      "2018-11-27T04:20:23.364199: step 1170, loss 1.68757, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 571/599 (start=36544, end=36608)\n",
      "2018-11-27T04:20:23.712370: step 1171, loss 1.60959, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 572/599 (start=36608, end=36672)\n",
      "2018-11-27T04:20:24.062778: step 1172, loss 1.60511, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 573/599 (start=36672, end=36736)\n",
      "2018-11-27T04:20:24.396871: step 1173, loss 1.68961, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 574/599 (start=36736, end=36800)\n",
      "2018-11-27T04:20:24.734776: step 1174, loss 1.75215, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 575/599 (start=36800, end=36864)\n",
      "2018-11-27T04:20:25.084095: step 1175, loss 1.64378, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 576/599 (start=36864, end=36928)\n",
      "2018-11-27T04:20:25.405014: step 1176, loss 1.44075, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 577/599 (start=36928, end=36992)\n",
      "2018-11-27T04:20:25.725354: step 1177, loss 1.80142, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 578/599 (start=36992, end=37056)\n",
      "2018-11-27T04:20:26.069291: step 1178, loss 1.51687, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 579/599 (start=37056, end=37120)\n",
      "2018-11-27T04:20:26.417736: step 1179, loss 1.60485, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 580/599 (start=37120, end=37184)\n",
      "2018-11-27T04:20:26.769633: step 1180, loss 1.71812, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 581/599 (start=37184, end=37248)\n",
      "2018-11-27T04:20:27.104527: step 1181, loss 1.72849, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 582/599 (start=37248, end=37312)\n",
      "2018-11-27T04:20:27.453701: step 1182, loss 1.58557, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 583/599 (start=37312, end=37376)\n",
      "2018-11-27T04:20:27.786365: step 1183, loss 1.67025, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 584/599 (start=37376, end=37440)\n",
      "2018-11-27T04:20:28.126057: step 1184, loss 1.38398, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 585/599 (start=37440, end=37504)\n",
      "2018-11-27T04:20:28.443045: step 1185, loss 1.77767, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 586/599 (start=37504, end=37568)\n",
      "2018-11-27T04:20:28.777935: step 1186, loss 2.02174, acc 0.390625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 587/599 (start=37568, end=37632)\n",
      "2018-11-27T04:20:29.086160: step 1187, loss 2.02411, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 588/599 (start=37632, end=37696)\n",
      "2018-11-27T04:20:29.409208: step 1188, loss 1.5985, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 589/599 (start=37696, end=37760)\n",
      "2018-11-27T04:20:29.766966: step 1189, loss 1.80029, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 590/599 (start=37760, end=37824)\n",
      "2018-11-27T04:20:30.102213: step 1190, loss 1.57341, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 591/599 (start=37824, end=37888)\n",
      "2018-11-27T04:20:30.413905: step 1191, loss 1.68014, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 592/599 (start=37888, end=37952)\n",
      "2018-11-27T04:20:30.741980: step 1192, loss 1.58868, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 593/599 (start=37952, end=38016)\n",
      "2018-11-27T04:20:31.080372: step 1193, loss 1.53471, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 594/599 (start=38016, end=38080)\n",
      "2018-11-27T04:20:31.407371: step 1194, loss 1.62774, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 595/599 (start=38080, end=38144)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:20:31.770051: step 1195, loss 1.6822, acc 0.453125\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 596/599 (start=38144, end=38208)\n",
      "2018-11-27T04:20:32.131139: step 1196, loss 1.60544, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 597/599 (start=38208, end=38272)\n",
      "2018-11-27T04:20:32.484907: step 1197, loss 1.76297, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 1/10, Batch 598/599 (start=38272, end=38281)\n",
      "2018-11-27T04:20:32.699210: step 1198, loss 1.71933, acc 0.444444\n",
      "***********************************************\n",
      "Epoch 2/10\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 0/599 (start=0, end=64)\n",
      "2018-11-27T04:20:33.034413: step 1199, loss 1.41241, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 1/599 (start=64, end=128)\n",
      "2018-11-27T04:20:33.360084: step 1200, loss 1.34569, acc 0.65625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:20:39.660807: step 1200, loss 1.72753, acc 0.480527\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-1200\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 2/599 (start=128, end=192)\n",
      "2018-11-27T04:20:40.417556: step 1201, loss 1.39169, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 3/599 (start=192, end=256)\n",
      "2018-11-27T04:20:40.750341: step 1202, loss 1.44016, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 4/599 (start=256, end=320)\n",
      "2018-11-27T04:20:41.066233: step 1203, loss 1.23181, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 5/599 (start=320, end=384)\n",
      "2018-11-27T04:20:41.406010: step 1204, loss 1.57174, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 6/599 (start=384, end=448)\n",
      "2018-11-27T04:20:41.748782: step 1205, loss 1.29804, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 7/599 (start=448, end=512)\n",
      "2018-11-27T04:20:42.067150: step 1206, loss 1.40385, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 8/599 (start=512, end=576)\n",
      "2018-11-27T04:20:42.392773: step 1207, loss 1.33098, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 9/599 (start=576, end=640)\n",
      "2018-11-27T04:20:42.710633: step 1208, loss 1.16865, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 10/599 (start=640, end=704)\n",
      "2018-11-27T04:20:43.038485: step 1209, loss 1.31964, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 11/599 (start=704, end=768)\n",
      "2018-11-27T04:20:43.374563: step 1210, loss 1.26144, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 12/599 (start=768, end=832)\n",
      "2018-11-27T04:20:43.692319: step 1211, loss 1.31617, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 13/599 (start=832, end=896)\n",
      "2018-11-27T04:20:44.002872: step 1212, loss 1.35337, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 14/599 (start=896, end=960)\n",
      "2018-11-27T04:20:44.323761: step 1213, loss 1.40283, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 15/599 (start=960, end=1024)\n",
      "2018-11-27T04:20:44.659398: step 1214, loss 1.36547, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 16/599 (start=1024, end=1088)\n",
      "2018-11-27T04:20:45.008382: step 1215, loss 1.35378, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 17/599 (start=1088, end=1152)\n",
      "2018-11-27T04:20:45.355178: step 1216, loss 1.20094, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 18/599 (start=1152, end=1216)\n",
      "2018-11-27T04:20:45.691615: step 1217, loss 1.4298, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 19/599 (start=1216, end=1280)\n",
      "2018-11-27T04:20:46.029694: step 1218, loss 1.62093, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 20/599 (start=1280, end=1344)\n",
      "2018-11-27T04:20:46.379806: step 1219, loss 1.31366, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 21/599 (start=1344, end=1408)\n",
      "2018-11-27T04:20:46.722313: step 1220, loss 1.20748, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 22/599 (start=1408, end=1472)\n",
      "2018-11-27T04:20:47.031943: step 1221, loss 1.33785, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 23/599 (start=1472, end=1536)\n",
      "2018-11-27T04:20:47.367890: step 1222, loss 1.41534, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 24/599 (start=1536, end=1600)\n",
      "2018-11-27T04:20:47.714549: step 1223, loss 1.31954, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 25/599 (start=1600, end=1664)\n",
      "2018-11-27T04:20:48.024097: step 1224, loss 1.59116, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 26/599 (start=1664, end=1728)\n",
      "2018-11-27T04:20:48.354102: step 1225, loss 1.47705, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 27/599 (start=1728, end=1792)\n",
      "2018-11-27T04:20:48.693547: step 1226, loss 1.35858, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 28/599 (start=1792, end=1856)\n",
      "2018-11-27T04:20:49.033514: step 1227, loss 1.57376, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 29/599 (start=1856, end=1920)\n",
      "2018-11-27T04:20:49.372439: step 1228, loss 1.40273, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 30/599 (start=1920, end=1984)\n",
      "2018-11-27T04:20:49.702444: step 1229, loss 1.43914, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 31/599 (start=1984, end=2048)\n",
      "2018-11-27T04:20:50.028688: step 1230, loss 1.57978, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 32/599 (start=2048, end=2112)\n",
      "2018-11-27T04:20:50.347092: step 1231, loss 1.37151, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 33/599 (start=2112, end=2176)\n",
      "2018-11-27T04:20:50.675289: step 1232, loss 1.36449, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 34/599 (start=2176, end=2240)\n",
      "2018-11-27T04:20:51.031353: step 1233, loss 1.45386, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 35/599 (start=2240, end=2304)\n",
      "2018-11-27T04:20:51.367909: step 1234, loss 1.37634, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 36/599 (start=2304, end=2368)\n",
      "2018-11-27T04:20:51.684900: step 1235, loss 1.22919, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 37/599 (start=2368, end=2432)\n",
      "2018-11-27T04:20:52.023409: step 1236, loss 1.65171, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 38/599 (start=2432, end=2496)\n",
      "2018-11-27T04:20:52.345560: step 1237, loss 1.43788, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 39/599 (start=2496, end=2560)\n",
      "2018-11-27T04:20:52.664593: step 1238, loss 1.5976, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 40/599 (start=2560, end=2624)\n",
      "2018-11-27T04:20:52.974451: step 1239, loss 1.17951, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 41/599 (start=2624, end=2688)\n",
      "2018-11-27T04:20:53.315677: step 1240, loss 1.11828, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 42/599 (start=2688, end=2752)\n",
      "2018-11-27T04:20:53.648927: step 1241, loss 1.19494, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 43/599 (start=2752, end=2816)\n",
      "2018-11-27T04:20:53.991351: step 1242, loss 1.14804, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 44/599 (start=2816, end=2880)\n",
      "2018-11-27T04:20:54.293088: step 1243, loss 1.3523, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 45/599 (start=2880, end=2944)\n",
      "2018-11-27T04:20:54.641109: step 1244, loss 1.54136, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 46/599 (start=2944, end=3008)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:20:54.992064: step 1245, loss 1.33398, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 47/599 (start=3008, end=3072)\n",
      "2018-11-27T04:20:55.335019: step 1246, loss 1.16135, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 48/599 (start=3072, end=3136)\n",
      "2018-11-27T04:20:55.656959: step 1247, loss 1.43593, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 49/599 (start=3136, end=3200)\n",
      "2018-11-27T04:20:55.984561: step 1248, loss 1.27656, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 50/599 (start=3200, end=3264)\n",
      "2018-11-27T04:20:56.332716: step 1249, loss 1.14682, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 51/599 (start=3264, end=3328)\n",
      "2018-11-27T04:20:56.668800: step 1250, loss 1.18664, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 52/599 (start=3328, end=3392)\n",
      "2018-11-27T04:20:57.016833: step 1251, loss 1.2882, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 53/599 (start=3392, end=3456)\n",
      "2018-11-27T04:20:57.336629: step 1252, loss 1.18534, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 54/599 (start=3456, end=3520)\n",
      "2018-11-27T04:20:57.659754: step 1253, loss 1.39068, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 55/599 (start=3520, end=3584)\n",
      "2018-11-27T04:20:57.988572: step 1254, loss 1.19601, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 56/599 (start=3584, end=3648)\n",
      "2018-11-27T04:20:58.331401: step 1255, loss 1.43155, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 57/599 (start=3648, end=3712)\n",
      "2018-11-27T04:20:58.666560: step 1256, loss 1.26703, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 58/599 (start=3712, end=3776)\n",
      "2018-11-27T04:20:58.991519: step 1257, loss 1.19912, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 59/599 (start=3776, end=3840)\n",
      "2018-11-27T04:20:59.335581: step 1258, loss 1.50142, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 60/599 (start=3840, end=3904)\n",
      "2018-11-27T04:20:59.674522: step 1259, loss 1.13723, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 61/599 (start=3904, end=3968)\n",
      "2018-11-27T04:21:00.025502: step 1260, loss 1.26331, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 62/599 (start=3968, end=4032)\n",
      "2018-11-27T04:21:00.357273: step 1261, loss 1.48346, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 63/599 (start=4032, end=4096)\n",
      "2018-11-27T04:21:00.700342: step 1262, loss 1.44552, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 64/599 (start=4096, end=4160)\n",
      "2018-11-27T04:21:01.026533: step 1263, loss 1.37025, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 65/599 (start=4160, end=4224)\n",
      "2018-11-27T04:21:01.337789: step 1264, loss 1.284, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 66/599 (start=4224, end=4288)\n",
      "2018-11-27T04:21:01.676464: step 1265, loss 1.28509, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 67/599 (start=4288, end=4352)\n",
      "2018-11-27T04:21:02.018529: step 1266, loss 1.17643, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 68/599 (start=4352, end=4416)\n",
      "2018-11-27T04:21:02.363301: step 1267, loss 1.03141, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 69/599 (start=4416, end=4480)\n",
      "2018-11-27T04:21:02.707842: step 1268, loss 1.21575, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 70/599 (start=4480, end=4544)\n",
      "2018-11-27T04:21:03.054720: step 1269, loss 1.64989, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 71/599 (start=4544, end=4608)\n",
      "2018-11-27T04:21:03.371797: step 1270, loss 1.27974, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 72/599 (start=4608, end=4672)\n",
      "2018-11-27T04:21:03.704443: step 1271, loss 1.49289, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 73/599 (start=4672, end=4736)\n",
      "2018-11-27T04:21:04.039557: step 1272, loss 1.20419, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 74/599 (start=4736, end=4800)\n",
      "2018-11-27T04:21:04.355917: step 1273, loss 1.33256, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 75/599 (start=4800, end=4864)\n",
      "2018-11-27T04:21:04.701453: step 1274, loss 1.24296, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 76/599 (start=4864, end=4928)\n",
      "2018-11-27T04:21:05.016330: step 1275, loss 1.26286, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 77/599 (start=4928, end=4992)\n",
      "2018-11-27T04:21:05.351187: step 1276, loss 1.21276, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 78/599 (start=4992, end=5056)\n",
      "2018-11-27T04:21:05.662362: step 1277, loss 1.46921, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 79/599 (start=5056, end=5120)\n",
      "2018-11-27T04:21:06.000816: step 1278, loss 1.4545, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 80/599 (start=5120, end=5184)\n",
      "2018-11-27T04:21:06.314472: step 1279, loss 1.52422, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 81/599 (start=5184, end=5248)\n",
      "2018-11-27T04:21:06.638867: step 1280, loss 1.56001, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 82/599 (start=5248, end=5312)\n",
      "2018-11-27T04:21:06.985518: step 1281, loss 1.45754, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 83/599 (start=5312, end=5376)\n",
      "2018-11-27T04:21:07.303451: step 1282, loss 1.34338, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 84/599 (start=5376, end=5440)\n",
      "2018-11-27T04:21:07.638645: step 1283, loss 1.4159, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 85/599 (start=5440, end=5504)\n",
      "2018-11-27T04:21:07.980450: step 1284, loss 1.17927, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 86/599 (start=5504, end=5568)\n",
      "2018-11-27T04:21:08.323117: step 1285, loss 1.34578, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 87/599 (start=5568, end=5632)\n",
      "2018-11-27T04:21:08.653819: step 1286, loss 1.35917, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 88/599 (start=5632, end=5696)\n",
      "2018-11-27T04:21:09.000725: step 1287, loss 1.20764, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 89/599 (start=5696, end=5760)\n",
      "2018-11-27T04:21:09.339981: step 1288, loss 1.32475, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 90/599 (start=5760, end=5824)\n",
      "2018-11-27T04:21:09.652198: step 1289, loss 1.11916, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 91/599 (start=5824, end=5888)\n",
      "2018-11-27T04:21:09.992921: step 1290, loss 1.5279, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 92/599 (start=5888, end=5952)\n",
      "2018-11-27T04:21:10.338382: step 1291, loss 1.47819, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 93/599 (start=5952, end=6016)\n",
      "2018-11-27T04:21:10.669478: step 1292, loss 1.32022, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 94/599 (start=6016, end=6080)\n",
      "2018-11-27T04:21:11.004017: step 1293, loss 1.54739, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 95/599 (start=6080, end=6144)\n",
      "2018-11-27T04:21:11.348937: step 1294, loss 1.40873, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 96/599 (start=6144, end=6208)\n",
      "2018-11-27T04:21:11.668626: step 1295, loss 1.39502, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 97/599 (start=6208, end=6272)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:21:12.005121: step 1296, loss 1.13136, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 98/599 (start=6272, end=6336)\n",
      "2018-11-27T04:21:12.314129: step 1297, loss 1.32807, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 99/599 (start=6336, end=6400)\n",
      "2018-11-27T04:21:12.629708: step 1298, loss 1.22817, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 100/599 (start=6400, end=6464)\n",
      "2018-11-27T04:21:12.973956: step 1299, loss 1.22509, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 101/599 (start=6464, end=6528)\n",
      "2018-11-27T04:21:13.309558: step 1300, loss 1.33829, acc 0.625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:21:19.539768: step 1300, loss 1.73535, acc 0.47747\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-1300\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 102/599 (start=6528, end=6592)\n",
      "2018-11-27T04:21:20.303639: step 1301, loss 1.61389, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 103/599 (start=6592, end=6656)\n",
      "2018-11-27T04:21:20.616460: step 1302, loss 1.13859, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 104/599 (start=6656, end=6720)\n",
      "2018-11-27T04:21:20.956708: step 1303, loss 1.56588, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 105/599 (start=6720, end=6784)\n",
      "2018-11-27T04:21:21.290707: step 1304, loss 1.31829, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 106/599 (start=6784, end=6848)\n",
      "2018-11-27T04:21:21.642348: step 1305, loss 1.5699, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 107/599 (start=6848, end=6912)\n",
      "2018-11-27T04:21:21.981438: step 1306, loss 1.71373, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 108/599 (start=6912, end=6976)\n",
      "2018-11-27T04:21:22.321867: step 1307, loss 1.2603, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 109/599 (start=6976, end=7040)\n",
      "2018-11-27T04:21:22.652007: step 1308, loss 1.31156, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 110/599 (start=7040, end=7104)\n",
      "2018-11-27T04:21:22.985672: step 1309, loss 1.61307, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 111/599 (start=7104, end=7168)\n",
      "2018-11-27T04:21:23.325068: step 1310, loss 1.15377, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 112/599 (start=7168, end=7232)\n",
      "2018-11-27T04:21:23.665359: step 1311, loss 1.40308, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 113/599 (start=7232, end=7296)\n",
      "2018-11-27T04:21:24.012105: step 1312, loss 1.29976, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 114/599 (start=7296, end=7360)\n",
      "2018-11-27T04:21:24.327708: step 1313, loss 1.34445, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 115/599 (start=7360, end=7424)\n",
      "2018-11-27T04:21:24.675884: step 1314, loss 1.43442, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 116/599 (start=7424, end=7488)\n",
      "2018-11-27T04:21:25.003992: step 1315, loss 1.32023, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 117/599 (start=7488, end=7552)\n",
      "2018-11-27T04:21:25.331646: step 1316, loss 1.50994, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 118/599 (start=7552, end=7616)\n",
      "2018-11-27T04:21:25.647020: step 1317, loss 1.41564, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 119/599 (start=7616, end=7680)\n",
      "2018-11-27T04:21:26.009883: step 1318, loss 1.27986, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 120/599 (start=7680, end=7744)\n",
      "2018-11-27T04:21:26.326070: step 1319, loss 1.28265, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 121/599 (start=7744, end=7808)\n",
      "2018-11-27T04:21:26.656037: step 1320, loss 1.22643, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 122/599 (start=7808, end=7872)\n",
      "2018-11-27T04:21:26.972591: step 1321, loss 1.25797, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 123/599 (start=7872, end=7936)\n",
      "2018-11-27T04:21:27.299138: step 1322, loss 1.17086, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 124/599 (start=7936, end=8000)\n",
      "2018-11-27T04:21:27.641838: step 1323, loss 1.37657, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 125/599 (start=8000, end=8064)\n",
      "2018-11-27T04:21:27.974990: step 1324, loss 1.20683, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 126/599 (start=8064, end=8128)\n",
      "2018-11-27T04:21:28.323040: step 1325, loss 1.13269, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 127/599 (start=8128, end=8192)\n",
      "2018-11-27T04:21:28.637741: step 1326, loss 1.35331, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 128/599 (start=8192, end=8256)\n",
      "2018-11-27T04:21:28.943060: step 1327, loss 1.55507, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 129/599 (start=8256, end=8320)\n",
      "2018-11-27T04:21:29.250058: step 1328, loss 1.45073, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 130/599 (start=8320, end=8384)\n",
      "2018-11-27T04:21:29.585884: step 1329, loss 1.31793, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 131/599 (start=8384, end=8448)\n",
      "2018-11-27T04:21:29.908996: step 1330, loss 1.53816, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 132/599 (start=8448, end=8512)\n",
      "2018-11-27T04:21:30.218161: step 1331, loss 1.33111, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 133/599 (start=8512, end=8576)\n",
      "2018-11-27T04:21:30.565400: step 1332, loss 1.55536, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 134/599 (start=8576, end=8640)\n",
      "2018-11-27T04:21:30.902716: step 1333, loss 1.28741, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 135/599 (start=8640, end=8704)\n",
      "2018-11-27T04:21:31.252935: step 1334, loss 1.34214, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 136/599 (start=8704, end=8768)\n",
      "2018-11-27T04:21:31.571459: step 1335, loss 1.44626, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 137/599 (start=8768, end=8832)\n",
      "2018-11-27T04:21:31.896697: step 1336, loss 1.40157, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 138/599 (start=8832, end=8896)\n",
      "2018-11-27T04:21:32.244437: step 1337, loss 1.35406, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 139/599 (start=8896, end=8960)\n",
      "2018-11-27T04:21:32.589828: step 1338, loss 1.1098, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 140/599 (start=8960, end=9024)\n",
      "2018-11-27T04:21:32.904098: step 1339, loss 1.58902, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 141/599 (start=9024, end=9088)\n",
      "2018-11-27T04:21:33.231901: step 1340, loss 1.34897, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 142/599 (start=9088, end=9152)\n",
      "2018-11-27T04:21:33.559425: step 1341, loss 1.03416, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 143/599 (start=9152, end=9216)\n",
      "2018-11-27T04:21:33.874800: step 1342, loss 1.23313, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 144/599 (start=9216, end=9280)\n",
      "2018-11-27T04:21:34.213684: step 1343, loss 1.2802, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 145/599 (start=9280, end=9344)\n",
      "2018-11-27T04:21:34.554860: step 1344, loss 1.47682, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 146/599 (start=9344, end=9408)\n",
      "2018-11-27T04:21:34.864978: step 1345, loss 1.31534, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 147/599 (start=9408, end=9472)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:21:35.191471: step 1346, loss 1.48321, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 148/599 (start=9472, end=9536)\n",
      "2018-11-27T04:21:35.515571: step 1347, loss 1.11997, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 149/599 (start=9536, end=9600)\n",
      "2018-11-27T04:21:35.836606: step 1348, loss 1.4429, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 150/599 (start=9600, end=9664)\n",
      "2018-11-27T04:21:36.182527: step 1349, loss 1.29751, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 151/599 (start=9664, end=9728)\n",
      "2018-11-27T04:21:36.514356: step 1350, loss 1.27731, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 152/599 (start=9728, end=9792)\n",
      "2018-11-27T04:21:36.851438: step 1351, loss 1.52714, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 153/599 (start=9792, end=9856)\n",
      "2018-11-27T04:21:37.173769: step 1352, loss 1.26951, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 154/599 (start=9856, end=9920)\n",
      "2018-11-27T04:21:37.485337: step 1353, loss 1.13012, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 155/599 (start=9920, end=9984)\n",
      "2018-11-27T04:21:37.833485: step 1354, loss 1.29995, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 156/599 (start=9984, end=10048)\n",
      "2018-11-27T04:21:38.164038: step 1355, loss 1.44534, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 157/599 (start=10048, end=10112)\n",
      "2018-11-27T04:21:38.495458: step 1356, loss 1.18757, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 158/599 (start=10112, end=10176)\n",
      "2018-11-27T04:21:38.834565: step 1357, loss 1.62749, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 159/599 (start=10176, end=10240)\n",
      "2018-11-27T04:21:39.175247: step 1358, loss 1.39553, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 160/599 (start=10240, end=10304)\n",
      "2018-11-27T04:21:39.520333: step 1359, loss 1.43793, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 161/599 (start=10304, end=10368)\n",
      "2018-11-27T04:21:39.848537: step 1360, loss 1.22574, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 162/599 (start=10368, end=10432)\n",
      "2018-11-27T04:21:40.190105: step 1361, loss 1.12153, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 163/599 (start=10432, end=10496)\n",
      "2018-11-27T04:21:40.517249: step 1362, loss 1.34534, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 164/599 (start=10496, end=10560)\n",
      "2018-11-27T04:21:40.854304: step 1363, loss 1.32676, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 165/599 (start=10560, end=10624)\n",
      "2018-11-27T04:21:41.182980: step 1364, loss 1.19183, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 166/599 (start=10624, end=10688)\n",
      "2018-11-27T04:21:41.533941: step 1365, loss 1.63363, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 167/599 (start=10688, end=10752)\n",
      "2018-11-27T04:21:41.896174: step 1366, loss 1.27985, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 168/599 (start=10752, end=10816)\n",
      "2018-11-27T04:21:42.225316: step 1367, loss 1.08943, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 169/599 (start=10816, end=10880)\n",
      "2018-11-27T04:21:42.562915: step 1368, loss 1.38012, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 170/599 (start=10880, end=10944)\n",
      "2018-11-27T04:21:42.918478: step 1369, loss 1.36565, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 171/599 (start=10944, end=11008)\n",
      "2018-11-27T04:21:43.259720: step 1370, loss 1.52268, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 172/599 (start=11008, end=11072)\n",
      "2018-11-27T04:21:43.575627: step 1371, loss 1.48822, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 173/599 (start=11072, end=11136)\n",
      "2018-11-27T04:21:43.908084: step 1372, loss 1.54321, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 174/599 (start=11136, end=11200)\n",
      "2018-11-27T04:21:44.248801: step 1373, loss 1.40849, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 175/599 (start=11200, end=11264)\n",
      "2018-11-27T04:21:44.593807: step 1374, loss 1.35133, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 176/599 (start=11264, end=11328)\n",
      "2018-11-27T04:21:44.931849: step 1375, loss 1.45721, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 177/599 (start=11328, end=11392)\n",
      "2018-11-27T04:21:45.273933: step 1376, loss 1.40133, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 178/599 (start=11392, end=11456)\n",
      "2018-11-27T04:21:45.595312: step 1377, loss 1.16703, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 179/599 (start=11456, end=11520)\n",
      "2018-11-27T04:21:45.941349: step 1378, loss 1.59044, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 180/599 (start=11520, end=11584)\n",
      "2018-11-27T04:21:46.295384: step 1379, loss 1.39802, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 181/599 (start=11584, end=11648)\n",
      "2018-11-27T04:21:46.633846: step 1380, loss 1.22811, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 182/599 (start=11648, end=11712)\n",
      "2018-11-27T04:21:46.961004: step 1381, loss 1.40511, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 183/599 (start=11712, end=11776)\n",
      "2018-11-27T04:21:47.293934: step 1382, loss 1.3806, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 184/599 (start=11776, end=11840)\n",
      "2018-11-27T04:21:47.610041: step 1383, loss 1.22436, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 185/599 (start=11840, end=11904)\n",
      "2018-11-27T04:21:47.957220: step 1384, loss 1.42228, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 186/599 (start=11904, end=11968)\n",
      "2018-11-27T04:21:48.303217: step 1385, loss 1.48544, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 187/599 (start=11968, end=12032)\n",
      "2018-11-27T04:21:48.621508: step 1386, loss 1.11802, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 188/599 (start=12032, end=12096)\n",
      "2018-11-27T04:21:48.952421: step 1387, loss 1.55337, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 189/599 (start=12096, end=12160)\n",
      "2018-11-27T04:21:49.271395: step 1388, loss 1.6355, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 190/599 (start=12160, end=12224)\n",
      "2018-11-27T04:21:49.605346: step 1389, loss 1.17298, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 191/599 (start=12224, end=12288)\n",
      "2018-11-27T04:21:49.952888: step 1390, loss 1.14214, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 192/599 (start=12288, end=12352)\n",
      "2018-11-27T04:21:50.273472: step 1391, loss 1.26756, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 193/599 (start=12352, end=12416)\n",
      "2018-11-27T04:21:50.615850: step 1392, loss 1.4759, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 194/599 (start=12416, end=12480)\n",
      "2018-11-27T04:21:50.954633: step 1393, loss 1.33875, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 195/599 (start=12480, end=12544)\n",
      "2018-11-27T04:21:51.288578: step 1394, loss 1.35413, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 196/599 (start=12544, end=12608)\n",
      "2018-11-27T04:21:51.625399: step 1395, loss 1.357, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 197/599 (start=12608, end=12672)\n",
      "2018-11-27T04:21:51.940032: step 1396, loss 1.22471, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 198/599 (start=12672, end=12736)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:21:52.285974: step 1397, loss 1.78734, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 199/599 (start=12736, end=12800)\n",
      "2018-11-27T04:21:52.629385: step 1398, loss 1.45915, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 200/599 (start=12800, end=12864)\n",
      "2018-11-27T04:21:52.963178: step 1399, loss 1.47286, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 201/599 (start=12864, end=12928)\n",
      "2018-11-27T04:21:53.317347: step 1400, loss 1.34774, acc 0.578125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:21:59.568720: step 1400, loss 1.72454, acc 0.486717\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-1400\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 202/599 (start=12928, end=12992)\n",
      "2018-11-27T04:22:00.291549: step 1401, loss 1.12527, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 203/599 (start=12992, end=13056)\n",
      "2018-11-27T04:22:00.649579: step 1402, loss 1.48744, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 204/599 (start=13056, end=13120)\n",
      "2018-11-27T04:22:00.998013: step 1403, loss 1.51911, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 205/599 (start=13120, end=13184)\n",
      "2018-11-27T04:22:01.347476: step 1404, loss 1.27173, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 206/599 (start=13184, end=13248)\n",
      "2018-11-27T04:22:01.658897: step 1405, loss 1.28563, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 207/599 (start=13248, end=13312)\n",
      "2018-11-27T04:22:01.994581: step 1406, loss 1.30174, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 208/599 (start=13312, end=13376)\n",
      "2018-11-27T04:22:02.313751: step 1407, loss 1.32728, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 209/599 (start=13376, end=13440)\n",
      "2018-11-27T04:22:02.644415: step 1408, loss 1.44428, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 210/599 (start=13440, end=13504)\n",
      "2018-11-27T04:22:02.953864: step 1409, loss 1.38726, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 211/599 (start=13504, end=13568)\n",
      "2018-11-27T04:22:03.295422: step 1410, loss 1.41752, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 212/599 (start=13568, end=13632)\n",
      "2018-11-27T04:22:03.645307: step 1411, loss 1.25005, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 213/599 (start=13632, end=13696)\n",
      "2018-11-27T04:22:03.969455: step 1412, loss 1.22273, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 214/599 (start=13696, end=13760)\n",
      "2018-11-27T04:22:04.291458: step 1413, loss 1.51062, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 215/599 (start=13760, end=13824)\n",
      "2018-11-27T04:22:04.603890: step 1414, loss 1.38996, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 216/599 (start=13824, end=13888)\n",
      "2018-11-27T04:22:04.955781: step 1415, loss 1.24635, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 217/599 (start=13888, end=13952)\n",
      "2018-11-27T04:22:05.268716: step 1416, loss 1.78559, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 218/599 (start=13952, end=14016)\n",
      "2018-11-27T04:22:05.604439: step 1417, loss 1.17295, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 219/599 (start=14016, end=14080)\n",
      "2018-11-27T04:22:05.947155: step 1418, loss 1.18914, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 220/599 (start=14080, end=14144)\n",
      "2018-11-27T04:22:06.268921: step 1419, loss 1.41095, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 221/599 (start=14144, end=14208)\n",
      "2018-11-27T04:22:06.581871: step 1420, loss 1.27769, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 222/599 (start=14208, end=14272)\n",
      "2018-11-27T04:22:06.920322: step 1421, loss 1.4446, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 223/599 (start=14272, end=14336)\n",
      "2018-11-27T04:22:07.261627: step 1422, loss 1.45947, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 224/599 (start=14336, end=14400)\n",
      "2018-11-27T04:22:07.596262: step 1423, loss 1.26511, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 225/599 (start=14400, end=14464)\n",
      "2018-11-27T04:22:07.946266: step 1424, loss 1.34085, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 226/599 (start=14464, end=14528)\n",
      "2018-11-27T04:22:08.298894: step 1425, loss 1.23064, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 227/599 (start=14528, end=14592)\n",
      "2018-11-27T04:22:08.647261: step 1426, loss 1.28267, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 228/599 (start=14592, end=14656)\n",
      "2018-11-27T04:22:08.986083: step 1427, loss 1.29341, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 229/599 (start=14656, end=14720)\n",
      "2018-11-27T04:22:09.329983: step 1428, loss 1.19159, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 230/599 (start=14720, end=14784)\n",
      "2018-11-27T04:22:09.648956: step 1429, loss 1.48499, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 231/599 (start=14784, end=14848)\n",
      "2018-11-27T04:22:09.966408: step 1430, loss 1.13469, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 232/599 (start=14848, end=14912)\n",
      "2018-11-27T04:22:10.303988: step 1431, loss 1.48544, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 233/599 (start=14912, end=14976)\n",
      "2018-11-27T04:22:10.629400: step 1432, loss 1.31566, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 234/599 (start=14976, end=15040)\n",
      "2018-11-27T04:22:10.946168: step 1433, loss 1.3095, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 235/599 (start=15040, end=15104)\n",
      "2018-11-27T04:22:11.255971: step 1434, loss 1.43483, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 236/599 (start=15104, end=15168)\n",
      "2018-11-27T04:22:11.571732: step 1435, loss 1.45311, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 237/599 (start=15168, end=15232)\n",
      "2018-11-27T04:22:11.894842: step 1436, loss 1.17856, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 238/599 (start=15232, end=15296)\n",
      "2018-11-27T04:22:12.203731: step 1437, loss 1.13021, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 239/599 (start=15296, end=15360)\n",
      "2018-11-27T04:22:12.523129: step 1438, loss 1.13854, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 240/599 (start=15360, end=15424)\n",
      "2018-11-27T04:22:12.841078: step 1439, loss 0.982828, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 241/599 (start=15424, end=15488)\n",
      "2018-11-27T04:22:13.157987: step 1440, loss 1.43636, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 242/599 (start=15488, end=15552)\n",
      "2018-11-27T04:22:13.481175: step 1441, loss 1.20828, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 243/599 (start=15552, end=15616)\n",
      "2018-11-27T04:22:13.793669: step 1442, loss 1.33596, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 244/599 (start=15616, end=15680)\n",
      "2018-11-27T04:22:14.111698: step 1443, loss 1.19472, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 245/599 (start=15680, end=15744)\n",
      "2018-11-27T04:22:14.446460: step 1444, loss 1.34193, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 246/599 (start=15744, end=15808)\n",
      "2018-11-27T04:22:14.783556: step 1445, loss 1.48377, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 247/599 (start=15808, end=15872)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:22:15.103259: step 1446, loss 1.14031, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 248/599 (start=15872, end=15936)\n",
      "2018-11-27T04:22:15.438050: step 1447, loss 1.28577, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 249/599 (start=15936, end=16000)\n",
      "2018-11-27T04:22:15.766245: step 1448, loss 1.11324, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 250/599 (start=16000, end=16064)\n",
      "2018-11-27T04:22:16.094370: step 1449, loss 1.50144, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 251/599 (start=16064, end=16128)\n",
      "2018-11-27T04:22:16.440009: step 1450, loss 1.2893, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 252/599 (start=16128, end=16192)\n",
      "2018-11-27T04:22:16.769070: step 1451, loss 1.31819, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 253/599 (start=16192, end=16256)\n",
      "2018-11-27T04:22:17.082823: step 1452, loss 1.29897, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 254/599 (start=16256, end=16320)\n",
      "2018-11-27T04:22:17.398123: step 1453, loss 1.48968, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 255/599 (start=16320, end=16384)\n",
      "2018-11-27T04:22:17.749934: step 1454, loss 1.28493, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 256/599 (start=16384, end=16448)\n",
      "2018-11-27T04:22:18.084676: step 1455, loss 1.46819, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 257/599 (start=16448, end=16512)\n",
      "2018-11-27T04:22:18.420503: step 1456, loss 1.47761, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 258/599 (start=16512, end=16576)\n",
      "2018-11-27T04:22:18.758119: step 1457, loss 1.66784, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 259/599 (start=16576, end=16640)\n",
      "2018-11-27T04:22:19.101728: step 1458, loss 1.59787, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 260/599 (start=16640, end=16704)\n",
      "2018-11-27T04:22:19.425806: step 1459, loss 1.26014, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 261/599 (start=16704, end=16768)\n",
      "2018-11-27T04:22:19.744335: step 1460, loss 1.40702, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 262/599 (start=16768, end=16832)\n",
      "2018-11-27T04:22:20.089612: step 1461, loss 1.28491, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 263/599 (start=16832, end=16896)\n",
      "2018-11-27T04:22:20.429557: step 1462, loss 1.30014, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 264/599 (start=16896, end=16960)\n",
      "2018-11-27T04:22:20.742390: step 1463, loss 1.24997, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 265/599 (start=16960, end=17024)\n",
      "2018-11-27T04:22:21.050550: step 1464, loss 1.55464, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 266/599 (start=17024, end=17088)\n",
      "2018-11-27T04:22:21.373462: step 1465, loss 1.39371, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 267/599 (start=17088, end=17152)\n",
      "2018-11-27T04:22:21.705780: step 1466, loss 1.24751, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 268/599 (start=17152, end=17216)\n",
      "2018-11-27T04:22:22.040799: step 1467, loss 1.30197, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 269/599 (start=17216, end=17280)\n",
      "2018-11-27T04:22:22.387736: step 1468, loss 1.22411, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 270/599 (start=17280, end=17344)\n",
      "2018-11-27T04:22:22.726970: step 1469, loss 1.45639, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 271/599 (start=17344, end=17408)\n",
      "2018-11-27T04:22:23.057422: step 1470, loss 1.42178, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 272/599 (start=17408, end=17472)\n",
      "2018-11-27T04:22:23.397819: step 1471, loss 1.22865, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 273/599 (start=17472, end=17536)\n",
      "2018-11-27T04:22:23.734049: step 1472, loss 1.47411, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 274/599 (start=17536, end=17600)\n",
      "2018-11-27T04:22:24.074032: step 1473, loss 1.4202, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 275/599 (start=17600, end=17664)\n",
      "2018-11-27T04:22:24.407655: step 1474, loss 1.40167, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 276/599 (start=17664, end=17728)\n",
      "2018-11-27T04:22:24.723988: step 1475, loss 1.4644, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 277/599 (start=17728, end=17792)\n",
      "2018-11-27T04:22:25.038423: step 1476, loss 1.28488, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 278/599 (start=17792, end=17856)\n",
      "2018-11-27T04:22:25.362228: step 1477, loss 1.26074, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 279/599 (start=17856, end=17920)\n",
      "2018-11-27T04:22:25.680949: step 1478, loss 1.48707, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 280/599 (start=17920, end=17984)\n",
      "2018-11-27T04:22:26.020753: step 1479, loss 1.3471, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 281/599 (start=17984, end=18048)\n",
      "2018-11-27T04:22:26.380587: step 1480, loss 1.42882, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 282/599 (start=18048, end=18112)\n",
      "2018-11-27T04:22:26.717568: step 1481, loss 1.74915, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 283/599 (start=18112, end=18176)\n",
      "2018-11-27T04:22:27.051489: step 1482, loss 1.47883, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 284/599 (start=18176, end=18240)\n",
      "2018-11-27T04:22:27.378888: step 1483, loss 1.34319, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 285/599 (start=18240, end=18304)\n",
      "2018-11-27T04:22:27.712696: step 1484, loss 1.46056, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 286/599 (start=18304, end=18368)\n",
      "2018-11-27T04:22:28.042870: step 1485, loss 1.27494, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 287/599 (start=18368, end=18432)\n",
      "2018-11-27T04:22:28.361596: step 1486, loss 1.29526, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 288/599 (start=18432, end=18496)\n",
      "2018-11-27T04:22:28.691268: step 1487, loss 1.61011, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 289/599 (start=18496, end=18560)\n",
      "2018-11-27T04:22:29.002736: step 1488, loss 1.28857, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 290/599 (start=18560, end=18624)\n",
      "2018-11-27T04:22:29.338000: step 1489, loss 1.5333, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 291/599 (start=18624, end=18688)\n",
      "2018-11-27T04:22:29.662652: step 1490, loss 1.4345, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 292/599 (start=18688, end=18752)\n",
      "2018-11-27T04:22:29.982226: step 1491, loss 1.38466, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 293/599 (start=18752, end=18816)\n",
      "2018-11-27T04:22:30.319044: step 1492, loss 1.23239, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 294/599 (start=18816, end=18880)\n",
      "2018-11-27T04:22:30.645617: step 1493, loss 1.32418, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 295/599 (start=18880, end=18944)\n",
      "2018-11-27T04:22:30.973969: step 1494, loss 1.55716, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 296/599 (start=18944, end=19008)\n",
      "2018-11-27T04:22:31.318365: step 1495, loss 1.1163, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 297/599 (start=19008, end=19072)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:22:31.674789: step 1496, loss 1.26074, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 298/599 (start=19072, end=19136)\n",
      "2018-11-27T04:22:31.989540: step 1497, loss 1.26392, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 299/599 (start=19136, end=19200)\n",
      "2018-11-27T04:22:32.328180: step 1498, loss 1.34799, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 300/599 (start=19200, end=19264)\n",
      "2018-11-27T04:22:32.665944: step 1499, loss 1.41134, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 301/599 (start=19264, end=19328)\n",
      "2018-11-27T04:22:33.008413: step 1500, loss 1.55991, acc 0.53125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:22:39.472794: step 1500, loss 1.71977, acc 0.49424\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-1500\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 302/599 (start=19328, end=19392)\n",
      "2018-11-27T04:22:40.224376: step 1501, loss 1.4509, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 303/599 (start=19392, end=19456)\n",
      "2018-11-27T04:22:40.551171: step 1502, loss 1.51596, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 304/599 (start=19456, end=19520)\n",
      "2018-11-27T04:22:40.892083: step 1503, loss 1.25427, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 305/599 (start=19520, end=19584)\n",
      "2018-11-27T04:22:41.211246: step 1504, loss 1.25394, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 306/599 (start=19584, end=19648)\n",
      "2018-11-27T04:22:41.525381: step 1505, loss 1.22075, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 307/599 (start=19648, end=19712)\n",
      "2018-11-27T04:22:41.840812: step 1506, loss 1.34698, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 308/599 (start=19712, end=19776)\n",
      "2018-11-27T04:22:42.166002: step 1507, loss 1.29796, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 309/599 (start=19776, end=19840)\n",
      "2018-11-27T04:22:42.481953: step 1508, loss 1.60403, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 310/599 (start=19840, end=19904)\n",
      "2018-11-27T04:22:42.828864: step 1509, loss 1.38025, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 311/599 (start=19904, end=19968)\n",
      "2018-11-27T04:22:43.167395: step 1510, loss 1.50382, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 312/599 (start=19968, end=20032)\n",
      "2018-11-27T04:22:43.490597: step 1511, loss 1.50105, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 313/599 (start=20032, end=20096)\n",
      "2018-11-27T04:22:43.805520: step 1512, loss 1.22212, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 314/599 (start=20096, end=20160)\n",
      "2018-11-27T04:22:44.151056: step 1513, loss 1.28954, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 315/599 (start=20160, end=20224)\n",
      "2018-11-27T04:22:44.470733: step 1514, loss 1.24625, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 316/599 (start=20224, end=20288)\n",
      "2018-11-27T04:22:44.792097: step 1515, loss 1.27366, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 317/599 (start=20288, end=20352)\n",
      "2018-11-27T04:22:45.135970: step 1516, loss 1.2098, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 318/599 (start=20352, end=20416)\n",
      "2018-11-27T04:22:45.445676: step 1517, loss 1.23935, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 319/599 (start=20416, end=20480)\n",
      "2018-11-27T04:22:45.784488: step 1518, loss 1.52576, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 320/599 (start=20480, end=20544)\n",
      "2018-11-27T04:22:46.119956: step 1519, loss 1.65462, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 321/599 (start=20544, end=20608)\n",
      "2018-11-27T04:22:46.464703: step 1520, loss 1.14909, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 322/599 (start=20608, end=20672)\n",
      "2018-11-27T04:22:46.811591: step 1521, loss 1.20902, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 323/599 (start=20672, end=20736)\n",
      "2018-11-27T04:22:47.137466: step 1522, loss 1.41114, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 324/599 (start=20736, end=20800)\n",
      "2018-11-27T04:22:47.470683: step 1523, loss 1.55295, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 325/599 (start=20800, end=20864)\n",
      "2018-11-27T04:22:47.788216: step 1524, loss 1.56172, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 326/599 (start=20864, end=20928)\n",
      "2018-11-27T04:22:48.129310: step 1525, loss 1.41617, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 327/599 (start=20928, end=20992)\n",
      "2018-11-27T04:22:48.449453: step 1526, loss 1.23577, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 328/599 (start=20992, end=21056)\n",
      "2018-11-27T04:22:48.768268: step 1527, loss 1.23501, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 329/599 (start=21056, end=21120)\n",
      "2018-11-27T04:22:49.105715: step 1528, loss 1.18231, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 330/599 (start=21120, end=21184)\n",
      "2018-11-27T04:22:49.442445: step 1529, loss 1.41561, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 331/599 (start=21184, end=21248)\n",
      "2018-11-27T04:22:49.766761: step 1530, loss 1.13609, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 332/599 (start=21248, end=21312)\n",
      "2018-11-27T04:22:50.088709: step 1531, loss 1.31578, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 333/599 (start=21312, end=21376)\n",
      "2018-11-27T04:22:50.433019: step 1532, loss 1.29535, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 334/599 (start=21376, end=21440)\n",
      "2018-11-27T04:22:50.761364: step 1533, loss 1.42616, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 335/599 (start=21440, end=21504)\n",
      "2018-11-27T04:22:51.095903: step 1534, loss 1.35739, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 336/599 (start=21504, end=21568)\n",
      "2018-11-27T04:22:51.430175: step 1535, loss 1.47038, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 337/599 (start=21568, end=21632)\n",
      "2018-11-27T04:22:51.765985: step 1536, loss 1.43481, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 338/599 (start=21632, end=21696)\n",
      "2018-11-27T04:22:52.103214: step 1537, loss 1.51148, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 339/599 (start=21696, end=21760)\n",
      "2018-11-27T04:22:52.460160: step 1538, loss 1.19462, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 340/599 (start=21760, end=21824)\n",
      "2018-11-27T04:22:52.801802: step 1539, loss 1.22625, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 341/599 (start=21824, end=21888)\n",
      "2018-11-27T04:22:53.115978: step 1540, loss 1.48123, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 342/599 (start=21888, end=21952)\n",
      "2018-11-27T04:22:53.446765: step 1541, loss 1.65416, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 343/599 (start=21952, end=22016)\n",
      "2018-11-27T04:22:53.763135: step 1542, loss 1.52459, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 344/599 (start=22016, end=22080)\n",
      "2018-11-27T04:22:54.085247: step 1543, loss 1.31585, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 345/599 (start=22080, end=22144)\n",
      "2018-11-27T04:22:54.397885: step 1544, loss 1.21733, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 346/599 (start=22144, end=22208)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:22:54.705291: step 1545, loss 1.28005, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 347/599 (start=22208, end=22272)\n",
      "2018-11-27T04:22:55.024260: step 1546, loss 1.44251, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 348/599 (start=22272, end=22336)\n",
      "2018-11-27T04:22:55.369108: step 1547, loss 1.20825, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 349/599 (start=22336, end=22400)\n",
      "2018-11-27T04:22:55.686557: step 1548, loss 1.2881, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 350/599 (start=22400, end=22464)\n",
      "2018-11-27T04:22:56.008935: step 1549, loss 1.26987, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 351/599 (start=22464, end=22528)\n",
      "2018-11-27T04:22:56.340540: step 1550, loss 1.37761, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 352/599 (start=22528, end=22592)\n",
      "2018-11-27T04:22:56.675297: step 1551, loss 1.23572, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 353/599 (start=22592, end=22656)\n",
      "2018-11-27T04:22:57.012332: step 1552, loss 1.28368, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 354/599 (start=22656, end=22720)\n",
      "2018-11-27T04:22:57.360404: step 1553, loss 1.25771, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 355/599 (start=22720, end=22784)\n",
      "2018-11-27T04:22:57.694563: step 1554, loss 1.38729, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 356/599 (start=22784, end=22848)\n",
      "2018-11-27T04:22:58.012776: step 1555, loss 1.33391, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 357/599 (start=22848, end=22912)\n",
      "2018-11-27T04:22:58.331902: step 1556, loss 1.08119, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 358/599 (start=22912, end=22976)\n",
      "2018-11-27T04:22:58.649950: step 1557, loss 1.28075, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 359/599 (start=22976, end=23040)\n",
      "2018-11-27T04:22:58.965340: step 1558, loss 1.74744, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 360/599 (start=23040, end=23104)\n",
      "2018-11-27T04:22:59.279039: step 1559, loss 1.22789, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 361/599 (start=23104, end=23168)\n",
      "2018-11-27T04:22:59.619206: step 1560, loss 1.35114, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 362/599 (start=23168, end=23232)\n",
      "2018-11-27T04:22:59.961941: step 1561, loss 1.39468, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 363/599 (start=23232, end=23296)\n",
      "2018-11-27T04:23:00.297531: step 1562, loss 1.33096, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 364/599 (start=23296, end=23360)\n",
      "2018-11-27T04:23:00.652926: step 1563, loss 1.46892, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 365/599 (start=23360, end=23424)\n",
      "2018-11-27T04:23:00.979868: step 1564, loss 1.16106, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 366/599 (start=23424, end=23488)\n",
      "2018-11-27T04:23:01.320039: step 1565, loss 1.65395, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 367/599 (start=23488, end=23552)\n",
      "2018-11-27T04:23:01.664934: step 1566, loss 1.43764, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 368/599 (start=23552, end=23616)\n",
      "2018-11-27T04:23:01.991931: step 1567, loss 1.33761, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 369/599 (start=23616, end=23680)\n",
      "2018-11-27T04:23:02.321179: step 1568, loss 1.44663, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 370/599 (start=23680, end=23744)\n",
      "2018-11-27T04:23:02.638857: step 1569, loss 1.39186, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 371/599 (start=23744, end=23808)\n",
      "2018-11-27T04:23:02.975986: step 1570, loss 1.29412, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 372/599 (start=23808, end=23872)\n",
      "2018-11-27T04:23:03.311021: step 1571, loss 1.6646, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 373/599 (start=23872, end=23936)\n",
      "2018-11-27T04:23:03.660009: step 1572, loss 1.3551, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 374/599 (start=23936, end=24000)\n",
      "2018-11-27T04:23:03.992295: step 1573, loss 1.28912, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 375/599 (start=24000, end=24064)\n",
      "2018-11-27T04:23:04.305654: step 1574, loss 1.31802, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 376/599 (start=24064, end=24128)\n",
      "2018-11-27T04:23:04.640504: step 1575, loss 1.21061, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 377/599 (start=24128, end=24192)\n",
      "2018-11-27T04:23:04.990580: step 1576, loss 1.38496, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 378/599 (start=24192, end=24256)\n",
      "2018-11-27T04:23:05.300317: step 1577, loss 1.25285, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 379/599 (start=24256, end=24320)\n",
      "2018-11-27T04:23:05.648641: step 1578, loss 1.56979, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 380/599 (start=24320, end=24384)\n",
      "2018-11-27T04:23:05.988654: step 1579, loss 1.31501, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 381/599 (start=24384, end=24448)\n",
      "2018-11-27T04:23:06.313663: step 1580, loss 1.44062, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 382/599 (start=24448, end=24512)\n",
      "2018-11-27T04:23:06.641566: step 1581, loss 1.19046, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 383/599 (start=24512, end=24576)\n",
      "2018-11-27T04:23:06.961427: step 1582, loss 1.55508, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 384/599 (start=24576, end=24640)\n",
      "2018-11-27T04:23:07.305799: step 1583, loss 1.51105, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 385/599 (start=24640, end=24704)\n",
      "2018-11-27T04:23:07.611382: step 1584, loss 1.2396, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 386/599 (start=24704, end=24768)\n",
      "2018-11-27T04:23:07.944739: step 1585, loss 1.40121, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 387/599 (start=24768, end=24832)\n",
      "2018-11-27T04:23:08.256610: step 1586, loss 1.53578, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 388/599 (start=24832, end=24896)\n",
      "2018-11-27T04:23:08.598575: step 1587, loss 1.58794, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 389/599 (start=24896, end=24960)\n",
      "2018-11-27T04:23:08.932847: step 1588, loss 1.33445, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 390/599 (start=24960, end=25024)\n",
      "2018-11-27T04:23:09.251828: step 1589, loss 1.2572, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 391/599 (start=25024, end=25088)\n",
      "2018-11-27T04:23:09.599181: step 1590, loss 1.31632, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 392/599 (start=25088, end=25152)\n",
      "2018-11-27T04:23:09.933848: step 1591, loss 1.35709, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 393/599 (start=25152, end=25216)\n",
      "2018-11-27T04:23:10.239797: step 1592, loss 1.31115, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 394/599 (start=25216, end=25280)\n",
      "2018-11-27T04:23:10.557252: step 1593, loss 1.47327, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 395/599 (start=25280, end=25344)\n",
      "2018-11-27T04:23:10.882643: step 1594, loss 1.5625, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 396/599 (start=25344, end=25408)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:23:11.200709: step 1595, loss 1.19603, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 397/599 (start=25408, end=25472)\n",
      "2018-11-27T04:23:11.536660: step 1596, loss 1.32195, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 398/599 (start=25472, end=25536)\n",
      "2018-11-27T04:23:11.871183: step 1597, loss 1.42189, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 399/599 (start=25536, end=25600)\n",
      "2018-11-27T04:23:12.211435: step 1598, loss 1.35153, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 400/599 (start=25600, end=25664)\n",
      "2018-11-27T04:23:12.527923: step 1599, loss 1.08672, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 401/599 (start=25664, end=25728)\n",
      "2018-11-27T04:23:12.876781: step 1600, loss 1.2299, acc 0.671875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:23:19.091309: step 1600, loss 1.72092, acc 0.495494\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-1600\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 402/599 (start=25728, end=25792)\n",
      "2018-11-27T04:23:19.822504: step 1601, loss 1.36476, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 403/599 (start=25792, end=25856)\n",
      "2018-11-27T04:23:20.143623: step 1602, loss 1.18646, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 404/599 (start=25856, end=25920)\n",
      "2018-11-27T04:23:20.485055: step 1603, loss 1.68053, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 405/599 (start=25920, end=25984)\n",
      "2018-11-27T04:23:20.800453: step 1604, loss 1.47119, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 406/599 (start=25984, end=26048)\n",
      "2018-11-27T04:23:21.117742: step 1605, loss 1.25939, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 407/599 (start=26048, end=26112)\n",
      "2018-11-27T04:23:21.462053: step 1606, loss 1.42144, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 408/599 (start=26112, end=26176)\n",
      "2018-11-27T04:23:21.780561: step 1607, loss 1.33096, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 409/599 (start=26176, end=26240)\n",
      "2018-11-27T04:23:22.115289: step 1608, loss 1.41139, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 410/599 (start=26240, end=26304)\n",
      "2018-11-27T04:23:22.455114: step 1609, loss 1.24654, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 411/599 (start=26304, end=26368)\n",
      "2018-11-27T04:23:22.804409: step 1610, loss 1.23268, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 412/599 (start=26368, end=26432)\n",
      "2018-11-27T04:23:23.121125: step 1611, loss 1.16909, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 413/599 (start=26432, end=26496)\n",
      "2018-11-27T04:23:23.460231: step 1612, loss 1.73166, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 414/599 (start=26496, end=26560)\n",
      "2018-11-27T04:23:23.788357: step 1613, loss 1.65582, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 415/599 (start=26560, end=26624)\n",
      "2018-11-27T04:23:24.140447: step 1614, loss 1.27262, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 416/599 (start=26624, end=26688)\n",
      "2018-11-27T04:23:24.458246: step 1615, loss 1.57472, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 417/599 (start=26688, end=26752)\n",
      "2018-11-27T04:23:24.797724: step 1616, loss 1.30008, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 418/599 (start=26752, end=26816)\n",
      "2018-11-27T04:23:25.138535: step 1617, loss 1.16613, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 419/599 (start=26816, end=26880)\n",
      "2018-11-27T04:23:25.477788: step 1618, loss 1.41167, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 420/599 (start=26880, end=26944)\n",
      "2018-11-27T04:23:25.817387: step 1619, loss 1.18297, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 421/599 (start=26944, end=27008)\n",
      "2018-11-27T04:23:26.159763: step 1620, loss 1.45186, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 422/599 (start=27008, end=27072)\n",
      "2018-11-27T04:23:26.506583: step 1621, loss 1.41129, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 423/599 (start=27072, end=27136)\n",
      "2018-11-27T04:23:26.851443: step 1622, loss 1.35575, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 424/599 (start=27136, end=27200)\n",
      "2018-11-27T04:23:27.162241: step 1623, loss 1.35438, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 425/599 (start=27200, end=27264)\n",
      "2018-11-27T04:23:27.507524: step 1624, loss 1.25909, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 426/599 (start=27264, end=27328)\n",
      "2018-11-27T04:23:27.839021: step 1625, loss 1.50224, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 427/599 (start=27328, end=27392)\n",
      "2018-11-27T04:23:28.156135: step 1626, loss 1.41694, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 428/599 (start=27392, end=27456)\n",
      "2018-11-27T04:23:28.490615: step 1627, loss 1.31276, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 429/599 (start=27456, end=27520)\n",
      "2018-11-27T04:23:28.830228: step 1628, loss 1.44251, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 430/599 (start=27520, end=27584)\n",
      "2018-11-27T04:23:29.188123: step 1629, loss 1.20928, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 431/599 (start=27584, end=27648)\n",
      "2018-11-27T04:23:29.546007: step 1630, loss 1.43797, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 432/599 (start=27648, end=27712)\n",
      "2018-11-27T04:23:29.891636: step 1631, loss 1.28088, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 433/599 (start=27712, end=27776)\n",
      "2018-11-27T04:23:30.214200: step 1632, loss 1.43237, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 434/599 (start=27776, end=27840)\n",
      "2018-11-27T04:23:30.528503: step 1633, loss 1.34226, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 435/599 (start=27840, end=27904)\n",
      "2018-11-27T04:23:30.871561: step 1634, loss 1.40041, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 436/599 (start=27904, end=27968)\n",
      "2018-11-27T04:23:31.177233: step 1635, loss 1.69219, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 437/599 (start=27968, end=28032)\n",
      "2018-11-27T04:23:31.497092: step 1636, loss 1.42223, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 438/599 (start=28032, end=28096)\n",
      "2018-11-27T04:23:31.824403: step 1637, loss 1.44891, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 439/599 (start=28096, end=28160)\n",
      "2018-11-27T04:23:32.162252: step 1638, loss 1.38087, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 440/599 (start=28160, end=28224)\n",
      "2018-11-27T04:23:32.502950: step 1639, loss 1.52321, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 441/599 (start=28224, end=28288)\n",
      "2018-11-27T04:23:32.840627: step 1640, loss 1.26054, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 442/599 (start=28288, end=28352)\n",
      "2018-11-27T04:23:33.166748: step 1641, loss 1.27977, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 443/599 (start=28352, end=28416)\n",
      "2018-11-27T04:23:33.507496: step 1642, loss 1.28829, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 444/599 (start=28416, end=28480)\n",
      "2018-11-27T04:23:33.826050: step 1643, loss 1.32329, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 445/599 (start=28480, end=28544)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:23:34.159938: step 1644, loss 1.43226, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 446/599 (start=28544, end=28608)\n",
      "2018-11-27T04:23:34.478783: step 1645, loss 1.33472, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 447/599 (start=28608, end=28672)\n",
      "2018-11-27T04:23:34.820316: step 1646, loss 1.20987, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 448/599 (start=28672, end=28736)\n",
      "2018-11-27T04:23:35.147752: step 1647, loss 1.2562, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 449/599 (start=28736, end=28800)\n",
      "2018-11-27T04:23:35.460046: step 1648, loss 1.32559, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 450/599 (start=28800, end=28864)\n",
      "2018-11-27T04:23:35.802111: step 1649, loss 1.41305, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 451/599 (start=28864, end=28928)\n",
      "2018-11-27T04:23:36.137537: step 1650, loss 1.21695, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 452/599 (start=28928, end=28992)\n",
      "2018-11-27T04:23:36.450851: step 1651, loss 1.35429, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 453/599 (start=28992, end=29056)\n",
      "2018-11-27T04:23:36.770289: step 1652, loss 1.25375, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 454/599 (start=29056, end=29120)\n",
      "2018-11-27T04:23:37.087436: step 1653, loss 1.35032, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 455/599 (start=29120, end=29184)\n",
      "2018-11-27T04:23:37.409559: step 1654, loss 1.62733, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 456/599 (start=29184, end=29248)\n",
      "2018-11-27T04:23:37.753304: step 1655, loss 1.44214, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 457/599 (start=29248, end=29312)\n",
      "2018-11-27T04:23:38.102577: step 1656, loss 1.18049, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 458/599 (start=29312, end=29376)\n",
      "2018-11-27T04:23:38.422385: step 1657, loss 1.58257, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 459/599 (start=29376, end=29440)\n",
      "2018-11-27T04:23:38.732232: step 1658, loss 1.47167, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 460/599 (start=29440, end=29504)\n",
      "2018-11-27T04:23:39.063216: step 1659, loss 1.40834, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 461/599 (start=29504, end=29568)\n",
      "2018-11-27T04:23:39.396489: step 1660, loss 1.29268, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 462/599 (start=29568, end=29632)\n",
      "2018-11-27T04:23:39.709457: step 1661, loss 1.3804, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 463/599 (start=29632, end=29696)\n",
      "2018-11-27T04:23:40.024620: step 1662, loss 1.22213, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 464/599 (start=29696, end=29760)\n",
      "2018-11-27T04:23:40.354477: step 1663, loss 1.4518, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 465/599 (start=29760, end=29824)\n",
      "2018-11-27T04:23:40.673427: step 1664, loss 1.24986, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 466/599 (start=29824, end=29888)\n",
      "2018-11-27T04:23:41.009273: step 1665, loss 1.76419, acc 0.5\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 467/599 (start=29888, end=29952)\n",
      "2018-11-27T04:23:41.322893: step 1666, loss 1.32559, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 468/599 (start=29952, end=30016)\n",
      "2018-11-27T04:23:41.644616: step 1667, loss 1.20211, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 469/599 (start=30016, end=30080)\n",
      "2018-11-27T04:23:41.980498: step 1668, loss 1.33568, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 470/599 (start=30080, end=30144)\n",
      "2018-11-27T04:23:42.312179: step 1669, loss 1.44331, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 471/599 (start=30144, end=30208)\n",
      "2018-11-27T04:23:42.650734: step 1670, loss 1.24084, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 472/599 (start=30208, end=30272)\n",
      "2018-11-27T04:23:42.984845: step 1671, loss 1.45551, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 473/599 (start=30272, end=30336)\n",
      "2018-11-27T04:23:43.337225: step 1672, loss 1.40456, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 474/599 (start=30336, end=30400)\n",
      "2018-11-27T04:23:43.667284: step 1673, loss 1.71102, acc 0.4375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 475/599 (start=30400, end=30464)\n",
      "2018-11-27T04:23:44.007773: step 1674, loss 1.07516, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 476/599 (start=30464, end=30528)\n",
      "2018-11-27T04:23:44.349547: step 1675, loss 1.49484, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 477/599 (start=30528, end=30592)\n",
      "2018-11-27T04:23:44.669342: step 1676, loss 1.35583, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 478/599 (start=30592, end=30656)\n",
      "2018-11-27T04:23:44.997624: step 1677, loss 1.36726, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 479/599 (start=30656, end=30720)\n",
      "2018-11-27T04:23:45.319701: step 1678, loss 1.35956, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 480/599 (start=30720, end=30784)\n",
      "2018-11-27T04:23:45.665216: step 1679, loss 1.54157, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 481/599 (start=30784, end=30848)\n",
      "2018-11-27T04:23:46.031656: step 1680, loss 1.37308, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 482/599 (start=30848, end=30912)\n",
      "2018-11-27T04:23:46.366473: step 1681, loss 1.53132, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 483/599 (start=30912, end=30976)\n",
      "2018-11-27T04:23:46.680552: step 1682, loss 1.36507, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 484/599 (start=30976, end=31040)\n",
      "2018-11-27T04:23:46.994444: step 1683, loss 1.31522, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 485/599 (start=31040, end=31104)\n",
      "2018-11-27T04:23:47.323040: step 1684, loss 1.23709, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 486/599 (start=31104, end=31168)\n",
      "2018-11-27T04:23:47.636712: step 1685, loss 1.46548, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 487/599 (start=31168, end=31232)\n",
      "2018-11-27T04:23:47.944067: step 1686, loss 1.33987, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 488/599 (start=31232, end=31296)\n",
      "2018-11-27T04:23:48.284927: step 1687, loss 1.63126, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 489/599 (start=31296, end=31360)\n",
      "2018-11-27T04:23:48.594826: step 1688, loss 1.26449, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 490/599 (start=31360, end=31424)\n",
      "2018-11-27T04:23:48.909452: step 1689, loss 1.31778, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 491/599 (start=31424, end=31488)\n",
      "2018-11-27T04:23:49.241260: step 1690, loss 1.45134, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 492/599 (start=31488, end=31552)\n",
      "2018-11-27T04:23:49.561293: step 1691, loss 1.40556, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 493/599 (start=31552, end=31616)\n",
      "2018-11-27T04:23:49.876835: step 1692, loss 1.12914, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 494/599 (start=31616, end=31680)\n",
      "2018-11-27T04:23:50.221093: step 1693, loss 1.32743, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 495/599 (start=31680, end=31744)\n",
      "2018-11-27T04:23:50.546883: step 1694, loss 1.31076, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 496/599 (start=31744, end=31808)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:23:50.888516: step 1695, loss 1.38597, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 497/599 (start=31808, end=31872)\n",
      "2018-11-27T04:23:51.198444: step 1696, loss 1.45697, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 498/599 (start=31872, end=31936)\n",
      "2018-11-27T04:23:51.550505: step 1697, loss 1.22235, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 499/599 (start=31936, end=32000)\n",
      "2018-11-27T04:23:51.896803: step 1698, loss 1.40398, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 500/599 (start=32000, end=32064)\n",
      "2018-11-27T04:23:52.224650: step 1699, loss 1.65047, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 501/599 (start=32064, end=32128)\n",
      "2018-11-27T04:23:52.560365: step 1700, loss 1.53546, acc 0.640625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:23:58.658508: step 1700, loss 1.71554, acc 0.498629\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-1700\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 502/599 (start=32128, end=32192)\n",
      "2018-11-27T04:23:59.405571: step 1701, loss 1.39986, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 503/599 (start=32192, end=32256)\n",
      "2018-11-27T04:23:59.751287: step 1702, loss 1.40686, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 504/599 (start=32256, end=32320)\n",
      "2018-11-27T04:24:00.106208: step 1703, loss 1.26806, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 505/599 (start=32320, end=32384)\n",
      "2018-11-27T04:24:00.441537: step 1704, loss 1.37249, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 506/599 (start=32384, end=32448)\n",
      "2018-11-27T04:24:00.753316: step 1705, loss 1.51189, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 507/599 (start=32448, end=32512)\n",
      "2018-11-27T04:24:01.083752: step 1706, loss 1.54697, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 508/599 (start=32512, end=32576)\n",
      "2018-11-27T04:24:01.423859: step 1707, loss 1.42908, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 509/599 (start=32576, end=32640)\n",
      "2018-11-27T04:24:01.759568: step 1708, loss 1.43117, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 510/599 (start=32640, end=32704)\n",
      "2018-11-27T04:24:02.102403: step 1709, loss 1.31461, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 511/599 (start=32704, end=32768)\n",
      "2018-11-27T04:24:02.435584: step 1710, loss 1.54226, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 512/599 (start=32768, end=32832)\n",
      "2018-11-27T04:24:02.750598: step 1711, loss 1.46661, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 513/599 (start=32832, end=32896)\n",
      "2018-11-27T04:24:03.089116: step 1712, loss 1.62, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 514/599 (start=32896, end=32960)\n",
      "2018-11-27T04:24:03.415237: step 1713, loss 1.51345, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 515/599 (start=32960, end=33024)\n",
      "2018-11-27T04:24:03.740028: step 1714, loss 1.53304, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 516/599 (start=33024, end=33088)\n",
      "2018-11-27T04:24:04.083709: step 1715, loss 1.46647, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 517/599 (start=33088, end=33152)\n",
      "2018-11-27T04:24:04.405819: step 1716, loss 1.26206, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 518/599 (start=33152, end=33216)\n",
      "2018-11-27T04:24:04.757003: step 1717, loss 1.27975, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 519/599 (start=33216, end=33280)\n",
      "2018-11-27T04:24:05.097088: step 1718, loss 1.38367, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 520/599 (start=33280, end=33344)\n",
      "2018-11-27T04:24:05.406381: step 1719, loss 1.20004, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 521/599 (start=33344, end=33408)\n",
      "2018-11-27T04:24:05.726163: step 1720, loss 1.24532, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 522/599 (start=33408, end=33472)\n",
      "2018-11-27T04:24:06.078279: step 1721, loss 1.38088, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 523/599 (start=33472, end=33536)\n",
      "2018-11-27T04:24:06.402410: step 1722, loss 1.15378, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 524/599 (start=33536, end=33600)\n",
      "2018-11-27T04:24:06.726275: step 1723, loss 1.30569, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 525/599 (start=33600, end=33664)\n",
      "2018-11-27T04:24:07.060632: step 1724, loss 1.51681, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 526/599 (start=33664, end=33728)\n",
      "2018-11-27T04:24:07.413579: step 1725, loss 1.44659, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 527/599 (start=33728, end=33792)\n",
      "2018-11-27T04:24:07.720284: step 1726, loss 1.2436, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 528/599 (start=33792, end=33856)\n",
      "2018-11-27T04:24:08.055027: step 1727, loss 1.52263, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 529/599 (start=33856, end=33920)\n",
      "2018-11-27T04:24:08.403647: step 1728, loss 1.54423, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 530/599 (start=33920, end=33984)\n",
      "2018-11-27T04:24:08.748413: step 1729, loss 1.5872, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 531/599 (start=33984, end=34048)\n",
      "2018-11-27T04:24:09.061590: step 1730, loss 1.33647, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 532/599 (start=34048, end=34112)\n",
      "2018-11-27T04:24:09.414870: step 1731, loss 1.26572, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 533/599 (start=34112, end=34176)\n",
      "2018-11-27T04:24:09.732804: step 1732, loss 1.58537, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 534/599 (start=34176, end=34240)\n",
      "2018-11-27T04:24:10.073092: step 1733, loss 1.46519, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 535/599 (start=34240, end=34304)\n",
      "2018-11-27T04:24:10.401023: step 1734, loss 1.23622, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 536/599 (start=34304, end=34368)\n",
      "2018-11-27T04:24:10.724316: step 1735, loss 1.19268, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 537/599 (start=34368, end=34432)\n",
      "2018-11-27T04:24:11.039035: step 1736, loss 1.46174, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 538/599 (start=34432, end=34496)\n",
      "2018-11-27T04:24:11.348050: step 1737, loss 1.42827, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 539/599 (start=34496, end=34560)\n",
      "2018-11-27T04:24:11.693069: step 1738, loss 1.0627, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 540/599 (start=34560, end=34624)\n",
      "2018-11-27T04:24:12.021189: step 1739, loss 1.48496, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 541/599 (start=34624, end=34688)\n",
      "2018-11-27T04:24:12.350666: step 1740, loss 1.2716, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 542/599 (start=34688, end=34752)\n",
      "2018-11-27T04:24:12.664340: step 1741, loss 1.47686, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 543/599 (start=34752, end=34816)\n",
      "2018-11-27T04:24:12.988284: step 1742, loss 1.34964, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 544/599 (start=34816, end=34880)\n",
      "2018-11-27T04:24:13.328116: step 1743, loss 1.22658, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 545/599 (start=34880, end=34944)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:24:13.660919: step 1744, loss 1.4983, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 546/599 (start=34944, end=35008)\n",
      "2018-11-27T04:24:14.001794: step 1745, loss 1.1571, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 547/599 (start=35008, end=35072)\n",
      "2018-11-27T04:24:14.351334: step 1746, loss 1.63441, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 548/599 (start=35072, end=35136)\n",
      "2018-11-27T04:24:14.694439: step 1747, loss 1.45095, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 549/599 (start=35136, end=35200)\n",
      "2018-11-27T04:24:15.019408: step 1748, loss 1.35208, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 550/599 (start=35200, end=35264)\n",
      "2018-11-27T04:24:15.367573: step 1749, loss 1.15041, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 551/599 (start=35264, end=35328)\n",
      "2018-11-27T04:24:15.716382: step 1750, loss 1.22133, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 552/599 (start=35328, end=35392)\n",
      "2018-11-27T04:24:16.053403: step 1751, loss 1.36951, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 553/599 (start=35392, end=35456)\n",
      "2018-11-27T04:24:16.387144: step 1752, loss 1.43727, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 554/599 (start=35456, end=35520)\n",
      "2018-11-27T04:24:16.735241: step 1753, loss 1.55264, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 555/599 (start=35520, end=35584)\n",
      "2018-11-27T04:24:17.071579: step 1754, loss 1.3991, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 556/599 (start=35584, end=35648)\n",
      "2018-11-27T04:24:17.401548: step 1755, loss 1.27836, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 557/599 (start=35648, end=35712)\n",
      "2018-11-27T04:24:17.754268: step 1756, loss 1.4361, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 558/599 (start=35712, end=35776)\n",
      "2018-11-27T04:24:18.099477: step 1757, loss 1.52199, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 559/599 (start=35776, end=35840)\n",
      "2018-11-27T04:24:18.408481: step 1758, loss 1.53749, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 560/599 (start=35840, end=35904)\n",
      "2018-11-27T04:24:18.746312: step 1759, loss 1.15931, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 561/599 (start=35904, end=35968)\n",
      "2018-11-27T04:24:19.084632: step 1760, loss 1.5106, acc 0.5625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 562/599 (start=35968, end=36032)\n",
      "2018-11-27T04:24:19.429235: step 1761, loss 1.68095, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 563/599 (start=36032, end=36096)\n",
      "2018-11-27T04:24:19.742960: step 1762, loss 1.24949, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 564/599 (start=36096, end=36160)\n",
      "2018-11-27T04:24:20.070259: step 1763, loss 1.43564, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 565/599 (start=36160, end=36224)\n",
      "2018-11-27T04:24:20.406415: step 1764, loss 1.07367, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 566/599 (start=36224, end=36288)\n",
      "2018-11-27T04:24:20.725191: step 1765, loss 1.5346, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 567/599 (start=36288, end=36352)\n",
      "2018-11-27T04:24:21.061011: step 1766, loss 1.2836, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 568/599 (start=36352, end=36416)\n",
      "2018-11-27T04:24:21.401518: step 1767, loss 1.36792, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 569/599 (start=36416, end=36480)\n",
      "2018-11-27T04:24:21.725916: step 1768, loss 1.50853, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 570/599 (start=36480, end=36544)\n",
      "2018-11-27T04:24:22.045568: step 1769, loss 1.59433, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 571/599 (start=36544, end=36608)\n",
      "2018-11-27T04:24:22.377595: step 1770, loss 1.58977, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 572/599 (start=36608, end=36672)\n",
      "2018-11-27T04:24:22.716868: step 1771, loss 1.37359, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 573/599 (start=36672, end=36736)\n",
      "2018-11-27T04:24:23.053698: step 1772, loss 1.35799, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 574/599 (start=36736, end=36800)\n",
      "2018-11-27T04:24:23.384783: step 1773, loss 1.3416, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 575/599 (start=36800, end=36864)\n",
      "2018-11-27T04:24:23.724578: step 1774, loss 1.3538, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 576/599 (start=36864, end=36928)\n",
      "2018-11-27T04:24:24.062683: step 1775, loss 1.37303, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 577/599 (start=36928, end=36992)\n",
      "2018-11-27T04:24:24.413539: step 1776, loss 1.38032, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 578/599 (start=36992, end=37056)\n",
      "2018-11-27T04:24:24.731472: step 1777, loss 1.49623, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 579/599 (start=37056, end=37120)\n",
      "2018-11-27T04:24:25.071929: step 1778, loss 1.36317, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 580/599 (start=37120, end=37184)\n",
      "2018-11-27T04:24:25.424925: step 1779, loss 1.5979, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 581/599 (start=37184, end=37248)\n",
      "2018-11-27T04:24:25.738532: step 1780, loss 1.47136, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 582/599 (start=37248, end=37312)\n",
      "2018-11-27T04:24:26.075981: step 1781, loss 1.76652, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 583/599 (start=37312, end=37376)\n",
      "2018-11-27T04:24:26.421314: step 1782, loss 1.29526, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 584/599 (start=37376, end=37440)\n",
      "2018-11-27T04:24:26.756758: step 1783, loss 1.62169, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 585/599 (start=37440, end=37504)\n",
      "2018-11-27T04:24:27.105497: step 1784, loss 1.45579, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 586/599 (start=37504, end=37568)\n",
      "2018-11-27T04:24:27.437559: step 1785, loss 1.32014, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 587/599 (start=37568, end=37632)\n",
      "2018-11-27T04:24:27.757378: step 1786, loss 1.57597, acc 0.46875\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 588/599 (start=37632, end=37696)\n",
      "2018-11-27T04:24:28.095423: step 1787, loss 1.31067, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 589/599 (start=37696, end=37760)\n",
      "2018-11-27T04:24:28.411491: step 1788, loss 1.3964, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 590/599 (start=37760, end=37824)\n",
      "2018-11-27T04:24:28.737498: step 1789, loss 1.81317, acc 0.484375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 591/599 (start=37824, end=37888)\n",
      "2018-11-27T04:24:29.068905: step 1790, loss 1.22101, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 592/599 (start=37888, end=37952)\n",
      "2018-11-27T04:24:29.416710: step 1791, loss 1.44576, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 593/599 (start=37952, end=38016)\n",
      "2018-11-27T04:24:29.760147: step 1792, loss 1.36384, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 594/599 (start=38016, end=38080)\n",
      "2018-11-27T04:24:30.067385: step 1793, loss 1.71411, acc 0.515625\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 595/599 (start=38080, end=38144)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:24:30.395610: step 1794, loss 1.41839, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 596/599 (start=38144, end=38208)\n",
      "2018-11-27T04:24:30.716247: step 1795, loss 1.16567, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 597/599 (start=38208, end=38272)\n",
      "2018-11-27T04:24:31.038604: step 1796, loss 1.44559, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 2/10, Batch 598/599 (start=38272, end=38281)\n",
      "2018-11-27T04:24:31.274424: step 1797, loss 1.7119, acc 0.444444\n",
      "***********************************************\n",
      "Epoch 3/10\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 0/599 (start=0, end=64)\n",
      "2018-11-27T04:24:31.627304: step 1798, loss 0.95832, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 1/599 (start=64, end=128)\n",
      "2018-11-27T04:24:31.941332: step 1799, loss 0.945159, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 2/599 (start=128, end=192)\n",
      "2018-11-27T04:24:32.262817: step 1800, loss 1.0161, acc 0.78125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:24:38.391793: step 1800, loss 1.71517, acc 0.490871\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-1800\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 3/599 (start=192, end=256)\n",
      "2018-11-27T04:24:39.128344: step 1801, loss 1.2063, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 4/599 (start=256, end=320)\n",
      "2018-11-27T04:24:39.449150: step 1802, loss 1.13195, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 5/599 (start=320, end=384)\n",
      "2018-11-27T04:24:39.796529: step 1803, loss 1.25504, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 6/599 (start=384, end=448)\n",
      "2018-11-27T04:24:40.105882: step 1804, loss 1.20659, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 7/599 (start=448, end=512)\n",
      "2018-11-27T04:24:40.429746: step 1805, loss 0.901509, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 8/599 (start=512, end=576)\n",
      "2018-11-27T04:24:40.754412: step 1806, loss 1.0813, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 9/599 (start=576, end=640)\n",
      "2018-11-27T04:24:41.097516: step 1807, loss 0.835377, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 10/599 (start=640, end=704)\n",
      "2018-11-27T04:24:41.426634: step 1808, loss 0.989149, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 11/599 (start=704, end=768)\n",
      "2018-11-27T04:24:41.768055: step 1809, loss 1.00466, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 12/599 (start=768, end=832)\n",
      "2018-11-27T04:24:42.075716: step 1810, loss 0.842581, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 13/599 (start=832, end=896)\n",
      "2018-11-27T04:24:42.418773: step 1811, loss 1.11809, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 14/599 (start=896, end=960)\n",
      "2018-11-27T04:24:42.730730: step 1812, loss 1.00765, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 15/599 (start=960, end=1024)\n",
      "2018-11-27T04:24:43.049957: step 1813, loss 1.39285, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 16/599 (start=1024, end=1088)\n",
      "2018-11-27T04:24:43.404535: step 1814, loss 1.16279, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 17/599 (start=1088, end=1152)\n",
      "2018-11-27T04:24:43.738608: step 1815, loss 1.11027, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 18/599 (start=1152, end=1216)\n",
      "2018-11-27T04:24:44.062253: step 1816, loss 1.05761, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 19/599 (start=1216, end=1280)\n",
      "2018-11-27T04:24:44.385663: step 1817, loss 1.42655, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 20/599 (start=1280, end=1344)\n",
      "2018-11-27T04:24:44.728123: step 1818, loss 1.04669, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 21/599 (start=1344, end=1408)\n",
      "2018-11-27T04:24:45.063067: step 1819, loss 1.00064, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 22/599 (start=1408, end=1472)\n",
      "2018-11-27T04:24:45.381659: step 1820, loss 1.13199, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 23/599 (start=1472, end=1536)\n",
      "2018-11-27T04:24:45.700223: step 1821, loss 0.893479, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 24/599 (start=1536, end=1600)\n",
      "2018-11-27T04:24:46.042040: step 1822, loss 1.19993, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 25/599 (start=1600, end=1664)\n",
      "2018-11-27T04:24:46.372088: step 1823, loss 1.04117, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 26/599 (start=1664, end=1728)\n",
      "2018-11-27T04:24:46.691343: step 1824, loss 1.10876, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 27/599 (start=1728, end=1792)\n",
      "2018-11-27T04:24:47.027472: step 1825, loss 1.06596, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 28/599 (start=1792, end=1856)\n",
      "2018-11-27T04:24:47.348978: step 1826, loss 0.970688, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 29/599 (start=1856, end=1920)\n",
      "2018-11-27T04:24:47.674753: step 1827, loss 1.04142, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 30/599 (start=1920, end=1984)\n",
      "2018-11-27T04:24:48.017454: step 1828, loss 1.20817, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 31/599 (start=1984, end=2048)\n",
      "2018-11-27T04:24:48.323404: step 1829, loss 1.09819, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 32/599 (start=2048, end=2112)\n",
      "2018-11-27T04:24:48.640511: step 1830, loss 0.910322, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 33/599 (start=2112, end=2176)\n",
      "2018-11-27T04:24:48.986811: step 1831, loss 1.05398, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 34/599 (start=2176, end=2240)\n",
      "2018-11-27T04:24:49.302819: step 1832, loss 1.04263, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 35/599 (start=2240, end=2304)\n",
      "2018-11-27T04:24:49.644168: step 1833, loss 1.07254, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 36/599 (start=2304, end=2368)\n",
      "2018-11-27T04:24:49.954016: step 1834, loss 1.16626, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 37/599 (start=2368, end=2432)\n",
      "2018-11-27T04:24:50.307237: step 1835, loss 1.22015, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 38/599 (start=2432, end=2496)\n",
      "2018-11-27T04:24:50.629044: step 1836, loss 1.19127, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 39/599 (start=2496, end=2560)\n",
      "2018-11-27T04:24:50.942353: step 1837, loss 1.00404, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 40/599 (start=2560, end=2624)\n",
      "2018-11-27T04:24:51.253441: step 1838, loss 0.888509, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 41/599 (start=2624, end=2688)\n",
      "2018-11-27T04:24:51.596545: step 1839, loss 1.15862, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 42/599 (start=2688, end=2752)\n",
      "2018-11-27T04:24:51.929227: step 1840, loss 1.20273, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 43/599 (start=2752, end=2816)\n",
      "2018-11-27T04:24:52.246279: step 1841, loss 1.13335, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 44/599 (start=2816, end=2880)\n",
      "2018-11-27T04:24:52.584424: step 1842, loss 1.04817, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 45/599 (start=2880, end=2944)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:24:52.916967: step 1843, loss 1.17464, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 46/599 (start=2944, end=3008)\n",
      "2018-11-27T04:24:53.234345: step 1844, loss 1.35257, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 47/599 (start=3008, end=3072)\n",
      "2018-11-27T04:24:53.549409: step 1845, loss 0.878228, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 48/599 (start=3072, end=3136)\n",
      "2018-11-27T04:24:53.874471: step 1846, loss 1.15715, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 49/599 (start=3136, end=3200)\n",
      "2018-11-27T04:24:54.211970: step 1847, loss 1.05952, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 50/599 (start=3200, end=3264)\n",
      "2018-11-27T04:24:54.529227: step 1848, loss 0.920964, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 51/599 (start=3264, end=3328)\n",
      "2018-11-27T04:24:54.864394: step 1849, loss 1.18771, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 52/599 (start=3328, end=3392)\n",
      "2018-11-27T04:24:55.195430: step 1850, loss 1.00564, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 53/599 (start=3392, end=3456)\n",
      "2018-11-27T04:24:55.536823: step 1851, loss 1.05175, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 54/599 (start=3456, end=3520)\n",
      "2018-11-27T04:24:55.865037: step 1852, loss 1.20568, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 55/599 (start=3520, end=3584)\n",
      "2018-11-27T04:24:56.202817: step 1853, loss 1.27249, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 56/599 (start=3584, end=3648)\n",
      "2018-11-27T04:24:56.541691: step 1854, loss 0.814344, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 57/599 (start=3648, end=3712)\n",
      "2018-11-27T04:24:56.880382: step 1855, loss 0.982411, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 58/599 (start=3712, end=3776)\n",
      "2018-11-27T04:24:57.215519: step 1856, loss 1.14505, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 59/599 (start=3776, end=3840)\n",
      "2018-11-27T04:24:57.552446: step 1857, loss 1.04168, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 60/599 (start=3840, end=3904)\n",
      "2018-11-27T04:24:57.896871: step 1858, loss 1.00001, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 61/599 (start=3904, end=3968)\n",
      "2018-11-27T04:24:58.208677: step 1859, loss 0.743609, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 62/599 (start=3968, end=4032)\n",
      "2018-11-27T04:24:58.527196: step 1860, loss 1.14565, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 63/599 (start=4032, end=4096)\n",
      "2018-11-27T04:24:58.881380: step 1861, loss 1.05652, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 64/599 (start=4096, end=4160)\n",
      "2018-11-27T04:24:59.217954: step 1862, loss 1.04876, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 65/599 (start=4160, end=4224)\n",
      "2018-11-27T04:24:59.559916: step 1863, loss 0.991251, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 66/599 (start=4224, end=4288)\n",
      "2018-11-27T04:24:59.882144: step 1864, loss 0.84844, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 67/599 (start=4288, end=4352)\n",
      "2018-11-27T04:25:00.244167: step 1865, loss 1.02558, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 68/599 (start=4352, end=4416)\n",
      "2018-11-27T04:25:00.581622: step 1866, loss 0.872858, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 69/599 (start=4416, end=4480)\n",
      "2018-11-27T04:25:00.910533: step 1867, loss 1.08113, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 70/599 (start=4480, end=4544)\n",
      "2018-11-27T04:25:01.242699: step 1868, loss 0.875735, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 71/599 (start=4544, end=4608)\n",
      "2018-11-27T04:25:01.579383: step 1869, loss 1.10277, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 72/599 (start=4608, end=4672)\n",
      "2018-11-27T04:25:01.919284: step 1870, loss 0.816804, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 73/599 (start=4672, end=4736)\n",
      "2018-11-27T04:25:02.259190: step 1871, loss 0.909081, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 74/599 (start=4736, end=4800)\n",
      "2018-11-27T04:25:02.595924: step 1872, loss 0.888954, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 75/599 (start=4800, end=4864)\n",
      "2018-11-27T04:25:02.933594: step 1873, loss 1.16144, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 76/599 (start=4864, end=4928)\n",
      "2018-11-27T04:25:03.257410: step 1874, loss 1.17364, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 77/599 (start=4928, end=4992)\n",
      "2018-11-27T04:25:03.599962: step 1875, loss 1.21316, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 78/599 (start=4992, end=5056)\n",
      "2018-11-27T04:25:03.919558: step 1876, loss 1.2102, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 79/599 (start=5056, end=5120)\n",
      "2018-11-27T04:25:04.252410: step 1877, loss 1.14695, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 80/599 (start=5120, end=5184)\n",
      "2018-11-27T04:25:04.589291: step 1878, loss 0.850196, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 81/599 (start=5184, end=5248)\n",
      "2018-11-27T04:25:04.896749: step 1879, loss 1.2079, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 82/599 (start=5248, end=5312)\n",
      "2018-11-27T04:25:05.224425: step 1880, loss 1.19137, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 83/599 (start=5312, end=5376)\n",
      "2018-11-27T04:25:05.560035: step 1881, loss 0.95167, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 84/599 (start=5376, end=5440)\n",
      "2018-11-27T04:25:05.907026: step 1882, loss 1.0779, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 85/599 (start=5440, end=5504)\n",
      "2018-11-27T04:25:06.231370: step 1883, loss 0.991398, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 86/599 (start=5504, end=5568)\n",
      "2018-11-27T04:25:06.576291: step 1884, loss 0.956239, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 87/599 (start=5568, end=5632)\n",
      "2018-11-27T04:25:06.917623: step 1885, loss 0.688, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 88/599 (start=5632, end=5696)\n",
      "2018-11-27T04:25:07.260918: step 1886, loss 1.06329, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 89/599 (start=5696, end=5760)\n",
      "2018-11-27T04:25:07.597184: step 1887, loss 1.04311, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 90/599 (start=5760, end=5824)\n",
      "2018-11-27T04:25:07.933890: step 1888, loss 1.04299, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 91/599 (start=5824, end=5888)\n",
      "2018-11-27T04:25:08.272816: step 1889, loss 0.923726, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 92/599 (start=5888, end=5952)\n",
      "2018-11-27T04:25:08.620025: step 1890, loss 1.15095, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 93/599 (start=5952, end=6016)\n",
      "2018-11-27T04:25:08.954431: step 1891, loss 0.922068, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 94/599 (start=6016, end=6080)\n",
      "2018-11-27T04:25:09.296713: step 1892, loss 1.2103, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 95/599 (start=6080, end=6144)\n",
      "2018-11-27T04:25:09.652280: step 1893, loss 1.19216, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 96/599 (start=6144, end=6208)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:25:09.968307: step 1894, loss 1.12652, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 97/599 (start=6208, end=6272)\n",
      "2018-11-27T04:25:10.300710: step 1895, loss 1.08793, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 98/599 (start=6272, end=6336)\n",
      "2018-11-27T04:25:10.622176: step 1896, loss 0.991849, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 99/599 (start=6336, end=6400)\n",
      "2018-11-27T04:25:10.947291: step 1897, loss 1.0761, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 100/599 (start=6400, end=6464)\n",
      "2018-11-27T04:25:11.283023: step 1898, loss 1.22101, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 101/599 (start=6464, end=6528)\n",
      "2018-11-27T04:25:11.599133: step 1899, loss 0.913807, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 102/599 (start=6528, end=6592)\n",
      "2018-11-27T04:25:11.935038: step 1900, loss 1.40704, acc 0.640625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:25:18.193926: step 1900, loss 1.72713, acc 0.501215\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-1900\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 103/599 (start=6592, end=6656)\n",
      "2018-11-27T04:25:18.929995: step 1901, loss 0.93223, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 104/599 (start=6656, end=6720)\n",
      "2018-11-27T04:25:19.270889: step 1902, loss 0.851459, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 105/599 (start=6720, end=6784)\n",
      "2018-11-27T04:25:19.604888: step 1903, loss 1.02084, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 106/599 (start=6784, end=6848)\n",
      "2018-11-27T04:25:19.916031: step 1904, loss 1.15192, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 107/599 (start=6848, end=6912)\n",
      "2018-11-27T04:25:20.236920: step 1905, loss 1.17017, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 108/599 (start=6912, end=6976)\n",
      "2018-11-27T04:25:20.570606: step 1906, loss 0.919924, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 109/599 (start=6976, end=7040)\n",
      "2018-11-27T04:25:20.891347: step 1907, loss 1.12601, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 110/599 (start=7040, end=7104)\n",
      "2018-11-27T04:25:21.229323: step 1908, loss 1.12664, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 111/599 (start=7104, end=7168)\n",
      "2018-11-27T04:25:21.565324: step 1909, loss 1.27929, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 112/599 (start=7168, end=7232)\n",
      "2018-11-27T04:25:21.922986: step 1910, loss 1.19545, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 113/599 (start=7232, end=7296)\n",
      "2018-11-27T04:25:22.257281: step 1911, loss 0.756123, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 114/599 (start=7296, end=7360)\n",
      "2018-11-27T04:25:22.598921: step 1912, loss 1.24508, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 115/599 (start=7360, end=7424)\n",
      "2018-11-27T04:25:22.936336: step 1913, loss 0.864591, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 116/599 (start=7424, end=7488)\n",
      "2018-11-27T04:25:23.274816: step 1914, loss 1.11131, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 117/599 (start=7488, end=7552)\n",
      "2018-11-27T04:25:23.612983: step 1915, loss 0.847546, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 118/599 (start=7552, end=7616)\n",
      "2018-11-27T04:25:23.966570: step 1916, loss 0.901604, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 119/599 (start=7616, end=7680)\n",
      "2018-11-27T04:25:24.286572: step 1917, loss 0.966496, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 120/599 (start=7680, end=7744)\n",
      "2018-11-27T04:25:24.604296: step 1918, loss 1.01468, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 121/599 (start=7744, end=7808)\n",
      "2018-11-27T04:25:24.913170: step 1919, loss 1.29911, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 122/599 (start=7808, end=7872)\n",
      "2018-11-27T04:25:25.237568: step 1920, loss 1.04022, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 123/599 (start=7872, end=7936)\n",
      "2018-11-27T04:25:25.560364: step 1921, loss 1.43236, acc 0.53125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 124/599 (start=7936, end=8000)\n",
      "2018-11-27T04:25:25.883488: step 1922, loss 1.28338, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 125/599 (start=8000, end=8064)\n",
      "2018-11-27T04:25:26.222946: step 1923, loss 0.990328, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 126/599 (start=8064, end=8128)\n",
      "2018-11-27T04:25:26.557938: step 1924, loss 1.06392, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 127/599 (start=8128, end=8192)\n",
      "2018-11-27T04:25:26.893637: step 1925, loss 1.05111, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 128/599 (start=8192, end=8256)\n",
      "2018-11-27T04:25:27.224599: step 1926, loss 0.976576, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 129/599 (start=8256, end=8320)\n",
      "2018-11-27T04:25:27.547716: step 1927, loss 1.17969, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 130/599 (start=8320, end=8384)\n",
      "2018-11-27T04:25:27.896751: step 1928, loss 1.10167, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 131/599 (start=8384, end=8448)\n",
      "2018-11-27T04:25:28.238926: step 1929, loss 0.942174, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 132/599 (start=8448, end=8512)\n",
      "2018-11-27T04:25:28.568033: step 1930, loss 1.02048, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 133/599 (start=8512, end=8576)\n",
      "2018-11-27T04:25:28.911102: step 1931, loss 1.18816, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 134/599 (start=8576, end=8640)\n",
      "2018-11-27T04:25:29.244097: step 1932, loss 1.57728, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 135/599 (start=8640, end=8704)\n",
      "2018-11-27T04:25:29.585520: step 1933, loss 1.07646, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 136/599 (start=8704, end=8768)\n",
      "2018-11-27T04:25:29.930041: step 1934, loss 1.10089, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 137/599 (start=8768, end=8832)\n",
      "2018-11-27T04:25:30.244172: step 1935, loss 1.50446, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 138/599 (start=8832, end=8896)\n",
      "2018-11-27T04:25:30.544630: step 1936, loss 0.99881, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 139/599 (start=8896, end=8960)\n",
      "2018-11-27T04:25:30.877493: step 1937, loss 1.02166, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 140/599 (start=8960, end=9024)\n",
      "2018-11-27T04:25:31.197047: step 1938, loss 1.13824, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 141/599 (start=9024, end=9088)\n",
      "2018-11-27T04:25:31.542847: step 1939, loss 1.17117, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 142/599 (start=9088, end=9152)\n",
      "2018-11-27T04:25:31.871171: step 1940, loss 1.15877, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 143/599 (start=9152, end=9216)\n",
      "2018-11-27T04:25:32.180917: step 1941, loss 1.03476, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 144/599 (start=9216, end=9280)\n",
      "2018-11-27T04:25:32.502677: step 1942, loss 1.0347, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 145/599 (start=9280, end=9344)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:25:32.825549: step 1943, loss 1.10996, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 146/599 (start=9344, end=9408)\n",
      "2018-11-27T04:25:33.160296: step 1944, loss 0.989961, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 147/599 (start=9408, end=9472)\n",
      "2018-11-27T04:25:33.499312: step 1945, loss 0.927048, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 148/599 (start=9472, end=9536)\n",
      "2018-11-27T04:25:33.822041: step 1946, loss 1.01628, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 149/599 (start=9536, end=9600)\n",
      "2018-11-27T04:25:34.151461: step 1947, loss 1.27488, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 150/599 (start=9600, end=9664)\n",
      "2018-11-27T04:25:34.494844: step 1948, loss 1.22735, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 151/599 (start=9664, end=9728)\n",
      "2018-11-27T04:25:34.836867: step 1949, loss 0.936321, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 152/599 (start=9728, end=9792)\n",
      "2018-11-27T04:25:35.186745: step 1950, loss 1.07943, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 153/599 (start=9792, end=9856)\n",
      "2018-11-27T04:25:35.530586: step 1951, loss 1.27449, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 154/599 (start=9856, end=9920)\n",
      "2018-11-27T04:25:35.890910: step 1952, loss 1.21618, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 155/599 (start=9920, end=9984)\n",
      "2018-11-27T04:25:36.201082: step 1953, loss 1.29057, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 156/599 (start=9984, end=10048)\n",
      "2018-11-27T04:25:36.542332: step 1954, loss 1.00203, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 157/599 (start=10048, end=10112)\n",
      "2018-11-27T04:25:36.870834: step 1955, loss 1.24988, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 158/599 (start=10112, end=10176)\n",
      "2018-11-27T04:25:37.185714: step 1956, loss 0.831911, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 159/599 (start=10176, end=10240)\n",
      "2018-11-27T04:25:37.523612: step 1957, loss 1.03621, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 160/599 (start=10240, end=10304)\n",
      "2018-11-27T04:25:37.840263: step 1958, loss 1.27117, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 161/599 (start=10304, end=10368)\n",
      "2018-11-27T04:25:38.178434: step 1959, loss 0.963389, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 162/599 (start=10368, end=10432)\n",
      "2018-11-27T04:25:38.503210: step 1960, loss 0.949658, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 163/599 (start=10432, end=10496)\n",
      "2018-11-27T04:25:38.832997: step 1961, loss 1.11168, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 164/599 (start=10496, end=10560)\n",
      "2018-11-27T04:25:39.193395: step 1962, loss 0.874094, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 165/599 (start=10560, end=10624)\n",
      "2018-11-27T04:25:39.541357: step 1963, loss 1.00316, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 166/599 (start=10624, end=10688)\n",
      "2018-11-27T04:25:39.865561: step 1964, loss 1.10255, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 167/599 (start=10688, end=10752)\n",
      "2018-11-27T04:25:40.173797: step 1965, loss 1.08044, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 168/599 (start=10752, end=10816)\n",
      "2018-11-27T04:25:40.510407: step 1966, loss 0.906775, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 169/599 (start=10816, end=10880)\n",
      "2018-11-27T04:25:40.844702: step 1967, loss 0.905691, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 170/599 (start=10880, end=10944)\n",
      "2018-11-27T04:25:41.190652: step 1968, loss 0.928221, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 171/599 (start=10944, end=11008)\n",
      "2018-11-27T04:25:41.527200: step 1969, loss 0.896613, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 172/599 (start=11008, end=11072)\n",
      "2018-11-27T04:25:41.856432: step 1970, loss 0.965735, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 173/599 (start=11072, end=11136)\n",
      "2018-11-27T04:25:42.211475: step 1971, loss 0.995028, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 174/599 (start=11136, end=11200)\n",
      "2018-11-27T04:25:42.539560: step 1972, loss 1.16294, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 175/599 (start=11200, end=11264)\n",
      "2018-11-27T04:25:42.879667: step 1973, loss 1.05143, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 176/599 (start=11264, end=11328)\n",
      "2018-11-27T04:25:43.227725: step 1974, loss 0.989695, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 177/599 (start=11328, end=11392)\n",
      "2018-11-27T04:25:43.575409: step 1975, loss 0.954957, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 178/599 (start=11392, end=11456)\n",
      "2018-11-27T04:25:43.925916: step 1976, loss 1.01697, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 179/599 (start=11456, end=11520)\n",
      "2018-11-27T04:25:44.269766: step 1977, loss 1.38969, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 180/599 (start=11520, end=11584)\n",
      "2018-11-27T04:25:44.583529: step 1978, loss 0.948077, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 181/599 (start=11584, end=11648)\n",
      "2018-11-27T04:25:44.931175: step 1979, loss 1.09468, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 182/599 (start=11648, end=11712)\n",
      "2018-11-27T04:25:45.242724: step 1980, loss 1.09123, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 183/599 (start=11712, end=11776)\n",
      "2018-11-27T04:25:45.590063: step 1981, loss 0.895851, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 184/599 (start=11776, end=11840)\n",
      "2018-11-27T04:25:45.907232: step 1982, loss 0.995995, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 185/599 (start=11840, end=11904)\n",
      "2018-11-27T04:25:46.246589: step 1983, loss 0.879983, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 186/599 (start=11904, end=11968)\n",
      "2018-11-27T04:25:46.569746: step 1984, loss 0.974412, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 187/599 (start=11968, end=12032)\n",
      "2018-11-27T04:25:46.916506: step 1985, loss 0.968188, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 188/599 (start=12032, end=12096)\n",
      "2018-11-27T04:25:47.238674: step 1986, loss 0.936969, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 189/599 (start=12096, end=12160)\n",
      "2018-11-27T04:25:47.563930: step 1987, loss 0.932404, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 190/599 (start=12160, end=12224)\n",
      "2018-11-27T04:25:47.908670: step 1988, loss 0.979135, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 191/599 (start=12224, end=12288)\n",
      "2018-11-27T04:25:48.238501: step 1989, loss 1.37911, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 192/599 (start=12288, end=12352)\n",
      "2018-11-27T04:25:48.584224: step 1990, loss 1.04922, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 193/599 (start=12352, end=12416)\n",
      "2018-11-27T04:25:48.921114: step 1991, loss 1.0002, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 194/599 (start=12416, end=12480)\n",
      "2018-11-27T04:25:49.256457: step 1992, loss 1.0871, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 195/599 (start=12480, end=12544)\n",
      "2018-11-27T04:25:49.615703: step 1993, loss 1.24867, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 196/599 (start=12544, end=12608)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:25:49.956394: step 1994, loss 1.1345, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 197/599 (start=12608, end=12672)\n",
      "2018-11-27T04:25:50.291985: step 1995, loss 1.27988, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 198/599 (start=12672, end=12736)\n",
      "2018-11-27T04:25:50.616528: step 1996, loss 1.15432, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 199/599 (start=12736, end=12800)\n",
      "2018-11-27T04:25:50.956164: step 1997, loss 0.944965, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 200/599 (start=12800, end=12864)\n",
      "2018-11-27T04:25:51.268633: step 1998, loss 1.16017, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 201/599 (start=12864, end=12928)\n",
      "2018-11-27T04:25:51.608165: step 1999, loss 1.00438, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 202/599 (start=12928, end=12992)\n",
      "2018-11-27T04:25:51.956734: step 2000, loss 1.09194, acc 0.75\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:25:58.234129: step 2000, loss 1.73396, acc 0.495729\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-2000\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 203/599 (start=12992, end=13056)\n",
      "2018-11-27T04:25:58.987319: step 2001, loss 1.12249, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 204/599 (start=13056, end=13120)\n",
      "2018-11-27T04:25:59.312483: step 2002, loss 1.05114, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 205/599 (start=13120, end=13184)\n",
      "2018-11-27T04:25:59.641853: step 2003, loss 1.04166, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 206/599 (start=13184, end=13248)\n",
      "2018-11-27T04:25:59.976121: step 2004, loss 1.20851, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 207/599 (start=13248, end=13312)\n",
      "2018-11-27T04:26:00.321359: step 2005, loss 1.01548, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 208/599 (start=13312, end=13376)\n",
      "2018-11-27T04:26:00.656226: step 2006, loss 1.25513, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 209/599 (start=13376, end=13440)\n",
      "2018-11-27T04:26:00.989956: step 2007, loss 1.30219, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 210/599 (start=13440, end=13504)\n",
      "2018-11-27T04:26:01.333554: step 2008, loss 1.13029, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 211/599 (start=13504, end=13568)\n",
      "2018-11-27T04:26:01.678715: step 2009, loss 1.07524, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 212/599 (start=13568, end=13632)\n",
      "2018-11-27T04:26:02.028669: step 2010, loss 1.12662, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 213/599 (start=13632, end=13696)\n",
      "2018-11-27T04:26:02.382895: step 2011, loss 0.905803, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 214/599 (start=13696, end=13760)\n",
      "2018-11-27T04:26:02.706286: step 2012, loss 1.10207, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 215/599 (start=13760, end=13824)\n",
      "2018-11-27T04:26:03.038261: step 2013, loss 1.03398, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 216/599 (start=13824, end=13888)\n",
      "2018-11-27T04:26:03.377599: step 2014, loss 1.07826, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 217/599 (start=13888, end=13952)\n",
      "2018-11-27T04:26:03.712363: step 2015, loss 1.04795, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 218/599 (start=13952, end=14016)\n",
      "2018-11-27T04:26:04.062389: step 2016, loss 0.998985, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 219/599 (start=14016, end=14080)\n",
      "2018-11-27T04:26:04.394137: step 2017, loss 1.05103, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 220/599 (start=14080, end=14144)\n",
      "2018-11-27T04:26:04.737889: step 2018, loss 1.26847, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 221/599 (start=14144, end=14208)\n",
      "2018-11-27T04:26:05.068942: step 2019, loss 1.10618, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 222/599 (start=14208, end=14272)\n",
      "2018-11-27T04:26:05.405976: step 2020, loss 1.0602, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 223/599 (start=14272, end=14336)\n",
      "2018-11-27T04:26:05.721215: step 2021, loss 1.18021, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 224/599 (start=14336, end=14400)\n",
      "2018-11-27T04:26:06.065740: step 2022, loss 1.10119, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 225/599 (start=14400, end=14464)\n",
      "2018-11-27T04:26:06.385609: step 2023, loss 0.948939, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 226/599 (start=14464, end=14528)\n",
      "2018-11-27T04:26:06.722125: step 2024, loss 0.893939, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 227/599 (start=14528, end=14592)\n",
      "2018-11-27T04:26:07.070105: step 2025, loss 0.957437, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 228/599 (start=14592, end=14656)\n",
      "2018-11-27T04:26:07.413911: step 2026, loss 1.26548, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 229/599 (start=14656, end=14720)\n",
      "2018-11-27T04:26:07.758755: step 2027, loss 0.823892, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 230/599 (start=14720, end=14784)\n",
      "2018-11-27T04:26:08.093063: step 2028, loss 1.02896, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 231/599 (start=14784, end=14848)\n",
      "2018-11-27T04:26:08.426757: step 2029, loss 1.31638, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 232/599 (start=14848, end=14912)\n",
      "2018-11-27T04:26:08.754102: step 2030, loss 0.801434, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 233/599 (start=14912, end=14976)\n",
      "2018-11-27T04:26:09.065282: step 2031, loss 1.15515, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 234/599 (start=14976, end=15040)\n",
      "2018-11-27T04:26:09.399934: step 2032, loss 0.918179, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 235/599 (start=15040, end=15104)\n",
      "2018-11-27T04:26:09.715514: step 2033, loss 1.16144, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 236/599 (start=15104, end=15168)\n",
      "2018-11-27T04:26:10.059973: step 2034, loss 1.27038, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 237/599 (start=15168, end=15232)\n",
      "2018-11-27T04:26:10.401839: step 2035, loss 0.820106, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 238/599 (start=15232, end=15296)\n",
      "2018-11-27T04:26:10.756293: step 2036, loss 1.06127, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 239/599 (start=15296, end=15360)\n",
      "2018-11-27T04:26:11.081512: step 2037, loss 0.931037, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 240/599 (start=15360, end=15424)\n",
      "2018-11-27T04:26:11.426636: step 2038, loss 1.00237, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 241/599 (start=15424, end=15488)\n",
      "2018-11-27T04:26:11.761952: step 2039, loss 1.36029, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 242/599 (start=15488, end=15552)\n",
      "2018-11-27T04:26:12.103608: step 2040, loss 0.936122, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 243/599 (start=15552, end=15616)\n",
      "2018-11-27T04:26:12.452948: step 2041, loss 1.08093, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 244/599 (start=15616, end=15680)\n",
      "2018-11-27T04:26:12.795889: step 2042, loss 1.01723, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 245/599 (start=15680, end=15744)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:26:13.134423: step 2043, loss 1.02577, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 246/599 (start=15744, end=15808)\n",
      "2018-11-27T04:26:13.467776: step 2044, loss 1.10143, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 247/599 (start=15808, end=15872)\n",
      "2018-11-27T04:26:13.801859: step 2045, loss 0.989984, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 248/599 (start=15872, end=15936)\n",
      "2018-11-27T04:26:14.134998: step 2046, loss 1.17719, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 249/599 (start=15936, end=16000)\n",
      "2018-11-27T04:26:14.477629: step 2047, loss 1.17331, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 250/599 (start=16000, end=16064)\n",
      "2018-11-27T04:26:14.818609: step 2048, loss 1.07429, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 251/599 (start=16064, end=16128)\n",
      "2018-11-27T04:26:15.163312: step 2049, loss 0.866563, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 252/599 (start=16128, end=16192)\n",
      "2018-11-27T04:26:15.478383: step 2050, loss 1.02684, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 253/599 (start=16192, end=16256)\n",
      "2018-11-27T04:26:15.798149: step 2051, loss 1.36769, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 254/599 (start=16256, end=16320)\n",
      "2018-11-27T04:26:16.131073: step 2052, loss 1.07551, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 255/599 (start=16320, end=16384)\n",
      "2018-11-27T04:26:16.440847: step 2053, loss 0.933304, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 256/599 (start=16384, end=16448)\n",
      "2018-11-27T04:26:16.793720: step 2054, loss 1.06441, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 257/599 (start=16448, end=16512)\n",
      "2018-11-27T04:26:17.107054: step 2055, loss 0.798224, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 258/599 (start=16512, end=16576)\n",
      "2018-11-27T04:26:17.429861: step 2056, loss 1.14441, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 259/599 (start=16576, end=16640)\n",
      "2018-11-27T04:26:17.739588: step 2057, loss 1.06026, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 260/599 (start=16640, end=16704)\n",
      "2018-11-27T04:26:18.059688: step 2058, loss 1.36639, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 261/599 (start=16704, end=16768)\n",
      "2018-11-27T04:26:18.407338: step 2059, loss 1.19132, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 262/599 (start=16768, end=16832)\n",
      "2018-11-27T04:26:18.738506: step 2060, loss 1.09295, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 263/599 (start=16832, end=16896)\n",
      "2018-11-27T04:26:19.073232: step 2061, loss 1.19004, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 264/599 (start=16896, end=16960)\n",
      "2018-11-27T04:26:19.416281: step 2062, loss 0.839868, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 265/599 (start=16960, end=17024)\n",
      "2018-11-27T04:26:19.766654: step 2063, loss 1.47066, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 266/599 (start=17024, end=17088)\n",
      "2018-11-27T04:26:20.101024: step 2064, loss 0.940302, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 267/599 (start=17088, end=17152)\n",
      "2018-11-27T04:26:20.428463: step 2065, loss 0.893507, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 268/599 (start=17152, end=17216)\n",
      "2018-11-27T04:26:20.752385: step 2066, loss 1.01984, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 269/599 (start=17216, end=17280)\n",
      "2018-11-27T04:26:21.053583: step 2067, loss 1.01802, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 270/599 (start=17280, end=17344)\n",
      "2018-11-27T04:26:21.387090: step 2068, loss 0.973929, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 271/599 (start=17344, end=17408)\n",
      "2018-11-27T04:26:21.724953: step 2069, loss 1.07209, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 272/599 (start=17408, end=17472)\n",
      "2018-11-27T04:26:22.061504: step 2070, loss 1.23869, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 273/599 (start=17472, end=17536)\n",
      "2018-11-27T04:26:22.397770: step 2071, loss 1.0276, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 274/599 (start=17536, end=17600)\n",
      "2018-11-27T04:26:22.705173: step 2072, loss 1.16271, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 275/599 (start=17600, end=17664)\n",
      "2018-11-27T04:26:23.054291: step 2073, loss 0.866277, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 276/599 (start=17664, end=17728)\n",
      "2018-11-27T04:26:23.393080: step 2074, loss 1.05812, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 277/599 (start=17728, end=17792)\n",
      "2018-11-27T04:26:23.706885: step 2075, loss 1.06479, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 278/599 (start=17792, end=17856)\n",
      "2018-11-27T04:26:24.043068: step 2076, loss 1.11178, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 279/599 (start=17856, end=17920)\n",
      "2018-11-27T04:26:24.363452: step 2077, loss 1.13076, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 280/599 (start=17920, end=17984)\n",
      "2018-11-27T04:26:24.693318: step 2078, loss 0.990945, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 281/599 (start=17984, end=18048)\n",
      "2018-11-27T04:26:25.023445: step 2079, loss 1.08044, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 282/599 (start=18048, end=18112)\n",
      "2018-11-27T04:26:25.348460: step 2080, loss 0.996441, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 283/599 (start=18112, end=18176)\n",
      "2018-11-27T04:26:25.691732: step 2081, loss 1.17049, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 284/599 (start=18176, end=18240)\n",
      "2018-11-27T04:26:26.014794: step 2082, loss 0.945803, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 285/599 (start=18240, end=18304)\n",
      "2018-11-27T04:26:26.339428: step 2083, loss 1.48069, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 286/599 (start=18304, end=18368)\n",
      "2018-11-27T04:26:26.657348: step 2084, loss 0.902213, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 287/599 (start=18368, end=18432)\n",
      "2018-11-27T04:26:26.979030: step 2085, loss 1.10421, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 288/599 (start=18432, end=18496)\n",
      "2018-11-27T04:26:27.317213: step 2086, loss 1.15879, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 289/599 (start=18496, end=18560)\n",
      "2018-11-27T04:26:27.660946: step 2087, loss 0.925269, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 290/599 (start=18560, end=18624)\n",
      "2018-11-27T04:26:28.002663: step 2088, loss 1.01577, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 291/599 (start=18624, end=18688)\n",
      "2018-11-27T04:26:28.328189: step 2089, loss 0.796497, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 292/599 (start=18688, end=18752)\n",
      "2018-11-27T04:26:28.658693: step 2090, loss 0.963265, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 293/599 (start=18752, end=18816)\n",
      "2018-11-27T04:26:29.023063: step 2091, loss 1.33552, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 294/599 (start=18816, end=18880)\n",
      "2018-11-27T04:26:29.363117: step 2092, loss 0.996289, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 295/599 (start=18880, end=18944)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:26:29.716772: step 2093, loss 1.37142, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 296/599 (start=18944, end=19008)\n",
      "2018-11-27T04:26:30.068697: step 2094, loss 1.05102, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 297/599 (start=19008, end=19072)\n",
      "2018-11-27T04:26:30.409365: step 2095, loss 1.01239, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 298/599 (start=19072, end=19136)\n",
      "2018-11-27T04:26:30.745367: step 2096, loss 1.05579, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 299/599 (start=19136, end=19200)\n",
      "2018-11-27T04:26:31.079863: step 2097, loss 1.00037, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 300/599 (start=19200, end=19264)\n",
      "2018-11-27T04:26:31.419444: step 2098, loss 1.07078, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 301/599 (start=19264, end=19328)\n",
      "2018-11-27T04:26:31.736997: step 2099, loss 1.40752, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 302/599 (start=19328, end=19392)\n",
      "2018-11-27T04:26:32.078453: step 2100, loss 0.966391, acc 0.796875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:26:38.258526: step 2100, loss 1.74971, acc 0.503879\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-2100\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 303/599 (start=19392, end=19456)\n",
      "2018-11-27T04:26:38.992046: step 2101, loss 1.12773, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 304/599 (start=19456, end=19520)\n",
      "2018-11-27T04:26:39.313536: step 2102, loss 0.974003, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 305/599 (start=19520, end=19584)\n",
      "2018-11-27T04:26:39.653918: step 2103, loss 1.24157, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 306/599 (start=19584, end=19648)\n",
      "2018-11-27T04:26:39.968136: step 2104, loss 1.14801, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 307/599 (start=19648, end=19712)\n",
      "2018-11-27T04:26:40.292672: step 2105, loss 1.08948, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 308/599 (start=19712, end=19776)\n",
      "2018-11-27T04:26:40.618382: step 2106, loss 1.11587, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 309/599 (start=19776, end=19840)\n",
      "2018-11-27T04:26:40.964148: step 2107, loss 0.917727, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 310/599 (start=19840, end=19904)\n",
      "2018-11-27T04:26:41.304017: step 2108, loss 1.15225, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 311/599 (start=19904, end=19968)\n",
      "2018-11-27T04:26:41.639314: step 2109, loss 1.24192, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 312/599 (start=19968, end=20032)\n",
      "2018-11-27T04:26:41.985183: step 2110, loss 0.958796, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 313/599 (start=20032, end=20096)\n",
      "2018-11-27T04:26:42.313875: step 2111, loss 1.22202, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 314/599 (start=20096, end=20160)\n",
      "2018-11-27T04:26:42.655882: step 2112, loss 1.28405, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 315/599 (start=20160, end=20224)\n",
      "2018-11-27T04:26:42.984934: step 2113, loss 1.10076, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 316/599 (start=20224, end=20288)\n",
      "2018-11-27T04:26:43.306689: step 2114, loss 1.06826, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 317/599 (start=20288, end=20352)\n",
      "2018-11-27T04:26:43.666230: step 2115, loss 0.935997, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 318/599 (start=20352, end=20416)\n",
      "2018-11-27T04:26:43.997079: step 2116, loss 0.926742, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 319/599 (start=20416, end=20480)\n",
      "2018-11-27T04:26:44.332390: step 2117, loss 1.00877, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 320/599 (start=20480, end=20544)\n",
      "2018-11-27T04:26:44.642022: step 2118, loss 1.29443, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 321/599 (start=20544, end=20608)\n",
      "2018-11-27T04:26:44.957377: step 2119, loss 1.16271, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 322/599 (start=20608, end=20672)\n",
      "2018-11-27T04:26:45.283436: step 2120, loss 1.1167, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 323/599 (start=20672, end=20736)\n",
      "2018-11-27T04:26:45.604482: step 2121, loss 1.24321, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 324/599 (start=20736, end=20800)\n",
      "2018-11-27T04:26:45.909301: step 2122, loss 1.0825, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 325/599 (start=20800, end=20864)\n",
      "2018-11-27T04:26:46.259695: step 2123, loss 1.36412, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 326/599 (start=20864, end=20928)\n",
      "2018-11-27T04:26:46.602006: step 2124, loss 1.36191, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 327/599 (start=20928, end=20992)\n",
      "2018-11-27T04:26:46.958309: step 2125, loss 1.10397, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 328/599 (start=20992, end=21056)\n",
      "2018-11-27T04:26:47.312065: step 2126, loss 0.950762, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 329/599 (start=21056, end=21120)\n",
      "2018-11-27T04:26:47.624280: step 2127, loss 1.14769, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 330/599 (start=21120, end=21184)\n",
      "2018-11-27T04:26:47.960954: step 2128, loss 1.04162, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 331/599 (start=21184, end=21248)\n",
      "2018-11-27T04:26:48.309758: step 2129, loss 1.02959, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 332/599 (start=21248, end=21312)\n",
      "2018-11-27T04:26:48.665916: step 2130, loss 1.03362, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 333/599 (start=21312, end=21376)\n",
      "2018-11-27T04:26:49.003139: step 2131, loss 1.22494, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 334/599 (start=21376, end=21440)\n",
      "2018-11-27T04:26:49.336565: step 2132, loss 1.02378, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 335/599 (start=21440, end=21504)\n",
      "2018-11-27T04:26:49.675067: step 2133, loss 0.850963, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 336/599 (start=21504, end=21568)\n",
      "2018-11-27T04:26:50.020646: step 2134, loss 1.04899, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 337/599 (start=21568, end=21632)\n",
      "2018-11-27T04:26:50.351690: step 2135, loss 1.20459, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 338/599 (start=21632, end=21696)\n",
      "2018-11-27T04:26:50.683715: step 2136, loss 1.17977, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 339/599 (start=21696, end=21760)\n",
      "2018-11-27T04:26:51.032397: step 2137, loss 0.912374, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 340/599 (start=21760, end=21824)\n",
      "2018-11-27T04:26:51.369730: step 2138, loss 0.989269, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 341/599 (start=21824, end=21888)\n",
      "2018-11-27T04:26:51.717008: step 2139, loss 1.07521, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 342/599 (start=21888, end=21952)\n",
      "2018-11-27T04:26:52.041012: step 2140, loss 1.00944, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 343/599 (start=21952, end=22016)\n",
      "2018-11-27T04:26:52.366594: step 2141, loss 1.14311, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 344/599 (start=22016, end=22080)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:26:52.705260: step 2142, loss 1.03699, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 345/599 (start=22080, end=22144)\n",
      "2018-11-27T04:26:53.048075: step 2143, loss 1.27208, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 346/599 (start=22144, end=22208)\n",
      "2018-11-27T04:26:53.390754: step 2144, loss 1.30692, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 347/599 (start=22208, end=22272)\n",
      "2018-11-27T04:26:53.712618: step 2145, loss 1.19887, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 348/599 (start=22272, end=22336)\n",
      "2018-11-27T04:26:54.056054: step 2146, loss 0.902952, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 349/599 (start=22336, end=22400)\n",
      "2018-11-27T04:26:54.371647: step 2147, loss 1.231, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 350/599 (start=22400, end=22464)\n",
      "2018-11-27T04:26:54.685731: step 2148, loss 1.20418, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 351/599 (start=22464, end=22528)\n",
      "2018-11-27T04:26:55.015112: step 2149, loss 1.03984, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 352/599 (start=22528, end=22592)\n",
      "2018-11-27T04:26:55.356227: step 2150, loss 1.24746, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 353/599 (start=22592, end=22656)\n",
      "2018-11-27T04:26:55.691109: step 2151, loss 1.10274, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 354/599 (start=22656, end=22720)\n",
      "2018-11-27T04:26:56.026428: step 2152, loss 1.31395, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 355/599 (start=22720, end=22784)\n",
      "2018-11-27T04:26:56.356702: step 2153, loss 0.98408, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 356/599 (start=22784, end=22848)\n",
      "2018-11-27T04:26:56.695682: step 2154, loss 0.887371, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 357/599 (start=22848, end=22912)\n",
      "2018-11-27T04:26:57.037258: step 2155, loss 1.0003, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 358/599 (start=22912, end=22976)\n",
      "2018-11-27T04:26:57.368336: step 2156, loss 1.28346, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 359/599 (start=22976, end=23040)\n",
      "2018-11-27T04:26:57.691894: step 2157, loss 1.08701, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 360/599 (start=23040, end=23104)\n",
      "2018-11-27T04:26:58.013484: step 2158, loss 1.20278, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 361/599 (start=23104, end=23168)\n",
      "2018-11-27T04:26:58.345382: step 2159, loss 1.21788, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 362/599 (start=23168, end=23232)\n",
      "2018-11-27T04:26:58.657465: step 2160, loss 1.05223, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 363/599 (start=23232, end=23296)\n",
      "2018-11-27T04:26:58.997312: step 2161, loss 1.22271, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 364/599 (start=23296, end=23360)\n",
      "2018-11-27T04:26:59.351519: step 2162, loss 0.883868, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 365/599 (start=23360, end=23424)\n",
      "2018-11-27T04:26:59.680200: step 2163, loss 1.15772, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 366/599 (start=23424, end=23488)\n",
      "2018-11-27T04:27:00.014900: step 2164, loss 1.04293, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 367/599 (start=23488, end=23552)\n",
      "2018-11-27T04:27:00.372566: step 2165, loss 1.25085, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 368/599 (start=23552, end=23616)\n",
      "2018-11-27T04:27:00.712640: step 2166, loss 1.10955, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 369/599 (start=23616, end=23680)\n",
      "2018-11-27T04:27:01.045114: step 2167, loss 0.79722, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 370/599 (start=23680, end=23744)\n",
      "2018-11-27T04:27:01.359820: step 2168, loss 1.08932, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 371/599 (start=23744, end=23808)\n",
      "2018-11-27T04:27:01.698329: step 2169, loss 1.14097, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 372/599 (start=23808, end=23872)\n",
      "2018-11-27T04:27:02.034523: step 2170, loss 0.95071, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 373/599 (start=23872, end=23936)\n",
      "2018-11-27T04:27:02.337527: step 2171, loss 0.790131, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 374/599 (start=23936, end=24000)\n",
      "2018-11-27T04:27:02.655961: step 2172, loss 1.12973, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 375/599 (start=24000, end=24064)\n",
      "2018-11-27T04:27:02.994865: step 2173, loss 0.941958, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 376/599 (start=24064, end=24128)\n",
      "2018-11-27T04:27:03.326375: step 2174, loss 0.980874, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 377/599 (start=24128, end=24192)\n",
      "2018-11-27T04:27:03.635304: step 2175, loss 1.24714, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 378/599 (start=24192, end=24256)\n",
      "2018-11-27T04:27:03.975924: step 2176, loss 0.910233, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 379/599 (start=24256, end=24320)\n",
      "2018-11-27T04:27:04.306087: step 2177, loss 1.02343, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 380/599 (start=24320, end=24384)\n",
      "2018-11-27T04:27:04.641767: step 2178, loss 0.930184, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 381/599 (start=24384, end=24448)\n",
      "2018-11-27T04:27:04.984363: step 2179, loss 1.06375, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 382/599 (start=24448, end=24512)\n",
      "2018-11-27T04:27:05.312547: step 2180, loss 1.10018, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 383/599 (start=24512, end=24576)\n",
      "2018-11-27T04:27:05.645674: step 2181, loss 0.966532, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 384/599 (start=24576, end=24640)\n",
      "2018-11-27T04:27:05.968732: step 2182, loss 1.17373, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 385/599 (start=24640, end=24704)\n",
      "2018-11-27T04:27:06.318422: step 2183, loss 0.974641, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 386/599 (start=24704, end=24768)\n",
      "2018-11-27T04:27:06.636105: step 2184, loss 1.18505, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 387/599 (start=24768, end=24832)\n",
      "2018-11-27T04:27:06.951029: step 2185, loss 0.959816, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 388/599 (start=24832, end=24896)\n",
      "2018-11-27T04:27:07.281986: step 2186, loss 0.860705, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 389/599 (start=24896, end=24960)\n",
      "2018-11-27T04:27:07.611669: step 2187, loss 1.12754, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 390/599 (start=24960, end=25024)\n",
      "2018-11-27T04:27:07.951362: step 2188, loss 1.00508, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 391/599 (start=25024, end=25088)\n",
      "2018-11-27T04:27:08.283801: step 2189, loss 1.10382, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 392/599 (start=25088, end=25152)\n",
      "2018-11-27T04:27:08.610393: step 2190, loss 1.00458, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 393/599 (start=25152, end=25216)\n",
      "2018-11-27T04:27:08.956388: step 2191, loss 1.24676, acc 0.609375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 394/599 (start=25216, end=25280)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:27:09.272264: step 2192, loss 1.13635, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 395/599 (start=25280, end=25344)\n",
      "2018-11-27T04:27:09.599992: step 2193, loss 1.17062, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 396/599 (start=25344, end=25408)\n",
      "2018-11-27T04:27:09.945691: step 2194, loss 1.03632, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 397/599 (start=25408, end=25472)\n",
      "2018-11-27T04:27:10.296609: step 2195, loss 1.1311, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 398/599 (start=25472, end=25536)\n",
      "2018-11-27T04:27:10.620932: step 2196, loss 1.04679, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 399/599 (start=25536, end=25600)\n",
      "2018-11-27T04:27:10.959647: step 2197, loss 1.46328, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 400/599 (start=25600, end=25664)\n",
      "2018-11-27T04:27:11.298731: step 2198, loss 1.13952, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 401/599 (start=25664, end=25728)\n",
      "2018-11-27T04:27:11.635662: step 2199, loss 1.37771, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 402/599 (start=25728, end=25792)\n",
      "2018-11-27T04:27:11.978954: step 2200, loss 1.1817, acc 0.734375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:27:18.182414: step 2200, loss 1.74844, acc 0.507014\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-2200\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 403/599 (start=25792, end=25856)\n",
      "2018-11-27T04:27:18.925744: step 2201, loss 1.07576, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 404/599 (start=25856, end=25920)\n",
      "2018-11-27T04:27:19.283854: step 2202, loss 1.23457, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 405/599 (start=25920, end=25984)\n",
      "2018-11-27T04:27:19.613697: step 2203, loss 1.28701, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 406/599 (start=25984, end=26048)\n",
      "2018-11-27T04:27:19.954069: step 2204, loss 1.42399, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 407/599 (start=26048, end=26112)\n",
      "2018-11-27T04:27:20.299960: step 2205, loss 1.23933, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 408/599 (start=26112, end=26176)\n",
      "2018-11-27T04:27:20.617700: step 2206, loss 1.16629, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 409/599 (start=26176, end=26240)\n",
      "2018-11-27T04:27:20.956519: step 2207, loss 1.08678, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 410/599 (start=26240, end=26304)\n",
      "2018-11-27T04:27:21.305092: step 2208, loss 1.20823, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 411/599 (start=26304, end=26368)\n",
      "2018-11-27T04:27:21.627218: step 2209, loss 0.920603, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 412/599 (start=26368, end=26432)\n",
      "2018-11-27T04:27:21.960888: step 2210, loss 0.927703, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 413/599 (start=26432, end=26496)\n",
      "2018-11-27T04:27:22.291826: step 2211, loss 0.840582, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 414/599 (start=26496, end=26560)\n",
      "2018-11-27T04:27:22.627593: step 2212, loss 1.14035, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 415/599 (start=26560, end=26624)\n",
      "2018-11-27T04:27:22.964345: step 2213, loss 1.02606, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 416/599 (start=26624, end=26688)\n",
      "2018-11-27T04:27:23.310673: step 2214, loss 1.02104, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 417/599 (start=26688, end=26752)\n",
      "2018-11-27T04:27:23.649533: step 2215, loss 1.13526, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 418/599 (start=26752, end=26816)\n",
      "2018-11-27T04:27:23.955937: step 2216, loss 1.07253, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 419/599 (start=26816, end=26880)\n",
      "2018-11-27T04:27:24.278394: step 2217, loss 1.60037, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 420/599 (start=26880, end=26944)\n",
      "2018-11-27T04:27:24.593058: step 2218, loss 1.06123, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 421/599 (start=26944, end=27008)\n",
      "2018-11-27T04:27:24.932196: step 2219, loss 0.890297, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 422/599 (start=27008, end=27072)\n",
      "2018-11-27T04:27:25.238250: step 2220, loss 1.31684, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 423/599 (start=27072, end=27136)\n",
      "2018-11-27T04:27:25.562037: step 2221, loss 1.01299, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 424/599 (start=27136, end=27200)\n",
      "2018-11-27T04:27:25.898046: step 2222, loss 1.2064, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 425/599 (start=27200, end=27264)\n",
      "2018-11-27T04:27:26.233587: step 2223, loss 0.910538, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 426/599 (start=27264, end=27328)\n",
      "2018-11-27T04:27:26.581992: step 2224, loss 1.05373, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 427/599 (start=27328, end=27392)\n",
      "2018-11-27T04:27:26.918260: step 2225, loss 1.36315, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 428/599 (start=27392, end=27456)\n",
      "2018-11-27T04:27:27.232866: step 2226, loss 0.873411, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 429/599 (start=27456, end=27520)\n",
      "2018-11-27T04:27:27.540709: step 2227, loss 0.946748, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 430/599 (start=27520, end=27584)\n",
      "2018-11-27T04:27:27.855045: step 2228, loss 0.992945, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 431/599 (start=27584, end=27648)\n",
      "2018-11-27T04:27:28.174345: step 2229, loss 1.057, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 432/599 (start=27648, end=27712)\n",
      "2018-11-27T04:27:28.491911: step 2230, loss 0.898854, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 433/599 (start=27712, end=27776)\n",
      "2018-11-27T04:27:28.819160: step 2231, loss 0.803143, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 434/599 (start=27776, end=27840)\n",
      "2018-11-27T04:27:29.146890: step 2232, loss 1.00267, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 435/599 (start=27840, end=27904)\n",
      "2018-11-27T04:27:29.476161: step 2233, loss 1.22505, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 436/599 (start=27904, end=27968)\n",
      "2018-11-27T04:27:29.795398: step 2234, loss 1.26984, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 437/599 (start=27968, end=28032)\n",
      "2018-11-27T04:27:30.139596: step 2235, loss 1.11168, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 438/599 (start=28032, end=28096)\n",
      "2018-11-27T04:27:30.479574: step 2236, loss 1.09159, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 439/599 (start=28096, end=28160)\n",
      "2018-11-27T04:27:30.791935: step 2237, loss 1.1736, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 440/599 (start=28160, end=28224)\n",
      "2018-11-27T04:27:31.101862: step 2238, loss 0.826509, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 441/599 (start=28224, end=28288)\n",
      "2018-11-27T04:27:31.429297: step 2239, loss 1.14098, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 442/599 (start=28288, end=28352)\n",
      "2018-11-27T04:27:31.746987: step 2240, loss 0.979143, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 443/599 (start=28352, end=28416)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:27:32.057657: step 2241, loss 1.42319, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 444/599 (start=28416, end=28480)\n",
      "2018-11-27T04:27:32.383782: step 2242, loss 0.950436, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 445/599 (start=28480, end=28544)\n",
      "2018-11-27T04:27:32.703380: step 2243, loss 1.03433, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 446/599 (start=28544, end=28608)\n",
      "2018-11-27T04:27:33.017616: step 2244, loss 1.10934, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 447/599 (start=28608, end=28672)\n",
      "2018-11-27T04:27:33.332157: step 2245, loss 1.11225, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 448/599 (start=28672, end=28736)\n",
      "2018-11-27T04:27:33.649094: step 2246, loss 1.3191, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 449/599 (start=28736, end=28800)\n",
      "2018-11-27T04:27:33.986422: step 2247, loss 1.20036, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 450/599 (start=28800, end=28864)\n",
      "2018-11-27T04:27:34.332232: step 2248, loss 0.836513, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 451/599 (start=28864, end=28928)\n",
      "2018-11-27T04:27:34.646909: step 2249, loss 1.03056, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 452/599 (start=28928, end=28992)\n",
      "2018-11-27T04:27:34.968945: step 2250, loss 1.0576, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 453/599 (start=28992, end=29056)\n",
      "2018-11-27T04:27:35.315830: step 2251, loss 0.999942, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 454/599 (start=29056, end=29120)\n",
      "2018-11-27T04:27:35.646658: step 2252, loss 1.04381, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 455/599 (start=29120, end=29184)\n",
      "2018-11-27T04:27:35.976251: step 2253, loss 1.37809, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 456/599 (start=29184, end=29248)\n",
      "2018-11-27T04:27:36.299022: step 2254, loss 1.14505, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 457/599 (start=29248, end=29312)\n",
      "2018-11-27T04:27:36.639139: step 2255, loss 1.14336, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 458/599 (start=29312, end=29376)\n",
      "2018-11-27T04:27:36.983679: step 2256, loss 1.24301, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 459/599 (start=29376, end=29440)\n",
      "2018-11-27T04:27:37.293325: step 2257, loss 1.58375, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 460/599 (start=29440, end=29504)\n",
      "2018-11-27T04:27:37.609782: step 2258, loss 0.956697, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 461/599 (start=29504, end=29568)\n",
      "2018-11-27T04:27:37.957330: step 2259, loss 1.07311, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 462/599 (start=29568, end=29632)\n",
      "2018-11-27T04:27:38.285151: step 2260, loss 1.12436, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 463/599 (start=29632, end=29696)\n",
      "2018-11-27T04:27:38.631318: step 2261, loss 1.01871, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 464/599 (start=29696, end=29760)\n",
      "2018-11-27T04:27:38.974856: step 2262, loss 0.971142, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 465/599 (start=29760, end=29824)\n",
      "2018-11-27T04:27:39.319045: step 2263, loss 1.02337, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 466/599 (start=29824, end=29888)\n",
      "2018-11-27T04:27:39.651141: step 2264, loss 1.05916, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 467/599 (start=29888, end=29952)\n",
      "2018-11-27T04:27:39.983861: step 2265, loss 1.0881, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 468/599 (start=29952, end=30016)\n",
      "2018-11-27T04:27:40.323740: step 2266, loss 1.0254, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 469/599 (start=30016, end=30080)\n",
      "2018-11-27T04:27:40.629858: step 2267, loss 0.909668, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 470/599 (start=30080, end=30144)\n",
      "2018-11-27T04:27:40.976512: step 2268, loss 1.08315, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 471/599 (start=30144, end=30208)\n",
      "2018-11-27T04:27:41.298964: step 2269, loss 1.05588, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 472/599 (start=30208, end=30272)\n",
      "2018-11-27T04:27:41.631260: step 2270, loss 1.04027, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 473/599 (start=30272, end=30336)\n",
      "2018-11-27T04:27:41.941841: step 2271, loss 1.07168, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 474/599 (start=30336, end=30400)\n",
      "2018-11-27T04:27:42.257312: step 2272, loss 1.31458, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 475/599 (start=30400, end=30464)\n",
      "2018-11-27T04:27:42.604915: step 2273, loss 0.970974, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 476/599 (start=30464, end=30528)\n",
      "2018-11-27T04:27:42.937740: step 2274, loss 1.12868, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 477/599 (start=30528, end=30592)\n",
      "2018-11-27T04:27:43.265208: step 2275, loss 1.00468, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 478/599 (start=30592, end=30656)\n",
      "2018-11-27T04:27:43.610787: step 2276, loss 1.41476, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 479/599 (start=30656, end=30720)\n",
      "2018-11-27T04:27:43.959675: step 2277, loss 1.0701, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 480/599 (start=30720, end=30784)\n",
      "2018-11-27T04:27:44.295068: step 2278, loss 1.13392, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 481/599 (start=30784, end=30848)\n",
      "2018-11-27T04:27:44.631435: step 2279, loss 0.883354, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 482/599 (start=30848, end=30912)\n",
      "2018-11-27T04:27:44.964806: step 2280, loss 1.15184, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 483/599 (start=30912, end=30976)\n",
      "2018-11-27T04:27:45.298407: step 2281, loss 1.01237, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 484/599 (start=30976, end=31040)\n",
      "2018-11-27T04:27:45.613894: step 2282, loss 1.17338, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 485/599 (start=31040, end=31104)\n",
      "2018-11-27T04:27:45.947963: step 2283, loss 0.938848, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 486/599 (start=31104, end=31168)\n",
      "2018-11-27T04:27:46.267914: step 2284, loss 0.961746, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 487/599 (start=31168, end=31232)\n",
      "2018-11-27T04:27:46.602832: step 2285, loss 1.08186, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 488/599 (start=31232, end=31296)\n",
      "2018-11-27T04:27:46.923731: step 2286, loss 1.41111, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 489/599 (start=31296, end=31360)\n",
      "2018-11-27T04:27:47.263533: step 2287, loss 1.08107, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 490/599 (start=31360, end=31424)\n",
      "2018-11-27T04:27:47.598166: step 2288, loss 1.3609, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 491/599 (start=31424, end=31488)\n",
      "2018-11-27T04:27:47.932967: step 2289, loss 0.981454, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 492/599 (start=31488, end=31552)\n",
      "2018-11-27T04:27:48.271123: step 2290, loss 1.30194, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 493/599 (start=31552, end=31616)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:27:48.611655: step 2291, loss 0.792787, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 494/599 (start=31616, end=31680)\n",
      "2018-11-27T04:27:48.926533: step 2292, loss 1.15635, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 495/599 (start=31680, end=31744)\n",
      "2018-11-27T04:27:49.259649: step 2293, loss 1.07806, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 496/599 (start=31744, end=31808)\n",
      "2018-11-27T04:27:49.600955: step 2294, loss 1.15787, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 497/599 (start=31808, end=31872)\n",
      "2018-11-27T04:27:49.947387: step 2295, loss 1.36664, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 498/599 (start=31872, end=31936)\n",
      "2018-11-27T04:27:50.251901: step 2296, loss 1.15411, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 499/599 (start=31936, end=32000)\n",
      "2018-11-27T04:27:50.597293: step 2297, loss 0.895222, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 500/599 (start=32000, end=32064)\n",
      "2018-11-27T04:27:50.940901: step 2298, loss 1.23888, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 501/599 (start=32064, end=32128)\n",
      "2018-11-27T04:27:51.264614: step 2299, loss 1.01346, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 502/599 (start=32128, end=32192)\n",
      "2018-11-27T04:27:51.602770: step 2300, loss 1.09593, acc 0.75\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:27:57.950853: step 2300, loss 1.75778, acc 0.51054\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-2300\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 503/599 (start=32192, end=32256)\n",
      "2018-11-27T04:27:58.714756: step 2301, loss 1.21352, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 504/599 (start=32256, end=32320)\n",
      "2018-11-27T04:27:59.035231: step 2302, loss 0.99245, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 505/599 (start=32320, end=32384)\n",
      "2018-11-27T04:27:59.370932: step 2303, loss 1.20256, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 506/599 (start=32384, end=32448)\n",
      "2018-11-27T04:27:59.701074: step 2304, loss 1.38887, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 507/599 (start=32448, end=32512)\n",
      "2018-11-27T04:28:00.015957: step 2305, loss 1.09443, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 508/599 (start=32512, end=32576)\n",
      "2018-11-27T04:28:00.370529: step 2306, loss 1.24314, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 509/599 (start=32576, end=32640)\n",
      "2018-11-27T04:28:00.722963: step 2307, loss 0.992617, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 510/599 (start=32640, end=32704)\n",
      "2018-11-27T04:28:01.040285: step 2308, loss 1.16864, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 511/599 (start=32704, end=32768)\n",
      "2018-11-27T04:28:01.382024: step 2309, loss 0.883866, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 512/599 (start=32768, end=32832)\n",
      "2018-11-27T04:28:01.728725: step 2310, loss 1.13893, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 513/599 (start=32832, end=32896)\n",
      "2018-11-27T04:28:02.057214: step 2311, loss 1.02754, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 514/599 (start=32896, end=32960)\n",
      "2018-11-27T04:28:02.376250: step 2312, loss 1.05682, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 515/599 (start=32960, end=33024)\n",
      "2018-11-27T04:28:02.724169: step 2313, loss 1.28288, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 516/599 (start=33024, end=33088)\n",
      "2018-11-27T04:28:03.074501: step 2314, loss 1.27616, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 517/599 (start=33088, end=33152)\n",
      "2018-11-27T04:28:03.424029: step 2315, loss 1.16629, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 518/599 (start=33152, end=33216)\n",
      "2018-11-27T04:28:03.760124: step 2316, loss 1.20577, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 519/599 (start=33216, end=33280)\n",
      "2018-11-27T04:28:04.074483: step 2317, loss 1.49402, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 520/599 (start=33280, end=33344)\n",
      "2018-11-27T04:28:04.414546: step 2318, loss 1.00973, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 521/599 (start=33344, end=33408)\n",
      "2018-11-27T04:28:04.749644: step 2319, loss 1.03997, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 522/599 (start=33408, end=33472)\n",
      "2018-11-27T04:28:05.103850: step 2320, loss 0.999493, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 523/599 (start=33472, end=33536)\n",
      "2018-11-27T04:28:05.421057: step 2321, loss 1.19966, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 524/599 (start=33536, end=33600)\n",
      "2018-11-27T04:28:05.748027: step 2322, loss 1.01366, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 525/599 (start=33600, end=33664)\n",
      "2018-11-27T04:28:06.092427: step 2323, loss 1.33849, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 526/599 (start=33664, end=33728)\n",
      "2018-11-27T04:28:06.442620: step 2324, loss 0.936582, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 527/599 (start=33728, end=33792)\n",
      "2018-11-27T04:28:06.764458: step 2325, loss 1.08375, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 528/599 (start=33792, end=33856)\n",
      "2018-11-27T04:28:07.101735: step 2326, loss 1.394, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 529/599 (start=33856, end=33920)\n",
      "2018-11-27T04:28:07.415941: step 2327, loss 1.00349, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 530/599 (start=33920, end=33984)\n",
      "2018-11-27T04:28:07.752677: step 2328, loss 1.24401, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 531/599 (start=33984, end=34048)\n",
      "2018-11-27T04:28:08.068853: step 2329, loss 1.36928, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 532/599 (start=34048, end=34112)\n",
      "2018-11-27T04:28:08.405520: step 2330, loss 0.879017, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 533/599 (start=34112, end=34176)\n",
      "2018-11-27T04:28:08.745270: step 2331, loss 1.2579, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 534/599 (start=34176, end=34240)\n",
      "2018-11-27T04:28:09.084196: step 2332, loss 0.977452, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 535/599 (start=34240, end=34304)\n",
      "2018-11-27T04:28:09.425585: step 2333, loss 1.25944, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 536/599 (start=34304, end=34368)\n",
      "2018-11-27T04:28:09.778716: step 2334, loss 1.17743, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 537/599 (start=34368, end=34432)\n",
      "2018-11-27T04:28:10.089375: step 2335, loss 1.21623, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 538/599 (start=34432, end=34496)\n",
      "2018-11-27T04:28:10.432599: step 2336, loss 0.892148, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 539/599 (start=34496, end=34560)\n",
      "2018-11-27T04:28:10.762307: step 2337, loss 1.19691, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 540/599 (start=34560, end=34624)\n",
      "2018-11-27T04:28:11.093152: step 2338, loss 1.13786, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 541/599 (start=34624, end=34688)\n",
      "2018-11-27T04:28:11.439416: step 2339, loss 1.02486, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 542/599 (start=34688, end=34752)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:28:11.766432: step 2340, loss 1.19997, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 543/599 (start=34752, end=34816)\n",
      "2018-11-27T04:28:12.093958: step 2341, loss 1.12138, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 544/599 (start=34816, end=34880)\n",
      "2018-11-27T04:28:12.432313: step 2342, loss 1.20974, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 545/599 (start=34880, end=34944)\n",
      "2018-11-27T04:28:12.770142: step 2343, loss 1.26674, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 546/599 (start=34944, end=35008)\n",
      "2018-11-27T04:28:13.080926: step 2344, loss 1.00405, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 547/599 (start=35008, end=35072)\n",
      "2018-11-27T04:28:13.423292: step 2345, loss 1.24401, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 548/599 (start=35072, end=35136)\n",
      "2018-11-27T04:28:13.761393: step 2346, loss 1.01158, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 549/599 (start=35136, end=35200)\n",
      "2018-11-27T04:28:14.077328: step 2347, loss 1.31277, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 550/599 (start=35200, end=35264)\n",
      "2018-11-27T04:28:14.420951: step 2348, loss 1.18978, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 551/599 (start=35264, end=35328)\n",
      "2018-11-27T04:28:14.760490: step 2349, loss 1.05371, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 552/599 (start=35328, end=35392)\n",
      "2018-11-27T04:28:15.107630: step 2350, loss 1.17553, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 553/599 (start=35392, end=35456)\n",
      "2018-11-27T04:28:15.443400: step 2351, loss 0.871231, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 554/599 (start=35456, end=35520)\n",
      "2018-11-27T04:28:15.783043: step 2352, loss 1.33211, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 555/599 (start=35520, end=35584)\n",
      "2018-11-27T04:28:16.114757: step 2353, loss 1.05852, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 556/599 (start=35584, end=35648)\n",
      "2018-11-27T04:28:16.448144: step 2354, loss 1.12561, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 557/599 (start=35648, end=35712)\n",
      "2018-11-27T04:28:16.786546: step 2355, loss 1.20588, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 558/599 (start=35712, end=35776)\n",
      "2018-11-27T04:28:17.125394: step 2356, loss 1.12441, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 559/599 (start=35776, end=35840)\n",
      "2018-11-27T04:28:17.444148: step 2357, loss 0.95013, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 560/599 (start=35840, end=35904)\n",
      "2018-11-27T04:28:17.778747: step 2358, loss 1.1586, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 561/599 (start=35904, end=35968)\n",
      "2018-11-27T04:28:18.113206: step 2359, loss 1.10227, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 562/599 (start=35968, end=36032)\n",
      "2018-11-27T04:28:18.454631: step 2360, loss 1.04551, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 563/599 (start=36032, end=36096)\n",
      "2018-11-27T04:28:18.778409: step 2361, loss 1.27422, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 564/599 (start=36096, end=36160)\n",
      "2018-11-27T04:28:19.085423: step 2362, loss 1.09954, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 565/599 (start=36160, end=36224)\n",
      "2018-11-27T04:28:19.391484: step 2363, loss 1.39008, acc 0.578125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 566/599 (start=36224, end=36288)\n",
      "2018-11-27T04:28:19.721502: step 2364, loss 1.32244, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 567/599 (start=36288, end=36352)\n",
      "2018-11-27T04:28:20.040024: step 2365, loss 1.25536, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 568/599 (start=36352, end=36416)\n",
      "2018-11-27T04:28:20.377574: step 2366, loss 0.950266, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 569/599 (start=36416, end=36480)\n",
      "2018-11-27T04:28:20.719826: step 2367, loss 1.28752, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 570/599 (start=36480, end=36544)\n",
      "2018-11-27T04:28:21.072742: step 2368, loss 1.1788, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 571/599 (start=36544, end=36608)\n",
      "2018-11-27T04:28:21.415292: step 2369, loss 1.05098, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 572/599 (start=36608, end=36672)\n",
      "2018-11-27T04:28:21.749156: step 2370, loss 1.23713, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 573/599 (start=36672, end=36736)\n",
      "2018-11-27T04:28:22.093685: step 2371, loss 1.3678, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 574/599 (start=36736, end=36800)\n",
      "2018-11-27T04:28:22.444451: step 2372, loss 1.04577, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 575/599 (start=36800, end=36864)\n",
      "2018-11-27T04:28:22.790945: step 2373, loss 1.10315, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 576/599 (start=36864, end=36928)\n",
      "2018-11-27T04:28:23.129044: step 2374, loss 1.15615, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 577/599 (start=36928, end=36992)\n",
      "2018-11-27T04:28:23.477095: step 2375, loss 1.03079, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 578/599 (start=36992, end=37056)\n",
      "2018-11-27T04:28:23.810930: step 2376, loss 0.975651, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 579/599 (start=37056, end=37120)\n",
      "2018-11-27T04:28:24.132226: step 2377, loss 1.01482, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 580/599 (start=37120, end=37184)\n",
      "2018-11-27T04:28:24.476488: step 2378, loss 1.4046, acc 0.59375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 581/599 (start=37184, end=37248)\n",
      "2018-11-27T04:28:24.828225: step 2379, loss 1.13058, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 582/599 (start=37248, end=37312)\n",
      "2018-11-27T04:28:25.180298: step 2380, loss 1.06757, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 583/599 (start=37312, end=37376)\n",
      "2018-11-27T04:28:25.525827: step 2381, loss 0.965269, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 584/599 (start=37376, end=37440)\n",
      "2018-11-27T04:28:25.847606: step 2382, loss 0.891584, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 585/599 (start=37440, end=37504)\n",
      "2018-11-27T04:28:26.184808: step 2383, loss 1.05857, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 586/599 (start=37504, end=37568)\n",
      "2018-11-27T04:28:26.519851: step 2384, loss 1.01878, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 587/599 (start=37568, end=37632)\n",
      "2018-11-27T04:28:26.840081: step 2385, loss 1.35252, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 588/599 (start=37632, end=37696)\n",
      "2018-11-27T04:28:27.180291: step 2386, loss 1.28217, acc 0.546875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 589/599 (start=37696, end=37760)\n",
      "2018-11-27T04:28:27.502264: step 2387, loss 1.08096, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 590/599 (start=37760, end=37824)\n",
      "2018-11-27T04:28:27.840888: step 2388, loss 1.13167, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 591/599 (start=37824, end=37888)\n",
      "2018-11-27T04:28:28.179575: step 2389, loss 1.30034, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 592/599 (start=37888, end=37952)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:28:28.519356: step 2390, loss 1.28189, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 593/599 (start=37952, end=38016)\n",
      "2018-11-27T04:28:28.864622: step 2391, loss 1.10402, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 594/599 (start=38016, end=38080)\n",
      "2018-11-27T04:28:29.198345: step 2392, loss 1.41196, acc 0.640625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 595/599 (start=38080, end=38144)\n",
      "2018-11-27T04:28:29.543354: step 2393, loss 1.25708, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 596/599 (start=38144, end=38208)\n",
      "2018-11-27T04:28:29.878964: step 2394, loss 1.01494, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 597/599 (start=38208, end=38272)\n",
      "2018-11-27T04:28:30.185958: step 2395, loss 0.818828, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 3/10, Batch 598/599 (start=38272, end=38281)\n",
      "2018-11-27T04:28:30.410074: step 2396, loss 0.586783, acc 0.777778\n",
      "***********************************************\n",
      "Epoch 4/10\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 0/599 (start=0, end=64)\n",
      "2018-11-27T04:28:30.755238: step 2397, loss 0.816551, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 1/599 (start=64, end=128)\n",
      "2018-11-27T04:28:31.095528: step 2398, loss 0.958352, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 2/599 (start=128, end=192)\n",
      "2018-11-27T04:28:31.413851: step 2399, loss 0.655294, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 3/599 (start=192, end=256)\n",
      "2018-11-27T04:28:31.727815: step 2400, loss 0.882541, acc 0.796875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:28:38.086363: step 2400, loss 1.74956, acc 0.504036\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-2400\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 4/599 (start=256, end=320)\n",
      "2018-11-27T04:28:38.825308: step 2401, loss 0.840514, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 5/599 (start=320, end=384)\n",
      "2018-11-27T04:28:39.149726: step 2402, loss 0.89559, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 6/599 (start=384, end=448)\n",
      "2018-11-27T04:28:39.494980: step 2403, loss 0.947638, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 7/599 (start=448, end=512)\n",
      "2018-11-27T04:28:39.830762: step 2404, loss 1.09436, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 8/599 (start=512, end=576)\n",
      "2018-11-27T04:28:40.146496: step 2405, loss 0.996725, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 9/599 (start=576, end=640)\n",
      "2018-11-27T04:28:40.466588: step 2406, loss 0.638067, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 10/599 (start=640, end=704)\n",
      "2018-11-27T04:28:40.799956: step 2407, loss 0.647937, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 11/599 (start=704, end=768)\n",
      "2018-11-27T04:28:41.129630: step 2408, loss 1.00072, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 12/599 (start=768, end=832)\n",
      "2018-11-27T04:28:41.458390: step 2409, loss 0.841229, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 13/599 (start=832, end=896)\n",
      "2018-11-27T04:28:41.797520: step 2410, loss 1.06403, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 14/599 (start=896, end=960)\n",
      "2018-11-27T04:28:42.118114: step 2411, loss 0.990264, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 15/599 (start=960, end=1024)\n",
      "2018-11-27T04:28:42.460831: step 2412, loss 0.625391, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 16/599 (start=1024, end=1088)\n",
      "2018-11-27T04:28:42.780826: step 2413, loss 0.748322, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 17/599 (start=1088, end=1152)\n",
      "2018-11-27T04:28:43.097439: step 2414, loss 0.604964, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 18/599 (start=1152, end=1216)\n",
      "2018-11-27T04:28:43.413239: step 2415, loss 0.668217, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 19/599 (start=1216, end=1280)\n",
      "2018-11-27T04:28:43.750736: step 2416, loss 0.64869, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 20/599 (start=1280, end=1344)\n",
      "2018-11-27T04:28:44.069271: step 2417, loss 0.711841, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 21/599 (start=1344, end=1408)\n",
      "2018-11-27T04:28:44.391355: step 2418, loss 0.785456, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 22/599 (start=1408, end=1472)\n",
      "2018-11-27T04:28:44.730174: step 2419, loss 0.673986, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 23/599 (start=1472, end=1536)\n",
      "2018-11-27T04:28:45.049560: step 2420, loss 0.830718, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 24/599 (start=1536, end=1600)\n",
      "2018-11-27T04:28:45.379192: step 2421, loss 0.945896, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 25/599 (start=1600, end=1664)\n",
      "2018-11-27T04:28:45.689998: step 2422, loss 0.77101, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 26/599 (start=1664, end=1728)\n",
      "2018-11-27T04:28:46.024822: step 2423, loss 0.939558, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 27/599 (start=1728, end=1792)\n",
      "2018-11-27T04:28:46.366157: step 2424, loss 0.913775, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 28/599 (start=1792, end=1856)\n",
      "2018-11-27T04:28:46.684828: step 2425, loss 0.924084, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 29/599 (start=1856, end=1920)\n",
      "2018-11-27T04:28:47.009314: step 2426, loss 0.957348, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 30/599 (start=1920, end=1984)\n",
      "2018-11-27T04:28:47.349102: step 2427, loss 0.974692, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 31/599 (start=1984, end=2048)\n",
      "2018-11-27T04:28:47.669464: step 2428, loss 0.886626, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 32/599 (start=2048, end=2112)\n",
      "2018-11-27T04:28:47.985005: step 2429, loss 0.881334, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 33/599 (start=2112, end=2176)\n",
      "2018-11-27T04:28:48.308149: step 2430, loss 0.987246, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 34/599 (start=2176, end=2240)\n",
      "2018-11-27T04:28:48.635975: step 2431, loss 0.951334, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 35/599 (start=2240, end=2304)\n",
      "2018-11-27T04:28:48.982597: step 2432, loss 0.789777, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 36/599 (start=2304, end=2368)\n",
      "2018-11-27T04:28:49.313471: step 2433, loss 0.884659, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 37/599 (start=2368, end=2432)\n",
      "2018-11-27T04:28:49.633848: step 2434, loss 0.835289, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 38/599 (start=2432, end=2496)\n",
      "2018-11-27T04:28:49.975006: step 2435, loss 0.800612, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 39/599 (start=2496, end=2560)\n",
      "2018-11-27T04:28:50.281177: step 2436, loss 0.84565, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 40/599 (start=2560, end=2624)\n",
      "2018-11-27T04:28:50.596476: step 2437, loss 1.00802, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 41/599 (start=2624, end=2688)\n",
      "2018-11-27T04:28:50.948073: step 2438, loss 0.748709, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 42/599 (start=2688, end=2752)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:28:51.298501: step 2439, loss 0.675469, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 43/599 (start=2752, end=2816)\n",
      "2018-11-27T04:28:51.641151: step 2440, loss 0.787831, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 44/599 (start=2816, end=2880)\n",
      "2018-11-27T04:28:51.989321: step 2441, loss 0.865607, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 45/599 (start=2880, end=2944)\n",
      "2018-11-27T04:28:52.313952: step 2442, loss 1.10684, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 46/599 (start=2944, end=3008)\n",
      "2018-11-27T04:28:52.651296: step 2443, loss 0.707162, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 47/599 (start=3008, end=3072)\n",
      "2018-11-27T04:28:52.965581: step 2444, loss 0.883723, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 48/599 (start=3072, end=3136)\n",
      "2018-11-27T04:28:53.292135: step 2445, loss 0.847548, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 49/599 (start=3136, end=3200)\n",
      "2018-11-27T04:28:53.621032: step 2446, loss 1.07688, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 50/599 (start=3200, end=3264)\n",
      "2018-11-27T04:28:53.952780: step 2447, loss 0.768547, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 51/599 (start=3264, end=3328)\n",
      "2018-11-27T04:28:54.291105: step 2448, loss 0.915526, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 52/599 (start=3328, end=3392)\n",
      "2018-11-27T04:28:54.623823: step 2449, loss 0.989912, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 53/599 (start=3392, end=3456)\n",
      "2018-11-27T04:28:54.963365: step 2450, loss 1.01812, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 54/599 (start=3456, end=3520)\n",
      "2018-11-27T04:28:55.272795: step 2451, loss 0.816944, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 55/599 (start=3520, end=3584)\n",
      "2018-11-27T04:28:55.601119: step 2452, loss 0.871799, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 56/599 (start=3584, end=3648)\n",
      "2018-11-27T04:28:55.921098: step 2453, loss 0.987733, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 57/599 (start=3648, end=3712)\n",
      "2018-11-27T04:28:56.252288: step 2454, loss 1.03049, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 58/599 (start=3712, end=3776)\n",
      "2018-11-27T04:28:56.579598: step 2455, loss 0.774725, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 59/599 (start=3776, end=3840)\n",
      "2018-11-27T04:28:56.918951: step 2456, loss 0.795071, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 60/599 (start=3840, end=3904)\n",
      "2018-11-27T04:28:57.238478: step 2457, loss 0.709036, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 61/599 (start=3904, end=3968)\n",
      "2018-11-27T04:28:57.557186: step 2458, loss 0.944546, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 62/599 (start=3968, end=4032)\n",
      "2018-11-27T04:28:57.905330: step 2459, loss 0.926283, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 63/599 (start=4032, end=4096)\n",
      "2018-11-27T04:28:58.226685: step 2460, loss 0.827787, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 64/599 (start=4096, end=4160)\n",
      "2018-11-27T04:28:58.550620: step 2461, loss 0.856909, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 65/599 (start=4160, end=4224)\n",
      "2018-11-27T04:28:58.884963: step 2462, loss 0.857737, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 66/599 (start=4224, end=4288)\n",
      "2018-11-27T04:28:59.232480: step 2463, loss 0.823361, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 67/599 (start=4288, end=4352)\n",
      "2018-11-27T04:28:59.583349: step 2464, loss 0.865153, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 68/599 (start=4352, end=4416)\n",
      "2018-11-27T04:28:59.902404: step 2465, loss 0.996583, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 69/599 (start=4416, end=4480)\n",
      "2018-11-27T04:29:00.223115: step 2466, loss 0.822045, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 70/599 (start=4480, end=4544)\n",
      "2018-11-27T04:29:00.562622: step 2467, loss 0.679283, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 71/599 (start=4544, end=4608)\n",
      "2018-11-27T04:29:00.902839: step 2468, loss 0.73456, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 72/599 (start=4608, end=4672)\n",
      "2018-11-27T04:29:01.229821: step 2469, loss 0.751986, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 73/599 (start=4672, end=4736)\n",
      "2018-11-27T04:29:01.579095: step 2470, loss 1.00874, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 74/599 (start=4736, end=4800)\n",
      "2018-11-27T04:29:01.898437: step 2471, loss 0.751788, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 75/599 (start=4800, end=4864)\n",
      "2018-11-27T04:29:02.230341: step 2472, loss 0.619537, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 76/599 (start=4864, end=4928)\n",
      "2018-11-27T04:29:02.568335: step 2473, loss 0.756882, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 77/599 (start=4928, end=4992)\n",
      "2018-11-27T04:29:02.890690: step 2474, loss 0.782373, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 78/599 (start=4992, end=5056)\n",
      "2018-11-27T04:29:03.210783: step 2475, loss 0.872354, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 79/599 (start=5056, end=5120)\n",
      "2018-11-27T04:29:03.541940: step 2476, loss 0.685549, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 80/599 (start=5120, end=5184)\n",
      "2018-11-27T04:29:03.854827: step 2477, loss 0.74209, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 81/599 (start=5184, end=5248)\n",
      "2018-11-27T04:29:04.175480: step 2478, loss 0.840333, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 82/599 (start=5248, end=5312)\n",
      "2018-11-27T04:29:04.526747: step 2479, loss 1.01322, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 83/599 (start=5312, end=5376)\n",
      "2018-11-27T04:29:04.859016: step 2480, loss 0.744796, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 84/599 (start=5376, end=5440)\n",
      "2018-11-27T04:29:05.207570: step 2481, loss 0.691391, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 85/599 (start=5440, end=5504)\n",
      "2018-11-27T04:29:05.547929: step 2482, loss 0.809675, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 86/599 (start=5504, end=5568)\n",
      "2018-11-27T04:29:05.861307: step 2483, loss 1.01593, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 87/599 (start=5568, end=5632)\n",
      "2018-11-27T04:29:06.189938: step 2484, loss 0.65682, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 88/599 (start=5632, end=5696)\n",
      "2018-11-27T04:29:06.547273: step 2485, loss 0.817203, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 89/599 (start=5696, end=5760)\n",
      "2018-11-27T04:29:06.874408: step 2486, loss 0.900965, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 90/599 (start=5760, end=5824)\n",
      "2018-11-27T04:29:07.210108: step 2487, loss 0.723344, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 91/599 (start=5824, end=5888)\n",
      "2018-11-27T04:29:07.524427: step 2488, loss 0.850923, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 92/599 (start=5888, end=5952)\n",
      "2018-11-27T04:29:07.859846: step 2489, loss 0.899079, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 93/599 (start=5952, end=6016)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:29:08.191256: step 2490, loss 0.831702, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 94/599 (start=6016, end=6080)\n",
      "2018-11-27T04:29:08.500900: step 2491, loss 0.627162, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 95/599 (start=6080, end=6144)\n",
      "2018-11-27T04:29:08.846518: step 2492, loss 0.680444, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 96/599 (start=6144, end=6208)\n",
      "2018-11-27T04:29:09.187821: step 2493, loss 0.690476, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 97/599 (start=6208, end=6272)\n",
      "2018-11-27T04:29:09.521453: step 2494, loss 0.806113, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 98/599 (start=6272, end=6336)\n",
      "2018-11-27T04:29:09.849217: step 2495, loss 0.883092, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 99/599 (start=6336, end=6400)\n",
      "2018-11-27T04:29:10.196061: step 2496, loss 0.9547, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 100/599 (start=6400, end=6464)\n",
      "2018-11-27T04:29:10.521426: step 2497, loss 0.865608, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 101/599 (start=6464, end=6528)\n",
      "2018-11-27T04:29:10.841672: step 2498, loss 0.926857, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 102/599 (start=6528, end=6592)\n",
      "2018-11-27T04:29:11.157831: step 2499, loss 0.847584, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 103/599 (start=6592, end=6656)\n",
      "2018-11-27T04:29:11.469864: step 2500, loss 0.730444, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:29:17.565379: step 2500, loss 1.77202, acc 0.512499\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-2500\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 104/599 (start=6656, end=6720)\n",
      "2018-11-27T04:29:18.327632: step 2501, loss 0.815571, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 105/599 (start=6720, end=6784)\n",
      "2018-11-27T04:29:18.635936: step 2502, loss 0.768108, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 106/599 (start=6784, end=6848)\n",
      "2018-11-27T04:29:18.984571: step 2503, loss 0.903837, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 107/599 (start=6848, end=6912)\n",
      "2018-11-27T04:29:19.313468: step 2504, loss 0.864837, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 108/599 (start=6912, end=6976)\n",
      "2018-11-27T04:29:19.627627: step 2505, loss 0.599442, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 109/599 (start=6976, end=7040)\n",
      "2018-11-27T04:29:19.965267: step 2506, loss 1.02445, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 110/599 (start=7040, end=7104)\n",
      "2018-11-27T04:29:20.306430: step 2507, loss 0.823862, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 111/599 (start=7104, end=7168)\n",
      "2018-11-27T04:29:20.634311: step 2508, loss 1.09123, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 112/599 (start=7168, end=7232)\n",
      "2018-11-27T04:29:20.961803: step 2509, loss 0.619152, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 113/599 (start=7232, end=7296)\n",
      "2018-11-27T04:29:21.288180: step 2510, loss 0.919071, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 114/599 (start=7296, end=7360)\n",
      "2018-11-27T04:29:21.605472: step 2511, loss 1.01726, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 115/599 (start=7360, end=7424)\n",
      "2018-11-27T04:29:21.953177: step 2512, loss 0.922861, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 116/599 (start=7424, end=7488)\n",
      "2018-11-27T04:29:22.287002: step 2513, loss 1.09845, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 117/599 (start=7488, end=7552)\n",
      "2018-11-27T04:29:22.607684: step 2514, loss 0.862566, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 118/599 (start=7552, end=7616)\n",
      "2018-11-27T04:29:22.943621: step 2515, loss 0.62423, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 119/599 (start=7616, end=7680)\n",
      "2018-11-27T04:29:23.281818: step 2516, loss 0.859867, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 120/599 (start=7680, end=7744)\n",
      "2018-11-27T04:29:23.601194: step 2517, loss 0.733159, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 121/599 (start=7744, end=7808)\n",
      "2018-11-27T04:29:23.929084: step 2518, loss 0.76131, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 122/599 (start=7808, end=7872)\n",
      "2018-11-27T04:29:24.246569: step 2519, loss 1.03503, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 123/599 (start=7872, end=7936)\n",
      "2018-11-27T04:29:24.585236: step 2520, loss 1.09036, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 124/599 (start=7936, end=8000)\n",
      "2018-11-27T04:29:24.913149: step 2521, loss 0.681505, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 125/599 (start=8000, end=8064)\n",
      "2018-11-27T04:29:25.239265: step 2522, loss 0.92765, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 126/599 (start=8064, end=8128)\n",
      "2018-11-27T04:29:25.583213: step 2523, loss 0.937038, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 127/599 (start=8128, end=8192)\n",
      "2018-11-27T04:29:25.922944: step 2524, loss 0.680498, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 128/599 (start=8192, end=8256)\n",
      "2018-11-27T04:29:26.227912: step 2525, loss 0.953725, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 129/599 (start=8256, end=8320)\n",
      "2018-11-27T04:29:26.577214: step 2526, loss 0.874863, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 130/599 (start=8320, end=8384)\n",
      "2018-11-27T04:29:26.912542: step 2527, loss 1.09442, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 131/599 (start=8384, end=8448)\n",
      "2018-11-27T04:29:27.250976: step 2528, loss 0.974402, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 132/599 (start=8448, end=8512)\n",
      "2018-11-27T04:29:27.562679: step 2529, loss 0.984007, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 133/599 (start=8512, end=8576)\n",
      "2018-11-27T04:29:27.871740: step 2530, loss 0.956184, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 134/599 (start=8576, end=8640)\n",
      "2018-11-27T04:29:28.218148: step 2531, loss 0.653959, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 135/599 (start=8640, end=8704)\n",
      "2018-11-27T04:29:28.537591: step 2532, loss 1.01167, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 136/599 (start=8704, end=8768)\n",
      "2018-11-27T04:29:28.856040: step 2533, loss 0.913869, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 137/599 (start=8768, end=8832)\n",
      "2018-11-27T04:29:29.197424: step 2534, loss 0.950012, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 138/599 (start=8832, end=8896)\n",
      "2018-11-27T04:29:29.519732: step 2535, loss 0.922744, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 139/599 (start=8896, end=8960)\n",
      "2018-11-27T04:29:29.858760: step 2536, loss 0.747656, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 140/599 (start=8960, end=9024)\n",
      "2018-11-27T04:29:30.200905: step 2537, loss 0.676746, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 141/599 (start=9024, end=9088)\n",
      "2018-11-27T04:29:30.528118: step 2538, loss 0.902203, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 142/599 (start=9088, end=9152)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:29:30.882574: step 2539, loss 0.787905, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 143/599 (start=9152, end=9216)\n",
      "2018-11-27T04:29:31.202375: step 2540, loss 0.815294, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 144/599 (start=9216, end=9280)\n",
      "2018-11-27T04:29:31.532252: step 2541, loss 0.963603, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 145/599 (start=9280, end=9344)\n",
      "2018-11-27T04:29:31.859688: step 2542, loss 0.883323, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 146/599 (start=9344, end=9408)\n",
      "2018-11-27T04:29:32.184590: step 2543, loss 0.98663, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 147/599 (start=9408, end=9472)\n",
      "2018-11-27T04:29:32.523824: step 2544, loss 0.799061, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 148/599 (start=9472, end=9536)\n",
      "2018-11-27T04:29:32.874469: step 2545, loss 0.880256, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 149/599 (start=9536, end=9600)\n",
      "2018-11-27T04:29:33.238862: step 2546, loss 0.749087, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 150/599 (start=9600, end=9664)\n",
      "2018-11-27T04:29:33.549670: step 2547, loss 0.679837, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 151/599 (start=9664, end=9728)\n",
      "2018-11-27T04:29:33.864737: step 2548, loss 0.812255, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 152/599 (start=9728, end=9792)\n",
      "2018-11-27T04:29:34.200604: step 2549, loss 0.740179, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 153/599 (start=9792, end=9856)\n",
      "2018-11-27T04:29:34.528707: step 2550, loss 0.915954, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 154/599 (start=9856, end=9920)\n",
      "2018-11-27T04:29:34.854295: step 2551, loss 0.862231, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 155/599 (start=9920, end=9984)\n",
      "2018-11-27T04:29:35.197211: step 2552, loss 0.763908, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 156/599 (start=9984, end=10048)\n",
      "2018-11-27T04:29:35.535969: step 2553, loss 0.884895, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 157/599 (start=10048, end=10112)\n",
      "2018-11-27T04:29:35.874519: step 2554, loss 0.828961, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 158/599 (start=10112, end=10176)\n",
      "2018-11-27T04:29:36.206694: step 2555, loss 1.06265, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 159/599 (start=10176, end=10240)\n",
      "2018-11-27T04:29:36.543070: step 2556, loss 0.872083, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 160/599 (start=10240, end=10304)\n",
      "2018-11-27T04:29:36.861278: step 2557, loss 0.760069, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 161/599 (start=10304, end=10368)\n",
      "2018-11-27T04:29:37.191008: step 2558, loss 0.82358, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 162/599 (start=10368, end=10432)\n",
      "2018-11-27T04:29:37.551415: step 2559, loss 0.779835, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 163/599 (start=10432, end=10496)\n",
      "2018-11-27T04:29:37.858478: step 2560, loss 0.786572, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 164/599 (start=10496, end=10560)\n",
      "2018-11-27T04:29:38.198403: step 2561, loss 0.743711, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 165/599 (start=10560, end=10624)\n",
      "2018-11-27T04:29:38.526382: step 2562, loss 0.735046, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 166/599 (start=10624, end=10688)\n",
      "2018-11-27T04:29:38.853563: step 2563, loss 0.745479, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 167/599 (start=10688, end=10752)\n",
      "2018-11-27T04:29:39.191951: step 2564, loss 0.999597, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 168/599 (start=10752, end=10816)\n",
      "2018-11-27T04:29:39.532296: step 2565, loss 0.858618, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 169/599 (start=10816, end=10880)\n",
      "2018-11-27T04:29:39.843659: step 2566, loss 0.674141, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 170/599 (start=10880, end=10944)\n",
      "2018-11-27T04:29:40.189076: step 2567, loss 0.72206, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 171/599 (start=10944, end=11008)\n",
      "2018-11-27T04:29:40.527139: step 2568, loss 0.91273, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 172/599 (start=11008, end=11072)\n",
      "2018-11-27T04:29:40.842037: step 2569, loss 0.696683, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 173/599 (start=11072, end=11136)\n",
      "2018-11-27T04:29:41.194782: step 2570, loss 0.797473, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 174/599 (start=11136, end=11200)\n",
      "2018-11-27T04:29:41.531837: step 2571, loss 0.764896, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 175/599 (start=11200, end=11264)\n",
      "2018-11-27T04:29:41.850680: step 2572, loss 0.803179, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 176/599 (start=11264, end=11328)\n",
      "2018-11-27T04:29:42.165836: step 2573, loss 0.993791, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 177/599 (start=11328, end=11392)\n",
      "2018-11-27T04:29:42.492084: step 2574, loss 0.743851, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 178/599 (start=11392, end=11456)\n",
      "2018-11-27T04:29:42.839824: step 2575, loss 0.5614, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 179/599 (start=11456, end=11520)\n",
      "2018-11-27T04:29:43.157775: step 2576, loss 0.692287, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 180/599 (start=11520, end=11584)\n",
      "2018-11-27T04:29:43.495289: step 2577, loss 0.995903, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 181/599 (start=11584, end=11648)\n",
      "2018-11-27T04:29:43.818461: step 2578, loss 0.821967, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 182/599 (start=11648, end=11712)\n",
      "2018-11-27T04:29:44.148588: step 2579, loss 0.920478, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 183/599 (start=11712, end=11776)\n",
      "2018-11-27T04:29:44.481183: step 2580, loss 0.655646, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 184/599 (start=11776, end=11840)\n",
      "2018-11-27T04:29:44.828285: step 2581, loss 0.986608, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 185/599 (start=11840, end=11904)\n",
      "2018-11-27T04:29:45.139726: step 2582, loss 0.747724, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 186/599 (start=11904, end=11968)\n",
      "2018-11-27T04:29:45.493456: step 2583, loss 0.726198, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 187/599 (start=11968, end=12032)\n",
      "2018-11-27T04:29:45.809285: step 2584, loss 1.16762, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 188/599 (start=12032, end=12096)\n",
      "2018-11-27T04:29:46.145671: step 2585, loss 0.886863, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 189/599 (start=12096, end=12160)\n",
      "2018-11-27T04:29:46.466984: step 2586, loss 0.876768, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 190/599 (start=12160, end=12224)\n",
      "2018-11-27T04:29:46.807284: step 2587, loss 0.838733, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 191/599 (start=12224, end=12288)\n",
      "2018-11-27T04:29:47.118771: step 2588, loss 0.687758, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 192/599 (start=12288, end=12352)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:29:47.436112: step 2589, loss 0.903404, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 193/599 (start=12352, end=12416)\n",
      "2018-11-27T04:29:47.760257: step 2590, loss 0.89936, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 194/599 (start=12416, end=12480)\n",
      "2018-11-27T04:29:48.088208: step 2591, loss 0.631155, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 195/599 (start=12480, end=12544)\n",
      "2018-11-27T04:29:48.426389: step 2592, loss 1.10553, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 196/599 (start=12544, end=12608)\n",
      "2018-11-27T04:29:48.769606: step 2593, loss 0.862021, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 197/599 (start=12608, end=12672)\n",
      "2018-11-27T04:29:49.081916: step 2594, loss 0.94847, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 198/599 (start=12672, end=12736)\n",
      "2018-11-27T04:29:49.421505: step 2595, loss 0.926614, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 199/599 (start=12736, end=12800)\n",
      "2018-11-27T04:29:49.738020: step 2596, loss 0.928887, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 200/599 (start=12800, end=12864)\n",
      "2018-11-27T04:29:50.077347: step 2597, loss 1.04451, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 201/599 (start=12864, end=12928)\n",
      "2018-11-27T04:29:50.390043: step 2598, loss 0.832786, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 202/599 (start=12928, end=12992)\n",
      "2018-11-27T04:29:50.698320: step 2599, loss 0.804769, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 203/599 (start=12992, end=13056)\n",
      "2018-11-27T04:29:51.021352: step 2600, loss 0.906151, acc 0.78125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:29:57.294332: step 2600, loss 1.79557, acc 0.512186\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-2600\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 204/599 (start=13056, end=13120)\n",
      "2018-11-27T04:29:58.028558: step 2601, loss 0.982591, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 205/599 (start=13120, end=13184)\n",
      "2018-11-27T04:29:58.354959: step 2602, loss 0.878334, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 206/599 (start=13184, end=13248)\n",
      "2018-11-27T04:29:58.692814: step 2603, loss 0.83697, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 207/599 (start=13248, end=13312)\n",
      "2018-11-27T04:29:59.021976: step 2604, loss 0.715956, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 208/599 (start=13312, end=13376)\n",
      "2018-11-27T04:29:59.349967: step 2605, loss 0.97078, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 209/599 (start=13376, end=13440)\n",
      "2018-11-27T04:29:59.699924: step 2606, loss 0.884448, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 210/599 (start=13440, end=13504)\n",
      "2018-11-27T04:30:00.039889: step 2607, loss 0.954071, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 211/599 (start=13504, end=13568)\n",
      "2018-11-27T04:30:00.382339: step 2608, loss 0.750294, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 212/599 (start=13568, end=13632)\n",
      "2018-11-27T04:30:00.719622: step 2609, loss 0.775435, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 213/599 (start=13632, end=13696)\n",
      "2018-11-27T04:30:01.038719: step 2610, loss 1.00902, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 214/599 (start=13696, end=13760)\n",
      "2018-11-27T04:30:01.369928: step 2611, loss 0.829932, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 215/599 (start=13760, end=13824)\n",
      "2018-11-27T04:30:01.709870: step 2612, loss 1.14026, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 216/599 (start=13824, end=13888)\n",
      "2018-11-27T04:30:02.056874: step 2613, loss 0.77094, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 217/599 (start=13888, end=13952)\n",
      "2018-11-27T04:30:02.390392: step 2614, loss 0.821105, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 218/599 (start=13952, end=14016)\n",
      "2018-11-27T04:30:02.728169: step 2615, loss 0.842632, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 219/599 (start=14016, end=14080)\n",
      "2018-11-27T04:30:03.063886: step 2616, loss 1.11988, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 220/599 (start=14080, end=14144)\n",
      "2018-11-27T04:30:03.385598: step 2617, loss 0.959252, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 221/599 (start=14144, end=14208)\n",
      "2018-11-27T04:30:03.721520: step 2618, loss 0.797886, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 222/599 (start=14208, end=14272)\n",
      "2018-11-27T04:30:04.040569: step 2619, loss 0.848037, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 223/599 (start=14272, end=14336)\n",
      "2018-11-27T04:30:04.355803: step 2620, loss 0.977383, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 224/599 (start=14336, end=14400)\n",
      "2018-11-27T04:30:04.676688: step 2621, loss 1.01659, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 225/599 (start=14400, end=14464)\n",
      "2018-11-27T04:30:05.000644: step 2622, loss 0.82687, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 226/599 (start=14464, end=14528)\n",
      "2018-11-27T04:30:05.321761: step 2623, loss 0.788833, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 227/599 (start=14528, end=14592)\n",
      "2018-11-27T04:30:05.642553: step 2624, loss 1.15038, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 228/599 (start=14592, end=14656)\n",
      "2018-11-27T04:30:05.968588: step 2625, loss 0.748726, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 229/599 (start=14656, end=14720)\n",
      "2018-11-27T04:30:06.302218: step 2626, loss 1.07037, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 230/599 (start=14720, end=14784)\n",
      "2018-11-27T04:30:06.621530: step 2627, loss 1.00311, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 231/599 (start=14784, end=14848)\n",
      "2018-11-27T04:30:06.974833: step 2628, loss 1.0023, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 232/599 (start=14848, end=14912)\n",
      "2018-11-27T04:30:07.308893: step 2629, loss 0.824942, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 233/599 (start=14912, end=14976)\n",
      "2018-11-27T04:30:07.648193: step 2630, loss 1.05023, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 234/599 (start=14976, end=15040)\n",
      "2018-11-27T04:30:07.975229: step 2631, loss 0.882801, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 235/599 (start=15040, end=15104)\n",
      "2018-11-27T04:30:08.312265: step 2632, loss 1.12294, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 236/599 (start=15104, end=15168)\n",
      "2018-11-27T04:30:08.644144: step 2633, loss 0.838165, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 237/599 (start=15168, end=15232)\n",
      "2018-11-27T04:30:08.985336: step 2634, loss 1.0029, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 238/599 (start=15232, end=15296)\n",
      "2018-11-27T04:30:09.303895: step 2635, loss 0.867474, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 239/599 (start=15296, end=15360)\n",
      "2018-11-27T04:30:09.628566: step 2636, loss 0.735744, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 240/599 (start=15360, end=15424)\n",
      "2018-11-27T04:30:09.960810: step 2637, loss 1.06749, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 241/599 (start=15424, end=15488)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:30:10.301693: step 2638, loss 1.22915, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 242/599 (start=15488, end=15552)\n",
      "2018-11-27T04:30:10.623223: step 2639, loss 0.915455, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 243/599 (start=15552, end=15616)\n",
      "2018-11-27T04:30:10.934049: step 2640, loss 0.836263, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 244/599 (start=15616, end=15680)\n",
      "2018-11-27T04:30:11.268840: step 2641, loss 0.818617, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 245/599 (start=15680, end=15744)\n",
      "2018-11-27T04:30:11.608093: step 2642, loss 0.860085, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 246/599 (start=15744, end=15808)\n",
      "2018-11-27T04:30:11.928226: step 2643, loss 0.99964, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 247/599 (start=15808, end=15872)\n",
      "2018-11-27T04:30:12.265815: step 2644, loss 1.02296, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 248/599 (start=15872, end=15936)\n",
      "2018-11-27T04:30:12.595668: step 2645, loss 0.945997, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 249/599 (start=15936, end=16000)\n",
      "2018-11-27T04:30:12.924379: step 2646, loss 1.03879, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 250/599 (start=16000, end=16064)\n",
      "2018-11-27T04:30:13.229838: step 2647, loss 1.07186, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 251/599 (start=16064, end=16128)\n",
      "2018-11-27T04:30:13.573366: step 2648, loss 0.923051, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 252/599 (start=16128, end=16192)\n",
      "2018-11-27T04:30:13.913157: step 2649, loss 0.818498, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 253/599 (start=16192, end=16256)\n",
      "2018-11-27T04:30:14.260061: step 2650, loss 0.745315, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 254/599 (start=16256, end=16320)\n",
      "2018-11-27T04:30:14.590479: step 2651, loss 0.705466, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 255/599 (start=16320, end=16384)\n",
      "2018-11-27T04:30:14.924388: step 2652, loss 1.00505, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 256/599 (start=16384, end=16448)\n",
      "2018-11-27T04:30:15.260021: step 2653, loss 0.996815, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 257/599 (start=16448, end=16512)\n",
      "2018-11-27T04:30:15.577601: step 2654, loss 0.686469, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 258/599 (start=16512, end=16576)\n",
      "2018-11-27T04:30:15.906511: step 2655, loss 0.960826, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 259/599 (start=16576, end=16640)\n",
      "2018-11-27T04:30:16.224604: step 2656, loss 0.850842, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 260/599 (start=16640, end=16704)\n",
      "2018-11-27T04:30:16.542513: step 2657, loss 0.804231, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 261/599 (start=16704, end=16768)\n",
      "2018-11-27T04:30:16.877422: step 2658, loss 0.816669, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 262/599 (start=16768, end=16832)\n",
      "2018-11-27T04:30:17.187572: step 2659, loss 1.08142, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 263/599 (start=16832, end=16896)\n",
      "2018-11-27T04:30:17.538524: step 2660, loss 1.09756, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 264/599 (start=16896, end=16960)\n",
      "2018-11-27T04:30:17.882199: step 2661, loss 1.02385, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 265/599 (start=16960, end=17024)\n",
      "2018-11-27T04:30:18.202595: step 2662, loss 0.963391, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 266/599 (start=17024, end=17088)\n",
      "2018-11-27T04:30:18.548216: step 2663, loss 0.980648, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 267/599 (start=17088, end=17152)\n",
      "2018-11-27T04:30:18.872276: step 2664, loss 0.652293, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 268/599 (start=17152, end=17216)\n",
      "2018-11-27T04:30:19.205453: step 2665, loss 0.869228, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 269/599 (start=17216, end=17280)\n",
      "2018-11-27T04:30:19.514556: step 2666, loss 0.659835, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 270/599 (start=17280, end=17344)\n",
      "2018-11-27T04:30:19.861196: step 2667, loss 1.15078, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 271/599 (start=17344, end=17408)\n",
      "2018-11-27T04:30:20.177145: step 2668, loss 0.997367, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 272/599 (start=17408, end=17472)\n",
      "2018-11-27T04:30:20.511379: step 2669, loss 1.22711, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 273/599 (start=17472, end=17536)\n",
      "2018-11-27T04:30:20.856862: step 2670, loss 0.905779, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 274/599 (start=17536, end=17600)\n",
      "2018-11-27T04:30:21.187854: step 2671, loss 0.758829, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 275/599 (start=17600, end=17664)\n",
      "2018-11-27T04:30:21.511841: step 2672, loss 0.89559, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 276/599 (start=17664, end=17728)\n",
      "2018-11-27T04:30:21.820311: step 2673, loss 1.16601, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 277/599 (start=17728, end=17792)\n",
      "2018-11-27T04:30:22.149424: step 2674, loss 0.683568, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 278/599 (start=17792, end=17856)\n",
      "2018-11-27T04:30:22.476081: step 2675, loss 0.828234, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 279/599 (start=17856, end=17920)\n",
      "2018-11-27T04:30:22.808427: step 2676, loss 0.793853, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 280/599 (start=17920, end=17984)\n",
      "2018-11-27T04:30:23.141509: step 2677, loss 0.980396, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 281/599 (start=17984, end=18048)\n",
      "2018-11-27T04:30:23.483044: step 2678, loss 0.701568, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 282/599 (start=18048, end=18112)\n",
      "2018-11-27T04:30:23.815958: step 2679, loss 0.778274, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 283/599 (start=18112, end=18176)\n",
      "2018-11-27T04:30:24.129887: step 2680, loss 0.533003, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 284/599 (start=18176, end=18240)\n",
      "2018-11-27T04:30:24.453642: step 2681, loss 0.788563, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 285/599 (start=18240, end=18304)\n",
      "2018-11-27T04:30:24.760386: step 2682, loss 0.631952, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 286/599 (start=18304, end=18368)\n",
      "2018-11-27T04:30:25.111213: step 2683, loss 0.934883, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 287/599 (start=18368, end=18432)\n",
      "2018-11-27T04:30:25.419463: step 2684, loss 0.745967, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 288/599 (start=18432, end=18496)\n",
      "2018-11-27T04:30:25.757024: step 2685, loss 1.06197, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 289/599 (start=18496, end=18560)\n",
      "2018-11-27T04:30:26.095299: step 2686, loss 0.789608, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 290/599 (start=18560, end=18624)\n",
      "2018-11-27T04:30:26.435324: step 2687, loss 1.13819, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 291/599 (start=18624, end=18688)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:30:26.763923: step 2688, loss 1.03617, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 292/599 (start=18688, end=18752)\n",
      "2018-11-27T04:30:27.087329: step 2689, loss 0.98556, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 293/599 (start=18752, end=18816)\n",
      "2018-11-27T04:30:27.424545: step 2690, loss 0.768021, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 294/599 (start=18816, end=18880)\n",
      "2018-11-27T04:30:27.743239: step 2691, loss 0.973141, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 295/599 (start=18880, end=18944)\n",
      "2018-11-27T04:30:28.061210: step 2692, loss 0.885713, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 296/599 (start=18944, end=19008)\n",
      "2018-11-27T04:30:28.395626: step 2693, loss 0.886276, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 297/599 (start=19008, end=19072)\n",
      "2018-11-27T04:30:28.752432: step 2694, loss 0.938766, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 298/599 (start=19072, end=19136)\n",
      "2018-11-27T04:30:29.084003: step 2695, loss 1.1011, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 299/599 (start=19136, end=19200)\n",
      "2018-11-27T04:30:29.436234: step 2696, loss 0.845264, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 300/599 (start=19200, end=19264)\n",
      "2018-11-27T04:30:29.781907: step 2697, loss 1.05613, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 301/599 (start=19264, end=19328)\n",
      "2018-11-27T04:30:30.088914: step 2698, loss 0.965779, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 302/599 (start=19328, end=19392)\n",
      "2018-11-27T04:30:30.406109: step 2699, loss 0.9505, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 303/599 (start=19392, end=19456)\n",
      "2018-11-27T04:30:30.721332: step 2700, loss 0.770268, acc 0.8125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:30:37.021337: step 2700, loss 1.79067, acc 0.514066\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-2700\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 304/599 (start=19456, end=19520)\n",
      "2018-11-27T04:30:37.767016: step 2701, loss 0.863892, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 305/599 (start=19520, end=19584)\n",
      "2018-11-27T04:30:38.109409: step 2702, loss 0.936244, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 306/599 (start=19584, end=19648)\n",
      "2018-11-27T04:30:38.437651: step 2703, loss 1.0051, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 307/599 (start=19648, end=19712)\n",
      "2018-11-27T04:30:38.763533: step 2704, loss 0.844619, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 308/599 (start=19712, end=19776)\n",
      "2018-11-27T04:30:39.111727: step 2705, loss 0.881683, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 309/599 (start=19776, end=19840)\n",
      "2018-11-27T04:30:39.452254: step 2706, loss 1.07224, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 310/599 (start=19840, end=19904)\n",
      "2018-11-27T04:30:39.794954: step 2707, loss 0.810463, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 311/599 (start=19904, end=19968)\n",
      "2018-11-27T04:30:40.130693: step 2708, loss 0.796227, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 312/599 (start=19968, end=20032)\n",
      "2018-11-27T04:30:40.468529: step 2709, loss 0.82286, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 313/599 (start=20032, end=20096)\n",
      "2018-11-27T04:30:40.791247: step 2710, loss 1.01932, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 314/599 (start=20096, end=20160)\n",
      "2018-11-27T04:30:41.127311: step 2711, loss 1.04138, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 315/599 (start=20160, end=20224)\n",
      "2018-11-27T04:30:41.463340: step 2712, loss 0.994766, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 316/599 (start=20224, end=20288)\n",
      "2018-11-27T04:30:41.787416: step 2713, loss 0.885555, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 317/599 (start=20288, end=20352)\n",
      "2018-11-27T04:30:42.109285: step 2714, loss 1.0363, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 318/599 (start=20352, end=20416)\n",
      "2018-11-27T04:30:42.446647: step 2715, loss 0.899888, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 319/599 (start=20416, end=20480)\n",
      "2018-11-27T04:30:42.790934: step 2716, loss 0.732052, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 320/599 (start=20480, end=20544)\n",
      "2018-11-27T04:30:43.142604: step 2717, loss 1.05883, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 321/599 (start=20544, end=20608)\n",
      "2018-11-27T04:30:43.472615: step 2718, loss 0.937997, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 322/599 (start=20608, end=20672)\n",
      "2018-11-27T04:30:43.823088: step 2719, loss 0.984223, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 323/599 (start=20672, end=20736)\n",
      "2018-11-27T04:30:44.156261: step 2720, loss 0.945276, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 324/599 (start=20736, end=20800)\n",
      "2018-11-27T04:30:44.492075: step 2721, loss 0.749014, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 325/599 (start=20800, end=20864)\n",
      "2018-11-27T04:30:44.815412: step 2722, loss 1.08768, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 326/599 (start=20864, end=20928)\n",
      "2018-11-27T04:30:45.151160: step 2723, loss 0.989177, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 327/599 (start=20928, end=20992)\n",
      "2018-11-27T04:30:45.476514: step 2724, loss 0.892607, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 328/599 (start=20992, end=21056)\n",
      "2018-11-27T04:30:45.808031: step 2725, loss 0.812758, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 329/599 (start=21056, end=21120)\n",
      "2018-11-27T04:30:46.152899: step 2726, loss 0.775555, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 330/599 (start=21120, end=21184)\n",
      "2018-11-27T04:30:46.480469: step 2727, loss 1.02135, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 331/599 (start=21184, end=21248)\n",
      "2018-11-27T04:30:46.824646: step 2728, loss 0.935847, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 332/599 (start=21248, end=21312)\n",
      "2018-11-27T04:30:47.156215: step 2729, loss 1.04119, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 333/599 (start=21312, end=21376)\n",
      "2018-11-27T04:30:47.476094: step 2730, loss 0.876498, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 334/599 (start=21376, end=21440)\n",
      "2018-11-27T04:30:47.799014: step 2731, loss 0.824267, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 335/599 (start=21440, end=21504)\n",
      "2018-11-27T04:30:48.134626: step 2732, loss 0.845617, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 336/599 (start=21504, end=21568)\n",
      "2018-11-27T04:30:48.445123: step 2733, loss 0.800371, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 337/599 (start=21568, end=21632)\n",
      "2018-11-27T04:30:48.761370: step 2734, loss 0.989178, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 338/599 (start=21632, end=21696)\n",
      "2018-11-27T04:30:49.100722: step 2735, loss 0.78642, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 339/599 (start=21696, end=21760)\n",
      "2018-11-27T04:30:49.448504: step 2736, loss 0.817141, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 340/599 (start=21760, end=21824)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:30:49.789060: step 2737, loss 0.715833, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 341/599 (start=21824, end=21888)\n",
      "2018-11-27T04:30:50.128938: step 2738, loss 0.874291, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 342/599 (start=21888, end=21952)\n",
      "2018-11-27T04:30:50.470672: step 2739, loss 0.835708, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 343/599 (start=21952, end=22016)\n",
      "2018-11-27T04:30:50.812231: step 2740, loss 0.989647, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 344/599 (start=22016, end=22080)\n",
      "2018-11-27T04:30:51.160041: step 2741, loss 0.892824, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 345/599 (start=22080, end=22144)\n",
      "2018-11-27T04:30:51.471232: step 2742, loss 1.01003, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 346/599 (start=22144, end=22208)\n",
      "2018-11-27T04:30:51.799921: step 2743, loss 0.898367, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 347/599 (start=22208, end=22272)\n",
      "2018-11-27T04:30:52.135296: step 2744, loss 0.836227, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 348/599 (start=22272, end=22336)\n",
      "2018-11-27T04:30:52.451835: step 2745, loss 0.956159, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 349/599 (start=22336, end=22400)\n",
      "2018-11-27T04:30:52.798478: step 2746, loss 0.837966, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 350/599 (start=22400, end=22464)\n",
      "2018-11-27T04:30:53.134849: step 2747, loss 0.926153, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 351/599 (start=22464, end=22528)\n",
      "2018-11-27T04:30:53.455978: step 2748, loss 0.848452, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 352/599 (start=22528, end=22592)\n",
      "2018-11-27T04:30:53.800316: step 2749, loss 0.793611, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 353/599 (start=22592, end=22656)\n",
      "2018-11-27T04:30:54.118431: step 2750, loss 0.907085, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 354/599 (start=22656, end=22720)\n",
      "2018-11-27T04:30:54.434514: step 2751, loss 0.730819, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 355/599 (start=22720, end=22784)\n",
      "2018-11-27T04:30:54.754220: step 2752, loss 0.904988, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 356/599 (start=22784, end=22848)\n",
      "2018-11-27T04:30:55.091911: step 2753, loss 0.585165, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 357/599 (start=22848, end=22912)\n",
      "2018-11-27T04:30:55.420581: step 2754, loss 0.766105, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 358/599 (start=22912, end=22976)\n",
      "2018-11-27T04:30:55.765482: step 2755, loss 0.918764, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 359/599 (start=22976, end=23040)\n",
      "2018-11-27T04:30:56.104241: step 2756, loss 0.932202, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 360/599 (start=23040, end=23104)\n",
      "2018-11-27T04:30:56.432808: step 2757, loss 0.761446, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 361/599 (start=23104, end=23168)\n",
      "2018-11-27T04:30:56.769705: step 2758, loss 0.906298, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 362/599 (start=23168, end=23232)\n",
      "2018-11-27T04:30:57.106235: step 2759, loss 0.892983, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 363/599 (start=23232, end=23296)\n",
      "2018-11-27T04:30:57.421992: step 2760, loss 0.92107, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 364/599 (start=23296, end=23360)\n",
      "2018-11-27T04:30:57.746764: step 2761, loss 1.22865, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 365/599 (start=23360, end=23424)\n",
      "2018-11-27T04:30:58.074529: step 2762, loss 0.639458, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 366/599 (start=23424, end=23488)\n",
      "2018-11-27T04:30:58.414243: step 2763, loss 0.848896, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 367/599 (start=23488, end=23552)\n",
      "2018-11-27T04:30:58.765235: step 2764, loss 0.913443, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 368/599 (start=23552, end=23616)\n",
      "2018-11-27T04:30:59.102161: step 2765, loss 1.10572, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 369/599 (start=23616, end=23680)\n",
      "2018-11-27T04:30:59.463594: step 2766, loss 0.847065, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 370/599 (start=23680, end=23744)\n",
      "2018-11-27T04:30:59.814493: step 2767, loss 0.768956, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 371/599 (start=23744, end=23808)\n",
      "2018-11-27T04:31:00.154441: step 2768, loss 0.939479, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 372/599 (start=23808, end=23872)\n",
      "2018-11-27T04:31:00.465557: step 2769, loss 0.947223, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 373/599 (start=23872, end=23936)\n",
      "2018-11-27T04:31:00.809580: step 2770, loss 0.827335, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 374/599 (start=23936, end=24000)\n",
      "2018-11-27T04:31:01.146239: step 2771, loss 1.11922, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 375/599 (start=24000, end=24064)\n",
      "2018-11-27T04:31:01.484746: step 2772, loss 0.804436, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 376/599 (start=24064, end=24128)\n",
      "2018-11-27T04:31:01.835952: step 2773, loss 1.00092, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 377/599 (start=24128, end=24192)\n",
      "2018-11-27T04:31:02.175228: step 2774, loss 0.893548, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 378/599 (start=24192, end=24256)\n",
      "2018-11-27T04:31:02.513170: step 2775, loss 0.835232, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 379/599 (start=24256, end=24320)\n",
      "2018-11-27T04:31:02.850792: step 2776, loss 0.738765, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 380/599 (start=24320, end=24384)\n",
      "2018-11-27T04:31:03.195224: step 2777, loss 0.702181, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 381/599 (start=24384, end=24448)\n",
      "2018-11-27T04:31:03.533705: step 2778, loss 0.85275, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 382/599 (start=24448, end=24512)\n",
      "2018-11-27T04:31:03.868707: step 2779, loss 0.931766, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 383/599 (start=24512, end=24576)\n",
      "2018-11-27T04:31:04.189904: step 2780, loss 0.94855, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 384/599 (start=24576, end=24640)\n",
      "2018-11-27T04:31:04.528421: step 2781, loss 0.952208, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 385/599 (start=24640, end=24704)\n",
      "2018-11-27T04:31:04.854898: step 2782, loss 0.950775, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 386/599 (start=24704, end=24768)\n",
      "2018-11-27T04:31:05.183748: step 2783, loss 0.972584, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 387/599 (start=24768, end=24832)\n",
      "2018-11-27T04:31:05.514805: step 2784, loss 0.796688, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 388/599 (start=24832, end=24896)\n",
      "2018-11-27T04:31:05.861948: step 2785, loss 0.822005, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 389/599 (start=24896, end=24960)\n",
      "2018-11-27T04:31:06.201828: step 2786, loss 0.906385, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 390/599 (start=24960, end=25024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:31:06.537879: step 2787, loss 0.980583, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 391/599 (start=25024, end=25088)\n",
      "2018-11-27T04:31:06.887352: step 2788, loss 0.972189, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 392/599 (start=25088, end=25152)\n",
      "2018-11-27T04:31:07.221022: step 2789, loss 0.932112, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 393/599 (start=25152, end=25216)\n",
      "2018-11-27T04:31:07.552780: step 2790, loss 0.901665, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 394/599 (start=25216, end=25280)\n",
      "2018-11-27T04:31:07.905772: step 2791, loss 1.00459, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 395/599 (start=25280, end=25344)\n",
      "2018-11-27T04:31:08.252588: step 2792, loss 1.09277, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 396/599 (start=25344, end=25408)\n",
      "2018-11-27T04:31:08.602369: step 2793, loss 1.03988, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 397/599 (start=25408, end=25472)\n",
      "2018-11-27T04:31:08.916130: step 2794, loss 0.909054, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 398/599 (start=25472, end=25536)\n",
      "2018-11-27T04:31:09.257350: step 2795, loss 0.662288, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 399/599 (start=25536, end=25600)\n",
      "2018-11-27T04:31:09.579625: step 2796, loss 0.919574, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 400/599 (start=25600, end=25664)\n",
      "2018-11-27T04:31:09.911125: step 2797, loss 0.886614, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 401/599 (start=25664, end=25728)\n",
      "2018-11-27T04:31:10.231366: step 2798, loss 0.912684, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 402/599 (start=25728, end=25792)\n",
      "2018-11-27T04:31:10.568803: step 2799, loss 1.11601, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 403/599 (start=25792, end=25856)\n",
      "2018-11-27T04:31:10.900939: step 2800, loss 1.04006, acc 0.75\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:31:17.246301: step 2800, loss 1.79064, acc 0.509443\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-2800\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 404/599 (start=25856, end=25920)\n",
      "2018-11-27T04:31:17.996791: step 2801, loss 0.945239, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 405/599 (start=25920, end=25984)\n",
      "2018-11-27T04:31:18.302847: step 2802, loss 1.03774, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 406/599 (start=25984, end=26048)\n",
      "2018-11-27T04:31:18.648036: step 2803, loss 0.777248, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 407/599 (start=26048, end=26112)\n",
      "2018-11-27T04:31:18.980485: step 2804, loss 0.732137, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 408/599 (start=26112, end=26176)\n",
      "2018-11-27T04:31:19.311940: step 2805, loss 0.93489, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 409/599 (start=26176, end=26240)\n",
      "2018-11-27T04:31:19.641485: step 2806, loss 0.99499, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 410/599 (start=26240, end=26304)\n",
      "2018-11-27T04:31:19.983559: step 2807, loss 0.838093, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 411/599 (start=26304, end=26368)\n",
      "2018-11-27T04:31:20.319990: step 2808, loss 1.04634, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 412/599 (start=26368, end=26432)\n",
      "2018-11-27T04:31:20.666867: step 2809, loss 0.686443, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 413/599 (start=26432, end=26496)\n",
      "2018-11-27T04:31:20.994685: step 2810, loss 0.830436, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 414/599 (start=26496, end=26560)\n",
      "2018-11-27T04:31:21.303105: step 2811, loss 0.875945, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 415/599 (start=26560, end=26624)\n",
      "2018-11-27T04:31:21.642813: step 2812, loss 0.92899, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 416/599 (start=26624, end=26688)\n",
      "2018-11-27T04:31:21.985165: step 2813, loss 0.589142, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 417/599 (start=26688, end=26752)\n",
      "2018-11-27T04:31:22.344593: step 2814, loss 0.773501, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 418/599 (start=26752, end=26816)\n",
      "2018-11-27T04:31:22.671364: step 2815, loss 0.768393, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 419/599 (start=26816, end=26880)\n",
      "2018-11-27T04:31:22.985630: step 2816, loss 1.10708, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 420/599 (start=26880, end=26944)\n",
      "2018-11-27T04:31:23.296759: step 2817, loss 0.663225, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 421/599 (start=26944, end=27008)\n",
      "2018-11-27T04:31:23.654029: step 2818, loss 1.12193, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 422/599 (start=27008, end=27072)\n",
      "2018-11-27T04:31:23.988416: step 2819, loss 0.876702, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 423/599 (start=27072, end=27136)\n",
      "2018-11-27T04:31:24.302000: step 2820, loss 0.827049, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 424/599 (start=27136, end=27200)\n",
      "2018-11-27T04:31:24.650845: step 2821, loss 0.902376, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 425/599 (start=27200, end=27264)\n",
      "2018-11-27T04:31:24.995260: step 2822, loss 0.906026, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 426/599 (start=27264, end=27328)\n",
      "2018-11-27T04:31:25.312755: step 2823, loss 0.83717, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 427/599 (start=27328, end=27392)\n",
      "2018-11-27T04:31:25.644141: step 2824, loss 0.743353, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 428/599 (start=27392, end=27456)\n",
      "2018-11-27T04:31:25.983161: step 2825, loss 0.85805, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 429/599 (start=27456, end=27520)\n",
      "2018-11-27T04:31:26.333167: step 2826, loss 0.978047, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 430/599 (start=27520, end=27584)\n",
      "2018-11-27T04:31:26.641383: step 2827, loss 0.820421, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 431/599 (start=27584, end=27648)\n",
      "2018-11-27T04:31:26.976399: step 2828, loss 0.861069, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 432/599 (start=27648, end=27712)\n",
      "2018-11-27T04:31:27.284444: step 2829, loss 0.994142, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 433/599 (start=27712, end=27776)\n",
      "2018-11-27T04:31:27.623979: step 2830, loss 0.984243, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 434/599 (start=27776, end=27840)\n",
      "2018-11-27T04:31:27.962383: step 2831, loss 0.871779, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 435/599 (start=27840, end=27904)\n",
      "2018-11-27T04:31:28.304269: step 2832, loss 1.23053, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 436/599 (start=27904, end=27968)\n",
      "2018-11-27T04:31:28.647830: step 2833, loss 1.1436, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 437/599 (start=27968, end=28032)\n",
      "2018-11-27T04:31:28.979781: step 2834, loss 0.940387, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 438/599 (start=28032, end=28096)\n",
      "2018-11-27T04:31:29.317829: step 2835, loss 0.994148, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 439/599 (start=28096, end=28160)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:31:29.668947: step 2836, loss 0.881983, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 440/599 (start=28160, end=28224)\n",
      "2018-11-27T04:31:30.004906: step 2837, loss 0.989503, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 441/599 (start=28224, end=28288)\n",
      "2018-11-27T04:31:30.337271: step 2838, loss 0.799915, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 442/599 (start=28288, end=28352)\n",
      "2018-11-27T04:31:30.662258: step 2839, loss 1.07493, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 443/599 (start=28352, end=28416)\n",
      "2018-11-27T04:31:30.977971: step 2840, loss 0.775227, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 444/599 (start=28416, end=28480)\n",
      "2018-11-27T04:31:31.320005: step 2841, loss 0.694482, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 445/599 (start=28480, end=28544)\n",
      "2018-11-27T04:31:31.656672: step 2842, loss 0.866791, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 446/599 (start=28544, end=28608)\n",
      "2018-11-27T04:31:31.981513: step 2843, loss 0.763031, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 447/599 (start=28608, end=28672)\n",
      "2018-11-27T04:31:32.328740: step 2844, loss 0.982647, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 448/599 (start=28672, end=28736)\n",
      "2018-11-27T04:31:32.641200: step 2845, loss 0.910188, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 449/599 (start=28736, end=28800)\n",
      "2018-11-27T04:31:32.950213: step 2846, loss 1.00862, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 450/599 (start=28800, end=28864)\n",
      "2018-11-27T04:31:33.289803: step 2847, loss 1.03959, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 451/599 (start=28864, end=28928)\n",
      "2018-11-27T04:31:33.619281: step 2848, loss 1.02424, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 452/599 (start=28928, end=28992)\n",
      "2018-11-27T04:31:33.961112: step 2849, loss 0.78415, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 453/599 (start=28992, end=29056)\n",
      "2018-11-27T04:31:34.283045: step 2850, loss 1.06911, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 454/599 (start=29056, end=29120)\n",
      "2018-11-27T04:31:34.630703: step 2851, loss 0.988344, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 455/599 (start=29120, end=29184)\n",
      "2018-11-27T04:31:34.972706: step 2852, loss 0.83932, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 456/599 (start=29184, end=29248)\n",
      "2018-11-27T04:31:35.282962: step 2853, loss 0.736379, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 457/599 (start=29248, end=29312)\n",
      "2018-11-27T04:31:35.626461: step 2854, loss 1.04655, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 458/599 (start=29312, end=29376)\n",
      "2018-11-27T04:31:35.951868: step 2855, loss 0.769169, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 459/599 (start=29376, end=29440)\n",
      "2018-11-27T04:31:36.291703: step 2856, loss 0.727836, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 460/599 (start=29440, end=29504)\n",
      "2018-11-27T04:31:36.631470: step 2857, loss 0.925485, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 461/599 (start=29504, end=29568)\n",
      "2018-11-27T04:31:36.948139: step 2858, loss 0.927353, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 462/599 (start=29568, end=29632)\n",
      "2018-11-27T04:31:37.285863: step 2859, loss 1.09218, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 463/599 (start=29632, end=29696)\n",
      "2018-11-27T04:31:37.619024: step 2860, loss 0.787746, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 464/599 (start=29696, end=29760)\n",
      "2018-11-27T04:31:37.942262: step 2861, loss 0.844077, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 465/599 (start=29760, end=29824)\n",
      "2018-11-27T04:31:38.286590: step 2862, loss 0.840676, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 466/599 (start=29824, end=29888)\n",
      "2018-11-27T04:31:38.606339: step 2863, loss 1.06258, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 467/599 (start=29888, end=29952)\n",
      "2018-11-27T04:31:38.947326: step 2864, loss 0.819317, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 468/599 (start=29952, end=30016)\n",
      "2018-11-27T04:31:39.282618: step 2865, loss 0.816936, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 469/599 (start=30016, end=30080)\n",
      "2018-11-27T04:31:39.623013: step 2866, loss 0.771994, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 470/599 (start=30080, end=30144)\n",
      "2018-11-27T04:31:39.935983: step 2867, loss 0.885525, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 471/599 (start=30144, end=30208)\n",
      "2018-11-27T04:31:40.273909: step 2868, loss 1.14696, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 472/599 (start=30208, end=30272)\n",
      "2018-11-27T04:31:40.624309: step 2869, loss 0.922876, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 473/599 (start=30272, end=30336)\n",
      "2018-11-27T04:31:40.955971: step 2870, loss 0.698238, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 474/599 (start=30336, end=30400)\n",
      "2018-11-27T04:31:41.264939: step 2871, loss 0.901177, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 475/599 (start=30400, end=30464)\n",
      "2018-11-27T04:31:41.605095: step 2872, loss 0.83127, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 476/599 (start=30464, end=30528)\n",
      "2018-11-27T04:31:41.972549: step 2873, loss 1.19038, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 477/599 (start=30528, end=30592)\n",
      "2018-11-27T04:31:42.288460: step 2874, loss 1.18346, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 478/599 (start=30592, end=30656)\n",
      "2018-11-27T04:31:42.595702: step 2875, loss 0.88538, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 479/599 (start=30656, end=30720)\n",
      "2018-11-27T04:31:42.921986: step 2876, loss 0.91686, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 480/599 (start=30720, end=30784)\n",
      "2018-11-27T04:31:43.268399: step 2877, loss 0.922404, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 481/599 (start=30784, end=30848)\n",
      "2018-11-27T04:31:43.577493: step 2878, loss 0.820184, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 482/599 (start=30848, end=30912)\n",
      "2018-11-27T04:31:43.927011: step 2879, loss 0.778623, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 483/599 (start=30912, end=30976)\n",
      "2018-11-27T04:31:44.279419: step 2880, loss 0.841721, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 484/599 (start=30976, end=31040)\n",
      "2018-11-27T04:31:44.618817: step 2881, loss 0.972795, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 485/599 (start=31040, end=31104)\n",
      "2018-11-27T04:31:44.967155: step 2882, loss 1.10886, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 486/599 (start=31104, end=31168)\n",
      "2018-11-27T04:31:45.316641: step 2883, loss 0.880029, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 487/599 (start=31168, end=31232)\n",
      "2018-11-27T04:31:45.668795: step 2884, loss 1.01035, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 488/599 (start=31232, end=31296)\n",
      "2018-11-27T04:31:45.985531: step 2885, loss 0.813558, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 489/599 (start=31296, end=31360)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:31:46.324339: step 2886, loss 0.851188, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 490/599 (start=31360, end=31424)\n",
      "2018-11-27T04:31:46.674278: step 2887, loss 1.11101, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 491/599 (start=31424, end=31488)\n",
      "2018-11-27T04:31:47.007219: step 2888, loss 0.769745, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 492/599 (start=31488, end=31552)\n",
      "2018-11-27T04:31:47.340662: step 2889, loss 1.03708, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 493/599 (start=31552, end=31616)\n",
      "2018-11-27T04:31:47.679964: step 2890, loss 0.970013, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 494/599 (start=31616, end=31680)\n",
      "2018-11-27T04:31:48.021435: step 2891, loss 1.07021, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 495/599 (start=31680, end=31744)\n",
      "2018-11-27T04:31:48.338424: step 2892, loss 0.893975, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 496/599 (start=31744, end=31808)\n",
      "2018-11-27T04:31:48.661952: step 2893, loss 0.725464, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 497/599 (start=31808, end=31872)\n",
      "2018-11-27T04:31:48.993124: step 2894, loss 0.972504, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 498/599 (start=31872, end=31936)\n",
      "2018-11-27T04:31:49.326079: step 2895, loss 0.840207, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 499/599 (start=31936, end=32000)\n",
      "2018-11-27T04:31:49.671115: step 2896, loss 0.854897, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 500/599 (start=32000, end=32064)\n",
      "2018-11-27T04:31:50.020295: step 2897, loss 0.835226, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 501/599 (start=32064, end=32128)\n",
      "2018-11-27T04:31:50.372605: step 2898, loss 0.973536, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 502/599 (start=32128, end=32192)\n",
      "2018-11-27T04:31:50.716793: step 2899, loss 0.857691, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 503/599 (start=32192, end=32256)\n",
      "2018-11-27T04:31:51.058408: step 2900, loss 0.798101, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:31:57.422726: step 2900, loss 1.79883, acc 0.511715\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-2900\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 504/599 (start=32256, end=32320)\n",
      "2018-11-27T04:31:58.176680: step 2901, loss 0.753016, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 505/599 (start=32320, end=32384)\n",
      "2018-11-27T04:31:58.522915: step 2902, loss 0.752446, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 506/599 (start=32384, end=32448)\n",
      "2018-11-27T04:31:58.854166: step 2903, loss 1.04579, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 507/599 (start=32448, end=32512)\n",
      "2018-11-27T04:31:59.185039: step 2904, loss 0.842422, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 508/599 (start=32512, end=32576)\n",
      "2018-11-27T04:31:59.508660: step 2905, loss 0.806558, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 509/599 (start=32576, end=32640)\n",
      "2018-11-27T04:31:59.844364: step 2906, loss 1.0542, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 510/599 (start=32640, end=32704)\n",
      "2018-11-27T04:32:00.200791: step 2907, loss 0.874043, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 511/599 (start=32704, end=32768)\n",
      "2018-11-27T04:32:00.514118: step 2908, loss 0.852635, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 512/599 (start=32768, end=32832)\n",
      "2018-11-27T04:32:00.852743: step 2909, loss 0.814796, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 513/599 (start=32832, end=32896)\n",
      "2018-11-27T04:32:01.197621: step 2910, loss 0.731065, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 514/599 (start=32896, end=32960)\n",
      "2018-11-27T04:32:01.527005: step 2911, loss 1.09749, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 515/599 (start=32960, end=33024)\n",
      "2018-11-27T04:32:01.876113: step 2912, loss 0.99311, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 516/599 (start=33024, end=33088)\n",
      "2018-11-27T04:32:02.214479: step 2913, loss 0.89655, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 517/599 (start=33088, end=33152)\n",
      "2018-11-27T04:32:02.543879: step 2914, loss 0.774115, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 518/599 (start=33152, end=33216)\n",
      "2018-11-27T04:32:02.877024: step 2915, loss 1.01919, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 519/599 (start=33216, end=33280)\n",
      "2018-11-27T04:32:03.219132: step 2916, loss 0.7922, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 520/599 (start=33280, end=33344)\n",
      "2018-11-27T04:32:03.542822: step 2917, loss 0.798214, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 521/599 (start=33344, end=33408)\n",
      "2018-11-27T04:32:03.854134: step 2918, loss 0.743111, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 522/599 (start=33408, end=33472)\n",
      "2018-11-27T04:32:04.201653: step 2919, loss 0.904389, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 523/599 (start=33472, end=33536)\n",
      "2018-11-27T04:32:04.547641: step 2920, loss 1.00191, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 524/599 (start=33536, end=33600)\n",
      "2018-11-27T04:32:04.887775: step 2921, loss 0.904146, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 525/599 (start=33600, end=33664)\n",
      "2018-11-27T04:32:05.227338: step 2922, loss 0.733718, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 526/599 (start=33664, end=33728)\n",
      "2018-11-27T04:32:05.589689: step 2923, loss 1.0981, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 527/599 (start=33728, end=33792)\n",
      "2018-11-27T04:32:05.932807: step 2924, loss 1.15737, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 528/599 (start=33792, end=33856)\n",
      "2018-11-27T04:32:06.260978: step 2925, loss 0.882691, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 529/599 (start=33856, end=33920)\n",
      "2018-11-27T04:32:06.594229: step 2926, loss 0.992765, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 530/599 (start=33920, end=33984)\n",
      "2018-11-27T04:32:06.905024: step 2927, loss 0.924707, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 531/599 (start=33984, end=34048)\n",
      "2018-11-27T04:32:07.230735: step 2928, loss 0.932921, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 532/599 (start=34048, end=34112)\n",
      "2018-11-27T04:32:07.557456: step 2929, loss 1.18675, acc 0.625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 533/599 (start=34112, end=34176)\n",
      "2018-11-27T04:32:07.874284: step 2930, loss 0.744426, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 534/599 (start=34176, end=34240)\n",
      "2018-11-27T04:32:08.199332: step 2931, loss 0.824564, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 535/599 (start=34240, end=34304)\n",
      "2018-11-27T04:32:08.532955: step 2932, loss 1.00289, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 536/599 (start=34304, end=34368)\n",
      "2018-11-27T04:32:08.867117: step 2933, loss 0.922322, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 537/599 (start=34368, end=34432)\n",
      "2018-11-27T04:32:09.212972: step 2934, loss 1.15158, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 538/599 (start=34432, end=34496)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:32:09.547499: step 2935, loss 1.03359, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 539/599 (start=34496, end=34560)\n",
      "2018-11-27T04:32:09.886506: step 2936, loss 1.22593, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 540/599 (start=34560, end=34624)\n",
      "2018-11-27T04:32:10.235115: step 2937, loss 0.816494, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 541/599 (start=34624, end=34688)\n",
      "2018-11-27T04:32:10.560356: step 2938, loss 0.869519, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 542/599 (start=34688, end=34752)\n",
      "2018-11-27T04:32:10.906401: step 2939, loss 1.08068, acc 0.671875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 543/599 (start=34752, end=34816)\n",
      "2018-11-27T04:32:11.243807: step 2940, loss 0.679788, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 544/599 (start=34816, end=34880)\n",
      "2018-11-27T04:32:11.590419: step 2941, loss 0.834429, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 545/599 (start=34880, end=34944)\n",
      "2018-11-27T04:32:11.933121: step 2942, loss 1.08928, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 546/599 (start=34944, end=35008)\n",
      "2018-11-27T04:32:12.275863: step 2943, loss 0.654071, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 547/599 (start=35008, end=35072)\n",
      "2018-11-27T04:32:12.607532: step 2944, loss 1.08737, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 548/599 (start=35072, end=35136)\n",
      "2018-11-27T04:32:12.928649: step 2945, loss 0.934349, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 549/599 (start=35136, end=35200)\n",
      "2018-11-27T04:32:13.270385: step 2946, loss 1.03126, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 550/599 (start=35200, end=35264)\n",
      "2018-11-27T04:32:13.585240: step 2947, loss 0.694561, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 551/599 (start=35264, end=35328)\n",
      "2018-11-27T04:32:13.919011: step 2948, loss 0.88859, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 552/599 (start=35328, end=35392)\n",
      "2018-11-27T04:32:14.242442: step 2949, loss 0.918745, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 553/599 (start=35392, end=35456)\n",
      "2018-11-27T04:32:14.596339: step 2950, loss 1.17883, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 554/599 (start=35456, end=35520)\n",
      "2018-11-27T04:32:14.929270: step 2951, loss 1.04965, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 555/599 (start=35520, end=35584)\n",
      "2018-11-27T04:32:15.276758: step 2952, loss 0.851645, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 556/599 (start=35584, end=35648)\n",
      "2018-11-27T04:32:15.597412: step 2953, loss 1.01207, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 557/599 (start=35648, end=35712)\n",
      "2018-11-27T04:32:15.940718: step 2954, loss 0.885709, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 558/599 (start=35712, end=35776)\n",
      "2018-11-27T04:32:16.258909: step 2955, loss 1.04675, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 559/599 (start=35776, end=35840)\n",
      "2018-11-27T04:32:16.586321: step 2956, loss 0.917099, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 560/599 (start=35840, end=35904)\n",
      "2018-11-27T04:32:16.907084: step 2957, loss 0.948248, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 561/599 (start=35904, end=35968)\n",
      "2018-11-27T04:32:17.248146: step 2958, loss 0.902044, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 562/599 (start=35968, end=36032)\n",
      "2018-11-27T04:32:17.595264: step 2959, loss 1.00191, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 563/599 (start=36032, end=36096)\n",
      "2018-11-27T04:32:17.939542: step 2960, loss 0.947746, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 564/599 (start=36096, end=36160)\n",
      "2018-11-27T04:32:18.263253: step 2961, loss 0.956559, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 565/599 (start=36160, end=36224)\n",
      "2018-11-27T04:32:18.596031: step 2962, loss 0.992904, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 566/599 (start=36224, end=36288)\n",
      "2018-11-27T04:32:18.941863: step 2963, loss 0.929602, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 567/599 (start=36288, end=36352)\n",
      "2018-11-27T04:32:19.273050: step 2964, loss 0.905334, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 568/599 (start=36352, end=36416)\n",
      "2018-11-27T04:32:19.595330: step 2965, loss 0.738701, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 569/599 (start=36416, end=36480)\n",
      "2018-11-27T04:32:19.910193: step 2966, loss 0.874689, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 570/599 (start=36480, end=36544)\n",
      "2018-11-27T04:32:20.227811: step 2967, loss 1.02345, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 571/599 (start=36544, end=36608)\n",
      "2018-11-27T04:32:20.566154: step 2968, loss 1.06449, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 572/599 (start=36608, end=36672)\n",
      "2018-11-27T04:32:20.899275: step 2969, loss 1.22659, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 573/599 (start=36672, end=36736)\n",
      "2018-11-27T04:32:21.248978: step 2970, loss 0.932638, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 574/599 (start=36736, end=36800)\n",
      "2018-11-27T04:32:21.563091: step 2971, loss 0.727388, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 575/599 (start=36800, end=36864)\n",
      "2018-11-27T04:32:21.876825: step 2972, loss 0.933441, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 576/599 (start=36864, end=36928)\n",
      "2018-11-27T04:32:22.184791: step 2973, loss 0.974427, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 577/599 (start=36928, end=36992)\n",
      "2018-11-27T04:32:22.527062: step 2974, loss 0.766128, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 578/599 (start=36992, end=37056)\n",
      "2018-11-27T04:32:22.872596: step 2975, loss 1.16099, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 579/599 (start=37056, end=37120)\n",
      "2018-11-27T04:32:23.207991: step 2976, loss 1.03673, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 580/599 (start=37120, end=37184)\n",
      "2018-11-27T04:32:23.540125: step 2977, loss 1.10628, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 581/599 (start=37184, end=37248)\n",
      "2018-11-27T04:32:23.872864: step 2978, loss 0.83777, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 582/599 (start=37248, end=37312)\n",
      "2018-11-27T04:32:24.190264: step 2979, loss 0.728234, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 583/599 (start=37312, end=37376)\n",
      "2018-11-27T04:32:24.512671: step 2980, loss 0.66662, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 584/599 (start=37376, end=37440)\n",
      "2018-11-27T04:32:24.849169: step 2981, loss 0.893169, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 585/599 (start=37440, end=37504)\n",
      "2018-11-27T04:32:25.166734: step 2982, loss 1.01284, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 586/599 (start=37504, end=37568)\n",
      "2018-11-27T04:32:25.472348: step 2983, loss 0.844336, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 587/599 (start=37568, end=37632)\n",
      "2018-11-27T04:32:25.809449: step 2984, loss 0.925785, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 588/599 (start=37632, end=37696)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:32:26.132902: step 2985, loss 0.955391, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 589/599 (start=37696, end=37760)\n",
      "2018-11-27T04:32:26.479967: step 2986, loss 0.966781, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 590/599 (start=37760, end=37824)\n",
      "2018-11-27T04:32:26.816928: step 2987, loss 1.05964, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 591/599 (start=37824, end=37888)\n",
      "2018-11-27T04:32:27.156238: step 2988, loss 1.10318, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 592/599 (start=37888, end=37952)\n",
      "2018-11-27T04:32:27.486974: step 2989, loss 1.11453, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 593/599 (start=37952, end=38016)\n",
      "2018-11-27T04:32:27.838031: step 2990, loss 0.825479, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 594/599 (start=38016, end=38080)\n",
      "2018-11-27T04:32:28.185917: step 2991, loss 0.665163, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 595/599 (start=38080, end=38144)\n",
      "2018-11-27T04:32:28.517654: step 2992, loss 0.90056, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 596/599 (start=38144, end=38208)\n",
      "2018-11-27T04:32:28.877061: step 2993, loss 1.00336, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 597/599 (start=38208, end=38272)\n",
      "2018-11-27T04:32:29.224022: step 2994, loss 0.840328, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 4/10, Batch 598/599 (start=38272, end=38281)\n",
      "2018-11-27T04:32:29.454439: step 2995, loss 1.31057, acc 0.666667\n",
      "***********************************************\n",
      "Epoch 5/10\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 0/599 (start=0, end=64)\n",
      "2018-11-27T04:32:29.797991: step 2996, loss 0.976347, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 1/599 (start=64, end=128)\n",
      "2018-11-27T04:32:30.122844: step 2997, loss 0.842777, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 2/599 (start=128, end=192)\n",
      "2018-11-27T04:32:30.435380: step 2998, loss 0.637163, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 3/599 (start=192, end=256)\n",
      "2018-11-27T04:32:30.784892: step 2999, loss 0.610214, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 4/599 (start=256, end=320)\n",
      "2018-11-27T04:32:31.097087: step 3000, loss 0.969987, acc 0.8125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:32:37.370584: step 3000, loss 1.80372, acc 0.503566\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-3000\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 5/599 (start=320, end=384)\n",
      "2018-11-27T04:32:38.110732: step 3001, loss 0.661526, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 6/599 (start=384, end=448)\n",
      "2018-11-27T04:32:38.437189: step 3002, loss 0.614238, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 7/599 (start=448, end=512)\n",
      "2018-11-27T04:32:38.750891: step 3003, loss 0.550546, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 8/599 (start=512, end=576)\n",
      "2018-11-27T04:32:39.084136: step 3004, loss 0.538179, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 9/599 (start=576, end=640)\n",
      "2018-11-27T04:32:39.408396: step 3005, loss 0.652728, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 10/599 (start=640, end=704)\n",
      "2018-11-27T04:32:39.744169: step 3006, loss 0.887299, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 11/599 (start=704, end=768)\n",
      "2018-11-27T04:32:40.050295: step 3007, loss 0.532142, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 12/599 (start=768, end=832)\n",
      "2018-11-27T04:32:40.391133: step 3008, loss 0.725327, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 13/599 (start=832, end=896)\n",
      "2018-11-27T04:32:40.738833: step 3009, loss 0.562705, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 14/599 (start=896, end=960)\n",
      "2018-11-27T04:32:41.049082: step 3010, loss 0.820033, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 15/599 (start=960, end=1024)\n",
      "2018-11-27T04:32:41.374803: step 3011, loss 0.729796, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 16/599 (start=1024, end=1088)\n",
      "2018-11-27T04:32:41.712828: step 3012, loss 1.01947, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 17/599 (start=1088, end=1152)\n",
      "2018-11-27T04:32:42.034045: step 3013, loss 0.708866, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 18/599 (start=1152, end=1216)\n",
      "2018-11-27T04:32:42.344342: step 3014, loss 0.500364, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 19/599 (start=1216, end=1280)\n",
      "2018-11-27T04:32:42.673119: step 3015, loss 0.605081, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 20/599 (start=1280, end=1344)\n",
      "2018-11-27T04:32:43.005545: step 3016, loss 0.655571, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 21/599 (start=1344, end=1408)\n",
      "2018-11-27T04:32:43.334240: step 3017, loss 0.760629, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 22/599 (start=1408, end=1472)\n",
      "2018-11-27T04:32:43.669470: step 3018, loss 0.606738, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 23/599 (start=1472, end=1536)\n",
      "2018-11-27T04:32:43.990508: step 3019, loss 0.765854, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 24/599 (start=1536, end=1600)\n",
      "2018-11-27T04:32:44.312429: step 3020, loss 1.09196, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 25/599 (start=1600, end=1664)\n",
      "2018-11-27T04:32:44.638491: step 3021, loss 0.601047, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 26/599 (start=1664, end=1728)\n",
      "2018-11-27T04:32:44.970350: step 3022, loss 0.672222, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 27/599 (start=1728, end=1792)\n",
      "2018-11-27T04:32:45.318727: step 3023, loss 0.711334, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 28/599 (start=1792, end=1856)\n",
      "2018-11-27T04:32:45.679661: step 3024, loss 0.533101, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 29/599 (start=1856, end=1920)\n",
      "2018-11-27T04:32:46.019767: step 3025, loss 0.607025, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 30/599 (start=1920, end=1984)\n",
      "2018-11-27T04:32:46.352002: step 3026, loss 0.843678, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 31/599 (start=1984, end=2048)\n",
      "2018-11-27T04:32:46.655903: step 3027, loss 0.810892, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 32/599 (start=2048, end=2112)\n",
      "2018-11-27T04:32:47.002704: step 3028, loss 0.636657, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 33/599 (start=2112, end=2176)\n",
      "2018-11-27T04:32:47.333792: step 3029, loss 0.558362, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 34/599 (start=2176, end=2240)\n",
      "2018-11-27T04:32:47.674816: step 3030, loss 0.859246, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 35/599 (start=2240, end=2304)\n",
      "2018-11-27T04:32:48.021256: step 3031, loss 0.499983, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 36/599 (start=2304, end=2368)\n",
      "2018-11-27T04:32:48.354451: step 3032, loss 0.632766, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 37/599 (start=2368, end=2432)\n",
      "2018-11-27T04:32:48.681095: step 3033, loss 0.657984, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 38/599 (start=2432, end=2496)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:32:49.005295: step 3034, loss 0.887867, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 39/599 (start=2496, end=2560)\n",
      "2018-11-27T04:32:49.329896: step 3035, loss 0.574085, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 40/599 (start=2560, end=2624)\n",
      "2018-11-27T04:32:49.638936: step 3036, loss 1.05457, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 41/599 (start=2624, end=2688)\n",
      "2018-11-27T04:32:49.983451: step 3037, loss 0.681382, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 42/599 (start=2688, end=2752)\n",
      "2018-11-27T04:32:50.303166: step 3038, loss 0.654154, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 43/599 (start=2752, end=2816)\n",
      "2018-11-27T04:32:50.641665: step 3039, loss 0.696077, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 44/599 (start=2816, end=2880)\n",
      "2018-11-27T04:32:50.973930: step 3040, loss 0.512712, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 45/599 (start=2880, end=2944)\n",
      "2018-11-27T04:32:51.329731: step 3041, loss 0.779677, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 46/599 (start=2944, end=3008)\n",
      "2018-11-27T04:32:51.676236: step 3042, loss 0.712505, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 47/599 (start=3008, end=3072)\n",
      "2018-11-27T04:32:51.992456: step 3043, loss 0.799996, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 48/599 (start=3072, end=3136)\n",
      "2018-11-27T04:32:52.325350: step 3044, loss 0.775355, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 49/599 (start=3136, end=3200)\n",
      "2018-11-27T04:32:52.664444: step 3045, loss 0.652797, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 50/599 (start=3200, end=3264)\n",
      "2018-11-27T04:32:53.008987: step 3046, loss 0.729382, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 51/599 (start=3264, end=3328)\n",
      "2018-11-27T04:32:53.340001: step 3047, loss 0.573789, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 52/599 (start=3328, end=3392)\n",
      "2018-11-27T04:32:53.680920: step 3048, loss 0.662833, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 53/599 (start=3392, end=3456)\n",
      "2018-11-27T04:32:54.029326: step 3049, loss 0.636432, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 54/599 (start=3456, end=3520)\n",
      "2018-11-27T04:32:54.337502: step 3050, loss 0.732072, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 55/599 (start=3520, end=3584)\n",
      "2018-11-27T04:32:54.673806: step 3051, loss 1.03769, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 56/599 (start=3584, end=3648)\n",
      "2018-11-27T04:32:55.000558: step 3052, loss 0.430415, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 57/599 (start=3648, end=3712)\n",
      "2018-11-27T04:32:55.321961: step 3053, loss 0.816727, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 58/599 (start=3712, end=3776)\n",
      "2018-11-27T04:32:55.662303: step 3054, loss 0.623191, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 59/599 (start=3776, end=3840)\n",
      "2018-11-27T04:32:55.982427: step 3055, loss 0.675782, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 60/599 (start=3840, end=3904)\n",
      "2018-11-27T04:32:56.321679: step 3056, loss 0.87709, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 61/599 (start=3904, end=3968)\n",
      "2018-11-27T04:32:56.640927: step 3057, loss 0.957985, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 62/599 (start=3968, end=4032)\n",
      "2018-11-27T04:32:56.971951: step 3058, loss 0.810471, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 63/599 (start=4032, end=4096)\n",
      "2018-11-27T04:32:57.296266: step 3059, loss 0.876524, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 64/599 (start=4096, end=4160)\n",
      "2018-11-27T04:32:57.638377: step 3060, loss 0.468571, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 65/599 (start=4160, end=4224)\n",
      "2018-11-27T04:32:57.955460: step 3061, loss 0.939547, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 66/599 (start=4224, end=4288)\n",
      "2018-11-27T04:32:58.280773: step 3062, loss 0.735012, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 67/599 (start=4288, end=4352)\n",
      "2018-11-27T04:32:58.601614: step 3063, loss 0.889189, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 68/599 (start=4352, end=4416)\n",
      "2018-11-27T04:32:58.949793: step 3064, loss 0.722663, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 69/599 (start=4416, end=4480)\n",
      "2018-11-27T04:32:59.299938: step 3065, loss 0.796856, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 70/599 (start=4480, end=4544)\n",
      "2018-11-27T04:32:59.631288: step 3066, loss 0.659371, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 71/599 (start=4544, end=4608)\n",
      "2018-11-27T04:32:59.971887: step 3067, loss 1.00155, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 72/599 (start=4608, end=4672)\n",
      "2018-11-27T04:33:00.308656: step 3068, loss 0.660235, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 73/599 (start=4672, end=4736)\n",
      "2018-11-27T04:33:00.638018: step 3069, loss 0.818477, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 74/599 (start=4736, end=4800)\n",
      "2018-11-27T04:33:00.994347: step 3070, loss 0.536969, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 75/599 (start=4800, end=4864)\n",
      "2018-11-27T04:33:01.343483: step 3071, loss 0.776469, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 76/599 (start=4864, end=4928)\n",
      "2018-11-27T04:33:01.680762: step 3072, loss 0.789525, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 77/599 (start=4928, end=4992)\n",
      "2018-11-27T04:33:02.021412: step 3073, loss 0.560724, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 78/599 (start=4992, end=5056)\n",
      "2018-11-27T04:33:02.326776: step 3074, loss 0.557041, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 79/599 (start=5056, end=5120)\n",
      "2018-11-27T04:33:02.670665: step 3075, loss 0.721227, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 80/599 (start=5120, end=5184)\n",
      "2018-11-27T04:33:02.996399: step 3076, loss 0.590549, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 81/599 (start=5184, end=5248)\n",
      "2018-11-27T04:33:03.321301: step 3077, loss 0.714664, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 82/599 (start=5248, end=5312)\n",
      "2018-11-27T04:33:03.644305: step 3078, loss 0.624116, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 83/599 (start=5312, end=5376)\n",
      "2018-11-27T04:33:03.985561: step 3079, loss 0.819906, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 84/599 (start=5376, end=5440)\n",
      "2018-11-27T04:33:04.294609: step 3080, loss 0.743602, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 85/599 (start=5440, end=5504)\n",
      "2018-11-27T04:33:04.623853: step 3081, loss 0.625673, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 86/599 (start=5504, end=5568)\n",
      "2018-11-27T04:33:04.944097: step 3082, loss 0.582567, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 87/599 (start=5568, end=5632)\n",
      "2018-11-27T04:33:05.290427: step 3083, loss 0.624253, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 88/599 (start=5632, end=5696)\n",
      "2018-11-27T04:33:05.604014: step 3084, loss 0.627735, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 89/599 (start=5696, end=5760)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:33:05.919018: step 3085, loss 0.685662, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 90/599 (start=5760, end=5824)\n",
      "2018-11-27T04:33:06.271816: step 3086, loss 0.365587, acc 0.984375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 91/599 (start=5824, end=5888)\n",
      "2018-11-27T04:33:06.609030: step 3087, loss 0.757694, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 92/599 (start=5888, end=5952)\n",
      "2018-11-27T04:33:06.935607: step 3088, loss 0.683927, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 93/599 (start=5952, end=6016)\n",
      "2018-11-27T04:33:07.283934: step 3089, loss 0.544295, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 94/599 (start=6016, end=6080)\n",
      "2018-11-27T04:33:07.608649: step 3090, loss 0.699906, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 95/599 (start=6080, end=6144)\n",
      "2018-11-27T04:33:07.946535: step 3091, loss 0.773714, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 96/599 (start=6144, end=6208)\n",
      "2018-11-27T04:33:08.309032: step 3092, loss 0.709033, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 97/599 (start=6208, end=6272)\n",
      "2018-11-27T04:33:08.657172: step 3093, loss 0.585049, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 98/599 (start=6272, end=6336)\n",
      "2018-11-27T04:33:08.987370: step 3094, loss 0.628371, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 99/599 (start=6336, end=6400)\n",
      "2018-11-27T04:33:09.317503: step 3095, loss 0.842922, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 100/599 (start=6400, end=6464)\n",
      "2018-11-27T04:33:09.649783: step 3096, loss 0.668857, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 101/599 (start=6464, end=6528)\n",
      "2018-11-27T04:33:09.960711: step 3097, loss 0.736537, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 102/599 (start=6528, end=6592)\n",
      "2018-11-27T04:33:10.299153: step 3098, loss 0.725688, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 103/599 (start=6592, end=6656)\n",
      "2018-11-27T04:33:10.620418: step 3099, loss 0.651732, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 104/599 (start=6656, end=6720)\n",
      "2018-11-27T04:33:10.962758: step 3100, loss 0.698624, acc 0.828125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:33:17.298947: step 3100, loss 1.83071, acc 0.517514\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-3100\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 105/599 (start=6720, end=6784)\n",
      "2018-11-27T04:33:18.084314: step 3101, loss 0.504078, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 106/599 (start=6784, end=6848)\n",
      "2018-11-27T04:33:18.417374: step 3102, loss 0.555885, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 107/599 (start=6848, end=6912)\n",
      "2018-11-27T04:33:18.751326: step 3103, loss 0.662788, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 108/599 (start=6912, end=6976)\n",
      "2018-11-27T04:33:19.100397: step 3104, loss 0.853038, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 109/599 (start=6976, end=7040)\n",
      "2018-11-27T04:33:19.444085: step 3105, loss 0.927197, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 110/599 (start=7040, end=7104)\n",
      "2018-11-27T04:33:19.762846: step 3106, loss 0.843378, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 111/599 (start=7104, end=7168)\n",
      "2018-11-27T04:33:20.090367: step 3107, loss 0.774565, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 112/599 (start=7168, end=7232)\n",
      "2018-11-27T04:33:20.430654: step 3108, loss 0.941627, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 113/599 (start=7232, end=7296)\n",
      "2018-11-27T04:33:20.764357: step 3109, loss 0.984348, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 114/599 (start=7296, end=7360)\n",
      "2018-11-27T04:33:21.079844: step 3110, loss 0.89075, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 115/599 (start=7360, end=7424)\n",
      "2018-11-27T04:33:21.413914: step 3111, loss 0.703432, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 116/599 (start=7424, end=7488)\n",
      "2018-11-27T04:33:21.773190: step 3112, loss 0.810287, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 117/599 (start=7488, end=7552)\n",
      "2018-11-27T04:33:22.113946: step 3113, loss 0.988584, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 118/599 (start=7552, end=7616)\n",
      "2018-11-27T04:33:22.450723: step 3114, loss 0.543935, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 119/599 (start=7616, end=7680)\n",
      "2018-11-27T04:33:22.789693: step 3115, loss 0.684951, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 120/599 (start=7680, end=7744)\n",
      "2018-11-27T04:33:23.101148: step 3116, loss 0.798691, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 121/599 (start=7744, end=7808)\n",
      "2018-11-27T04:33:23.435457: step 3117, loss 0.74041, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 122/599 (start=7808, end=7872)\n",
      "2018-11-27T04:33:23.787648: step 3118, loss 0.689806, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 123/599 (start=7872, end=7936)\n",
      "2018-11-27T04:33:24.135868: step 3119, loss 0.702353, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 124/599 (start=7936, end=8000)\n",
      "2018-11-27T04:33:24.452062: step 3120, loss 0.78075, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 125/599 (start=8000, end=8064)\n",
      "2018-11-27T04:33:24.812544: step 3121, loss 0.795279, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 126/599 (start=8064, end=8128)\n",
      "2018-11-27T04:33:25.137551: step 3122, loss 0.907947, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 127/599 (start=8128, end=8192)\n",
      "2018-11-27T04:33:25.469732: step 3123, loss 0.832554, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 128/599 (start=8192, end=8256)\n",
      "2018-11-27T04:33:25.784332: step 3124, loss 0.576696, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 129/599 (start=8256, end=8320)\n",
      "2018-11-27T04:33:26.127688: step 3125, loss 0.851014, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 130/599 (start=8320, end=8384)\n",
      "2018-11-27T04:33:26.465095: step 3126, loss 0.646172, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 131/599 (start=8384, end=8448)\n",
      "2018-11-27T04:33:26.818138: step 3127, loss 0.654523, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 132/599 (start=8448, end=8512)\n",
      "2018-11-27T04:33:27.157973: step 3128, loss 0.440215, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 133/599 (start=8512, end=8576)\n",
      "2018-11-27T04:33:27.508898: step 3129, loss 0.606505, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 134/599 (start=8576, end=8640)\n",
      "2018-11-27T04:33:27.858138: step 3130, loss 0.645361, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 135/599 (start=8640, end=8704)\n",
      "2018-11-27T04:33:28.174630: step 3131, loss 0.921104, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 136/599 (start=8704, end=8768)\n",
      "2018-11-27T04:33:28.522473: step 3132, loss 0.807905, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 137/599 (start=8768, end=8832)\n",
      "2018-11-27T04:33:28.840472: step 3133, loss 0.672938, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 138/599 (start=8832, end=8896)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:33:29.169962: step 3134, loss 0.794024, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 139/599 (start=8896, end=8960)\n",
      "2018-11-27T04:33:29.499192: step 3135, loss 0.856755, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 140/599 (start=8960, end=9024)\n",
      "2018-11-27T04:33:29.828231: step 3136, loss 0.695146, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 141/599 (start=9024, end=9088)\n",
      "2018-11-27T04:33:30.147426: step 3137, loss 0.756211, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 142/599 (start=9088, end=9152)\n",
      "2018-11-27T04:33:30.493315: step 3138, loss 0.852521, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 143/599 (start=9152, end=9216)\n",
      "2018-11-27T04:33:30.859878: step 3139, loss 0.856263, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 144/599 (start=9216, end=9280)\n",
      "2018-11-27T04:33:31.170702: step 3140, loss 0.633905, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 145/599 (start=9280, end=9344)\n",
      "2018-11-27T04:33:31.524226: step 3141, loss 0.771415, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 146/599 (start=9344, end=9408)\n",
      "2018-11-27T04:33:31.877136: step 3142, loss 0.836495, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 147/599 (start=9408, end=9472)\n",
      "2018-11-27T04:33:32.222247: step 3143, loss 0.899899, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 148/599 (start=9472, end=9536)\n",
      "2018-11-27T04:33:32.548590: step 3144, loss 0.896664, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 149/599 (start=9536, end=9600)\n",
      "2018-11-27T04:33:32.889815: step 3145, loss 0.760669, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 150/599 (start=9600, end=9664)\n",
      "2018-11-27T04:33:33.225580: step 3146, loss 0.777143, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 151/599 (start=9664, end=9728)\n",
      "2018-11-27T04:33:33.552006: step 3147, loss 0.633724, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 152/599 (start=9728, end=9792)\n",
      "2018-11-27T04:33:33.893673: step 3148, loss 0.837941, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 153/599 (start=9792, end=9856)\n",
      "2018-11-27T04:33:34.205656: step 3149, loss 1.13629, acc 0.65625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 154/599 (start=9856, end=9920)\n",
      "2018-11-27T04:33:34.551393: step 3150, loss 0.764432, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 155/599 (start=9920, end=9984)\n",
      "2018-11-27T04:33:34.891652: step 3151, loss 0.741973, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 156/599 (start=9984, end=10048)\n",
      "2018-11-27T04:33:35.242894: step 3152, loss 0.675465, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 157/599 (start=10048, end=10112)\n",
      "2018-11-27T04:33:35.579283: step 3153, loss 0.651786, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 158/599 (start=10112, end=10176)\n",
      "2018-11-27T04:33:35.924435: step 3154, loss 0.711417, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 159/599 (start=10176, end=10240)\n",
      "2018-11-27T04:33:36.236160: step 3155, loss 0.884206, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 160/599 (start=10240, end=10304)\n",
      "2018-11-27T04:33:36.559507: step 3156, loss 0.530606, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 161/599 (start=10304, end=10368)\n",
      "2018-11-27T04:33:36.909166: step 3157, loss 0.572773, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 162/599 (start=10368, end=10432)\n",
      "2018-11-27T04:33:37.266051: step 3158, loss 0.735795, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 163/599 (start=10432, end=10496)\n",
      "2018-11-27T04:33:37.599158: step 3159, loss 0.861337, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 164/599 (start=10496, end=10560)\n",
      "2018-11-27T04:33:37.907443: step 3160, loss 0.696747, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 165/599 (start=10560, end=10624)\n",
      "2018-11-27T04:33:38.242818: step 3161, loss 0.724077, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 166/599 (start=10624, end=10688)\n",
      "2018-11-27T04:33:38.589241: step 3162, loss 0.818222, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 167/599 (start=10688, end=10752)\n",
      "2018-11-27T04:33:38.930435: step 3163, loss 0.496825, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 168/599 (start=10752, end=10816)\n",
      "2018-11-27T04:33:39.276471: step 3164, loss 0.919844, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 169/599 (start=10816, end=10880)\n",
      "2018-11-27T04:33:39.616389: step 3165, loss 0.673441, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 170/599 (start=10880, end=10944)\n",
      "2018-11-27T04:33:39.931251: step 3166, loss 0.871415, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 171/599 (start=10944, end=11008)\n",
      "2018-11-27T04:33:40.274395: step 3167, loss 0.685636, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 172/599 (start=11008, end=11072)\n",
      "2018-11-27T04:33:40.609998: step 3168, loss 0.701105, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 173/599 (start=11072, end=11136)\n",
      "2018-11-27T04:33:40.933532: step 3169, loss 0.776241, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 174/599 (start=11136, end=11200)\n",
      "2018-11-27T04:33:41.273128: step 3170, loss 0.809229, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 175/599 (start=11200, end=11264)\n",
      "2018-11-27T04:33:41.614141: step 3171, loss 0.612965, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 176/599 (start=11264, end=11328)\n",
      "2018-11-27T04:33:41.965079: step 3172, loss 0.723315, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 177/599 (start=11328, end=11392)\n",
      "2018-11-27T04:33:42.289306: step 3173, loss 0.815157, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 178/599 (start=11392, end=11456)\n",
      "2018-11-27T04:33:42.629936: step 3174, loss 0.783013, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 179/599 (start=11456, end=11520)\n",
      "2018-11-27T04:33:42.971094: step 3175, loss 0.72729, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 180/599 (start=11520, end=11584)\n",
      "2018-11-27T04:33:43.297895: step 3176, loss 0.74984, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 181/599 (start=11584, end=11648)\n",
      "2018-11-27T04:33:43.635957: step 3177, loss 0.841083, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 182/599 (start=11648, end=11712)\n",
      "2018-11-27T04:33:43.970623: step 3178, loss 0.946694, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 183/599 (start=11712, end=11776)\n",
      "2018-11-27T04:33:44.296958: step 3179, loss 0.791784, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 184/599 (start=11776, end=11840)\n",
      "2018-11-27T04:33:44.639558: step 3180, loss 1.03361, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 185/599 (start=11840, end=11904)\n",
      "2018-11-27T04:33:44.978412: step 3181, loss 0.738591, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 186/599 (start=11904, end=11968)\n",
      "2018-11-27T04:33:45.322040: step 3182, loss 0.588797, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 187/599 (start=11968, end=12032)\n",
      "2018-11-27T04:33:45.639246: step 3183, loss 0.733861, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 188/599 (start=12032, end=12096)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:33:45.993728: step 3184, loss 0.773228, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 189/599 (start=12096, end=12160)\n",
      "2018-11-27T04:33:46.337667: step 3185, loss 0.637699, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 190/599 (start=12160, end=12224)\n",
      "2018-11-27T04:33:46.686983: step 3186, loss 0.744999, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 191/599 (start=12224, end=12288)\n",
      "2018-11-27T04:33:47.010568: step 3187, loss 0.785035, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 192/599 (start=12288, end=12352)\n",
      "2018-11-27T04:33:47.363193: step 3188, loss 0.722775, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 193/599 (start=12352, end=12416)\n",
      "2018-11-27T04:33:47.715440: step 3189, loss 0.773971, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 194/599 (start=12416, end=12480)\n",
      "2018-11-27T04:33:48.033690: step 3190, loss 0.987877, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 195/599 (start=12480, end=12544)\n",
      "2018-11-27T04:33:48.375569: step 3191, loss 0.965289, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 196/599 (start=12544, end=12608)\n",
      "2018-11-27T04:33:48.682243: step 3192, loss 0.892974, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 197/599 (start=12608, end=12672)\n",
      "2018-11-27T04:33:49.034510: step 3193, loss 0.597663, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 198/599 (start=12672, end=12736)\n",
      "2018-11-27T04:33:49.384079: step 3194, loss 0.880621, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 199/599 (start=12736, end=12800)\n",
      "2018-11-27T04:33:49.699442: step 3195, loss 0.712742, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 200/599 (start=12800, end=12864)\n",
      "2018-11-27T04:33:50.041376: step 3196, loss 0.558435, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 201/599 (start=12864, end=12928)\n",
      "2018-11-27T04:33:50.390413: step 3197, loss 0.768682, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 202/599 (start=12928, end=12992)\n",
      "2018-11-27T04:33:50.737413: step 3198, loss 0.574204, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 203/599 (start=12992, end=13056)\n",
      "2018-11-27T04:33:51.073481: step 3199, loss 0.608557, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 204/599 (start=13056, end=13120)\n",
      "2018-11-27T04:33:51.412256: step 3200, loss 0.849638, acc 0.78125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:33:57.719887: step 3200, loss 1.83406, acc 0.511402\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-3200\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 205/599 (start=13120, end=13184)\n",
      "2018-11-27T04:33:58.471860: step 3201, loss 0.772836, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 206/599 (start=13184, end=13248)\n",
      "2018-11-27T04:33:58.797899: step 3202, loss 0.822074, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 207/599 (start=13248, end=13312)\n",
      "2018-11-27T04:33:59.139245: step 3203, loss 0.608978, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 208/599 (start=13312, end=13376)\n",
      "2018-11-27T04:33:59.452588: step 3204, loss 0.916591, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 209/599 (start=13376, end=13440)\n",
      "2018-11-27T04:33:59.791442: step 3205, loss 0.806535, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 210/599 (start=13440, end=13504)\n",
      "2018-11-27T04:34:00.129739: step 3206, loss 0.679373, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 211/599 (start=13504, end=13568)\n",
      "2018-11-27T04:34:00.439664: step 3207, loss 0.613718, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 212/599 (start=13568, end=13632)\n",
      "2018-11-27T04:34:00.750868: step 3208, loss 0.80416, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 213/599 (start=13632, end=13696)\n",
      "2018-11-27T04:34:01.073417: step 3209, loss 1.00126, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 214/599 (start=13696, end=13760)\n",
      "2018-11-27T04:34:01.413917: step 3210, loss 0.748191, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 215/599 (start=13760, end=13824)\n",
      "2018-11-27T04:34:01.721689: step 3211, loss 0.76219, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 216/599 (start=13824, end=13888)\n",
      "2018-11-27T04:34:02.032604: step 3212, loss 0.672637, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 217/599 (start=13888, end=13952)\n",
      "2018-11-27T04:34:02.351905: step 3213, loss 1.00811, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 218/599 (start=13952, end=14016)\n",
      "2018-11-27T04:34:02.695433: step 3214, loss 0.689713, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 219/599 (start=14016, end=14080)\n",
      "2018-11-27T04:34:03.005678: step 3215, loss 0.763645, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 220/599 (start=14080, end=14144)\n",
      "2018-11-27T04:34:03.340306: step 3216, loss 1.0633, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 221/599 (start=14144, end=14208)\n",
      "2018-11-27T04:34:03.670000: step 3217, loss 0.748471, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 222/599 (start=14208, end=14272)\n",
      "2018-11-27T04:34:04.011593: step 3218, loss 0.957669, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 223/599 (start=14272, end=14336)\n",
      "2018-11-27T04:34:04.349931: step 3219, loss 0.626677, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 224/599 (start=14336, end=14400)\n",
      "2018-11-27T04:34:04.694994: step 3220, loss 0.634841, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 225/599 (start=14400, end=14464)\n",
      "2018-11-27T04:34:05.047147: step 3221, loss 0.834139, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 226/599 (start=14464, end=14528)\n",
      "2018-11-27T04:34:05.366375: step 3222, loss 0.940366, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 227/599 (start=14528, end=14592)\n",
      "2018-11-27T04:34:05.685593: step 3223, loss 0.617199, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 228/599 (start=14592, end=14656)\n",
      "2018-11-27T04:34:06.025876: step 3224, loss 0.727802, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 229/599 (start=14656, end=14720)\n",
      "2018-11-27T04:34:06.351982: step 3225, loss 0.709549, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 230/599 (start=14720, end=14784)\n",
      "2018-11-27T04:34:06.688373: step 3226, loss 0.602688, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 231/599 (start=14784, end=14848)\n",
      "2018-11-27T04:34:07.037015: step 3227, loss 0.696929, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 232/599 (start=14848, end=14912)\n",
      "2018-11-27T04:34:07.371537: step 3228, loss 0.583951, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 233/599 (start=14912, end=14976)\n",
      "2018-11-27T04:34:07.682425: step 3229, loss 0.863271, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 234/599 (start=14976, end=15040)\n",
      "2018-11-27T04:34:07.999125: step 3230, loss 0.827924, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 235/599 (start=15040, end=15104)\n",
      "2018-11-27T04:34:08.323867: step 3231, loss 0.831733, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 236/599 (start=15104, end=15168)\n",
      "2018-11-27T04:34:08.666168: step 3232, loss 0.602468, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 237/599 (start=15168, end=15232)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:34:08.981071: step 3233, loss 0.905771, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 238/599 (start=15232, end=15296)\n",
      "2018-11-27T04:34:09.295553: step 3234, loss 0.613548, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 239/599 (start=15296, end=15360)\n",
      "2018-11-27T04:34:09.647171: step 3235, loss 0.60967, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 240/599 (start=15360, end=15424)\n",
      "2018-11-27T04:34:09.999156: step 3236, loss 0.703024, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 241/599 (start=15424, end=15488)\n",
      "2018-11-27T04:34:10.313464: step 3237, loss 0.815209, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 242/599 (start=15488, end=15552)\n",
      "2018-11-27T04:34:10.633953: step 3238, loss 0.776829, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 243/599 (start=15552, end=15616)\n",
      "2018-11-27T04:34:10.981423: step 3239, loss 0.708045, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 244/599 (start=15616, end=15680)\n",
      "2018-11-27T04:34:11.295514: step 3240, loss 0.616962, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 245/599 (start=15680, end=15744)\n",
      "2018-11-27T04:34:11.633124: step 3241, loss 0.620711, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 246/599 (start=15744, end=15808)\n",
      "2018-11-27T04:34:11.986964: step 3242, loss 0.854363, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 247/599 (start=15808, end=15872)\n",
      "2018-11-27T04:34:12.331406: step 3243, loss 0.973454, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 248/599 (start=15872, end=15936)\n",
      "2018-11-27T04:34:12.662785: step 3244, loss 0.82133, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 249/599 (start=15936, end=16000)\n",
      "2018-11-27T04:34:13.014902: step 3245, loss 0.817532, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 250/599 (start=16000, end=16064)\n",
      "2018-11-27T04:34:13.361008: step 3246, loss 0.887904, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 251/599 (start=16064, end=16128)\n",
      "2018-11-27T04:34:13.692047: step 3247, loss 0.882599, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 252/599 (start=16128, end=16192)\n",
      "2018-11-27T04:34:14.021937: step 3248, loss 0.704816, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 253/599 (start=16192, end=16256)\n",
      "2018-11-27T04:34:14.352514: step 3249, loss 0.829291, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 254/599 (start=16256, end=16320)\n",
      "2018-11-27T04:34:14.691022: step 3250, loss 0.714718, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 255/599 (start=16320, end=16384)\n",
      "2018-11-27T04:34:15.025959: step 3251, loss 0.665591, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 256/599 (start=16384, end=16448)\n",
      "2018-11-27T04:34:15.358522: step 3252, loss 0.827644, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 257/599 (start=16448, end=16512)\n",
      "2018-11-27T04:34:15.687356: step 3253, loss 0.643674, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 258/599 (start=16512, end=16576)\n",
      "2018-11-27T04:34:16.025975: step 3254, loss 0.693981, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 259/599 (start=16576, end=16640)\n",
      "2018-11-27T04:34:16.355032: step 3255, loss 0.888744, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 260/599 (start=16640, end=16704)\n",
      "2018-11-27T04:34:16.699582: step 3256, loss 0.828975, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 261/599 (start=16704, end=16768)\n",
      "2018-11-27T04:34:17.016375: step 3257, loss 0.921203, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 262/599 (start=16768, end=16832)\n",
      "2018-11-27T04:34:17.334827: step 3258, loss 0.712348, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 263/599 (start=16832, end=16896)\n",
      "2018-11-27T04:34:17.642005: step 3259, loss 0.728583, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 264/599 (start=16896, end=16960)\n",
      "2018-11-27T04:34:17.960017: step 3260, loss 0.720132, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 265/599 (start=16960, end=17024)\n",
      "2018-11-27T04:34:18.293215: step 3261, loss 0.862828, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 266/599 (start=17024, end=17088)\n",
      "2018-11-27T04:34:18.627466: step 3262, loss 1.04248, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 267/599 (start=17088, end=17152)\n",
      "2018-11-27T04:34:18.970532: step 3263, loss 0.954739, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 268/599 (start=17152, end=17216)\n",
      "2018-11-27T04:34:19.283768: step 3264, loss 0.653601, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 269/599 (start=17216, end=17280)\n",
      "2018-11-27T04:34:19.631306: step 3265, loss 0.624474, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 270/599 (start=17280, end=17344)\n",
      "2018-11-27T04:34:19.968040: step 3266, loss 0.730814, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 271/599 (start=17344, end=17408)\n",
      "2018-11-27T04:34:20.316295: step 3267, loss 0.670915, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 272/599 (start=17408, end=17472)\n",
      "2018-11-27T04:34:20.661990: step 3268, loss 0.722068, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 273/599 (start=17472, end=17536)\n",
      "2018-11-27T04:34:20.994909: step 3269, loss 0.651795, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 274/599 (start=17536, end=17600)\n",
      "2018-11-27T04:34:21.306745: step 3270, loss 0.550785, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 275/599 (start=17600, end=17664)\n",
      "2018-11-27T04:34:21.649473: step 3271, loss 0.804138, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 276/599 (start=17664, end=17728)\n",
      "2018-11-27T04:34:21.984827: step 3272, loss 0.828691, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 277/599 (start=17728, end=17792)\n",
      "2018-11-27T04:34:22.311958: step 3273, loss 0.787735, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 278/599 (start=17792, end=17856)\n",
      "2018-11-27T04:34:22.654061: step 3274, loss 0.717916, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 279/599 (start=17856, end=17920)\n",
      "2018-11-27T04:34:22.963776: step 3275, loss 0.774791, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 280/599 (start=17920, end=17984)\n",
      "2018-11-27T04:34:23.308323: step 3276, loss 0.871689, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 281/599 (start=17984, end=18048)\n",
      "2018-11-27T04:34:23.653869: step 3277, loss 0.931821, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 282/599 (start=18048, end=18112)\n",
      "2018-11-27T04:34:23.982810: step 3278, loss 0.568014, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 283/599 (start=18112, end=18176)\n",
      "2018-11-27T04:34:24.308073: step 3279, loss 0.844636, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 284/599 (start=18176, end=18240)\n",
      "2018-11-27T04:34:24.638774: step 3280, loss 1.08406, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 285/599 (start=18240, end=18304)\n",
      "2018-11-27T04:34:24.976787: step 3281, loss 0.644999, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 286/599 (start=18304, end=18368)\n",
      "2018-11-27T04:34:25.309281: step 3282, loss 0.689912, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 287/599 (start=18368, end=18432)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:34:25.643468: step 3283, loss 0.636926, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 288/599 (start=18432, end=18496)\n",
      "2018-11-27T04:34:25.957264: step 3284, loss 0.729467, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 289/599 (start=18496, end=18560)\n",
      "2018-11-27T04:34:26.296328: step 3285, loss 0.875202, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 290/599 (start=18560, end=18624)\n",
      "2018-11-27T04:34:26.629062: step 3286, loss 0.657692, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 291/599 (start=18624, end=18688)\n",
      "2018-11-27T04:34:26.969460: step 3287, loss 0.815595, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 292/599 (start=18688, end=18752)\n",
      "2018-11-27T04:34:27.314438: step 3288, loss 0.732924, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 293/599 (start=18752, end=18816)\n",
      "2018-11-27T04:34:27.630099: step 3289, loss 0.685142, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 294/599 (start=18816, end=18880)\n",
      "2018-11-27T04:34:27.978243: step 3290, loss 0.673886, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 295/599 (start=18880, end=18944)\n",
      "2018-11-27T04:34:28.319798: step 3291, loss 0.77745, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 296/599 (start=18944, end=19008)\n",
      "2018-11-27T04:34:28.665601: step 3292, loss 0.890559, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 297/599 (start=19008, end=19072)\n",
      "2018-11-27T04:34:28.994611: step 3293, loss 0.65505, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 298/599 (start=19072, end=19136)\n",
      "2018-11-27T04:34:29.306500: step 3294, loss 0.700867, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 299/599 (start=19136, end=19200)\n",
      "2018-11-27T04:34:29.646570: step 3295, loss 0.830103, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 300/599 (start=19200, end=19264)\n",
      "2018-11-27T04:34:29.990619: step 3296, loss 1.00753, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 301/599 (start=19264, end=19328)\n",
      "2018-11-27T04:34:30.339634: step 3297, loss 0.705501, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 302/599 (start=19328, end=19392)\n",
      "2018-11-27T04:34:30.658536: step 3298, loss 0.902851, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 303/599 (start=19392, end=19456)\n",
      "2018-11-27T04:34:30.981319: step 3299, loss 0.84393, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 304/599 (start=19456, end=19520)\n",
      "2018-11-27T04:34:31.318701: step 3300, loss 0.710928, acc 0.828125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:34:37.419351: step 3300, loss 1.85341, acc 0.51916\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-3300\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 305/599 (start=19520, end=19584)\n",
      "2018-11-27T04:34:38.159415: step 3301, loss 0.843951, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 306/599 (start=19584, end=19648)\n",
      "2018-11-27T04:34:38.472185: step 3302, loss 0.6909, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 307/599 (start=19648, end=19712)\n",
      "2018-11-27T04:34:38.805543: step 3303, loss 0.878991, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 308/599 (start=19712, end=19776)\n",
      "2018-11-27T04:34:39.116529: step 3304, loss 0.739399, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 309/599 (start=19776, end=19840)\n",
      "2018-11-27T04:34:39.438252: step 3305, loss 0.703089, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 310/599 (start=19840, end=19904)\n",
      "2018-11-27T04:34:39.755810: step 3306, loss 0.771287, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 311/599 (start=19904, end=19968)\n",
      "2018-11-27T04:34:40.100465: step 3307, loss 0.937578, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 312/599 (start=19968, end=20032)\n",
      "2018-11-27T04:34:40.434963: step 3308, loss 1.05993, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 313/599 (start=20032, end=20096)\n",
      "2018-11-27T04:34:40.774409: step 3309, loss 0.960993, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 314/599 (start=20096, end=20160)\n",
      "2018-11-27T04:34:41.125158: step 3310, loss 0.732216, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 315/599 (start=20160, end=20224)\n",
      "2018-11-27T04:34:41.465548: step 3311, loss 0.852803, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 316/599 (start=20224, end=20288)\n",
      "2018-11-27T04:34:41.800494: step 3312, loss 0.875034, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 317/599 (start=20288, end=20352)\n",
      "2018-11-27T04:34:42.152794: step 3313, loss 0.907698, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 318/599 (start=20352, end=20416)\n",
      "2018-11-27T04:34:42.493784: step 3314, loss 0.679805, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 319/599 (start=20416, end=20480)\n",
      "2018-11-27T04:34:42.820970: step 3315, loss 1.0187, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 320/599 (start=20480, end=20544)\n",
      "2018-11-27T04:34:43.158376: step 3316, loss 0.779756, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 321/599 (start=20544, end=20608)\n",
      "2018-11-27T04:34:43.507243: step 3317, loss 0.800777, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 322/599 (start=20608, end=20672)\n",
      "2018-11-27T04:34:43.839367: step 3318, loss 0.656182, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 323/599 (start=20672, end=20736)\n",
      "2018-11-27T04:34:44.173696: step 3319, loss 0.945605, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 324/599 (start=20736, end=20800)\n",
      "2018-11-27T04:34:44.529585: step 3320, loss 0.697012, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 325/599 (start=20800, end=20864)\n",
      "2018-11-27T04:34:44.883961: step 3321, loss 0.56798, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 326/599 (start=20864, end=20928)\n",
      "2018-11-27T04:34:45.194696: step 3322, loss 0.881128, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 327/599 (start=20928, end=20992)\n",
      "2018-11-27T04:34:45.511059: step 3323, loss 0.999801, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 328/599 (start=20992, end=21056)\n",
      "2018-11-27T04:34:45.824949: step 3324, loss 0.85767, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 329/599 (start=21056, end=21120)\n",
      "2018-11-27T04:34:46.161693: step 3325, loss 0.766587, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 330/599 (start=21120, end=21184)\n",
      "2018-11-27T04:34:46.484684: step 3326, loss 0.83668, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 331/599 (start=21184, end=21248)\n",
      "2018-11-27T04:34:46.803125: step 3327, loss 0.814564, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 332/599 (start=21248, end=21312)\n",
      "2018-11-27T04:34:47.156339: step 3328, loss 0.577725, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 333/599 (start=21312, end=21376)\n",
      "2018-11-27T04:34:47.498171: step 3329, loss 0.599841, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 334/599 (start=21376, end=21440)\n",
      "2018-11-27T04:34:47.832973: step 3330, loss 0.946287, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 335/599 (start=21440, end=21504)\n",
      "2018-11-27T04:34:48.163123: step 3331, loss 0.696715, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 336/599 (start=21504, end=21568)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:34:48.509767: step 3332, loss 0.711293, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 337/599 (start=21568, end=21632)\n",
      "2018-11-27T04:34:48.853319: step 3333, loss 1.07201, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 338/599 (start=21632, end=21696)\n",
      "2018-11-27T04:34:49.171337: step 3334, loss 0.873729, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 339/599 (start=21696, end=21760)\n",
      "2018-11-27T04:34:49.526599: step 3335, loss 0.59752, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 340/599 (start=21760, end=21824)\n",
      "2018-11-27T04:34:49.863735: step 3336, loss 0.952389, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 341/599 (start=21824, end=21888)\n",
      "2018-11-27T04:34:50.176742: step 3337, loss 0.662011, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 342/599 (start=21888, end=21952)\n",
      "2018-11-27T04:34:50.514742: step 3338, loss 0.511302, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 343/599 (start=21952, end=22016)\n",
      "2018-11-27T04:34:50.861066: step 3339, loss 0.78452, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 344/599 (start=22016, end=22080)\n",
      "2018-11-27T04:34:51.193135: step 3340, loss 0.842304, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 345/599 (start=22080, end=22144)\n",
      "2018-11-27T04:34:51.505255: step 3341, loss 0.663285, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 346/599 (start=22144, end=22208)\n",
      "2018-11-27T04:34:51.845125: step 3342, loss 0.856708, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 347/599 (start=22208, end=22272)\n",
      "2018-11-27T04:34:52.157075: step 3343, loss 0.895934, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 348/599 (start=22272, end=22336)\n",
      "2018-11-27T04:34:52.467701: step 3344, loss 0.724876, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 349/599 (start=22336, end=22400)\n",
      "2018-11-27T04:34:52.811922: step 3345, loss 0.639284, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 350/599 (start=22400, end=22464)\n",
      "2018-11-27T04:34:53.155851: step 3346, loss 0.807434, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 351/599 (start=22464, end=22528)\n",
      "2018-11-27T04:34:53.491084: step 3347, loss 0.903408, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 352/599 (start=22528, end=22592)\n",
      "2018-11-27T04:34:53.807706: step 3348, loss 0.706193, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 353/599 (start=22592, end=22656)\n",
      "2018-11-27T04:34:54.161093: step 3349, loss 0.876927, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 354/599 (start=22656, end=22720)\n",
      "2018-11-27T04:34:54.487040: step 3350, loss 0.865383, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 355/599 (start=22720, end=22784)\n",
      "2018-11-27T04:34:54.801227: step 3351, loss 0.701329, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 356/599 (start=22784, end=22848)\n",
      "2018-11-27T04:34:55.151728: step 3352, loss 0.585605, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 357/599 (start=22848, end=22912)\n",
      "2018-11-27T04:34:55.494860: step 3353, loss 0.617992, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 358/599 (start=22912, end=22976)\n",
      "2018-11-27T04:34:55.834585: step 3354, loss 0.699891, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 359/599 (start=22976, end=23040)\n",
      "2018-11-27T04:34:56.140210: step 3355, loss 0.839936, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 360/599 (start=23040, end=23104)\n",
      "2018-11-27T04:34:56.472279: step 3356, loss 0.777076, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 361/599 (start=23104, end=23168)\n",
      "2018-11-27T04:34:56.794724: step 3357, loss 0.808591, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 362/599 (start=23168, end=23232)\n",
      "2018-11-27T04:34:57.130487: step 3358, loss 0.849754, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 363/599 (start=23232, end=23296)\n",
      "2018-11-27T04:34:57.461330: step 3359, loss 0.789908, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 364/599 (start=23296, end=23360)\n",
      "2018-11-27T04:34:57.770454: step 3360, loss 0.857539, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 365/599 (start=23360, end=23424)\n",
      "2018-11-27T04:34:58.097009: step 3361, loss 0.776846, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 366/599 (start=23424, end=23488)\n",
      "2018-11-27T04:34:58.424059: step 3362, loss 0.732761, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 367/599 (start=23488, end=23552)\n",
      "2018-11-27T04:34:58.750697: step 3363, loss 0.780487, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 368/599 (start=23552, end=23616)\n",
      "2018-11-27T04:34:59.104334: step 3364, loss 0.734394, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 369/599 (start=23616, end=23680)\n",
      "2018-11-27T04:34:59.444082: step 3365, loss 0.867359, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 370/599 (start=23680, end=23744)\n",
      "2018-11-27T04:34:59.753148: step 3366, loss 0.601685, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 371/599 (start=23744, end=23808)\n",
      "2018-11-27T04:35:00.108605: step 3367, loss 0.767701, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 372/599 (start=23808, end=23872)\n",
      "2018-11-27T04:35:00.443201: step 3368, loss 0.757958, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 373/599 (start=23872, end=23936)\n",
      "2018-11-27T04:35:00.784418: step 3369, loss 0.679279, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 374/599 (start=23936, end=24000)\n",
      "2018-11-27T04:35:01.133671: step 3370, loss 0.805529, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 375/599 (start=24000, end=24064)\n",
      "2018-11-27T04:35:01.452242: step 3371, loss 0.647752, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 376/599 (start=24064, end=24128)\n",
      "2018-11-27T04:35:01.766287: step 3372, loss 0.691137, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 377/599 (start=24128, end=24192)\n",
      "2018-11-27T04:35:02.104206: step 3373, loss 0.792667, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 378/599 (start=24192, end=24256)\n",
      "2018-11-27T04:35:02.440972: step 3374, loss 0.846064, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 379/599 (start=24256, end=24320)\n",
      "2018-11-27T04:35:02.780766: step 3375, loss 0.705236, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 380/599 (start=24320, end=24384)\n",
      "2018-11-27T04:35:03.116041: step 3376, loss 0.887321, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 381/599 (start=24384, end=24448)\n",
      "2018-11-27T04:35:03.428856: step 3377, loss 0.625096, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 382/599 (start=24448, end=24512)\n",
      "2018-11-27T04:35:03.775923: step 3378, loss 0.957891, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 383/599 (start=24512, end=24576)\n",
      "2018-11-27T04:35:04.115373: step 3379, loss 1.03595, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 384/599 (start=24576, end=24640)\n",
      "2018-11-27T04:35:04.451428: step 3380, loss 0.745944, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 385/599 (start=24640, end=24704)\n",
      "2018-11-27T04:35:04.789817: step 3381, loss 0.750403, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 386/599 (start=24704, end=24768)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:35:05.129670: step 3382, loss 0.706144, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 387/599 (start=24768, end=24832)\n",
      "2018-11-27T04:35:05.467873: step 3383, loss 0.895464, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 388/599 (start=24832, end=24896)\n",
      "2018-11-27T04:35:05.791168: step 3384, loss 0.756979, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 389/599 (start=24896, end=24960)\n",
      "2018-11-27T04:35:06.131958: step 3385, loss 0.666919, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 390/599 (start=24960, end=25024)\n",
      "2018-11-27T04:35:06.474387: step 3386, loss 0.92745, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 391/599 (start=25024, end=25088)\n",
      "2018-11-27T04:35:06.797619: step 3387, loss 0.741207, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 392/599 (start=25088, end=25152)\n",
      "2018-11-27T04:35:07.140641: step 3388, loss 0.687193, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 393/599 (start=25152, end=25216)\n",
      "2018-11-27T04:35:07.479366: step 3389, loss 0.746175, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 394/599 (start=25216, end=25280)\n",
      "2018-11-27T04:35:07.778691: step 3390, loss 0.681267, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 395/599 (start=25280, end=25344)\n",
      "2018-11-27T04:35:08.142824: step 3391, loss 0.778476, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 396/599 (start=25344, end=25408)\n",
      "2018-11-27T04:35:08.485277: step 3392, loss 0.739634, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 397/599 (start=25408, end=25472)\n",
      "2018-11-27T04:35:08.809419: step 3393, loss 0.71495, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 398/599 (start=25472, end=25536)\n",
      "2018-11-27T04:35:09.149214: step 3394, loss 0.743912, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 399/599 (start=25536, end=25600)\n",
      "2018-11-27T04:35:09.493427: step 3395, loss 0.859556, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 400/599 (start=25600, end=25664)\n",
      "2018-11-27T04:35:09.828792: step 3396, loss 0.746206, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 401/599 (start=25664, end=25728)\n",
      "2018-11-27T04:35:10.173446: step 3397, loss 0.800359, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 402/599 (start=25728, end=25792)\n",
      "2018-11-27T04:35:10.510894: step 3398, loss 0.657796, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 403/599 (start=25792, end=25856)\n",
      "2018-11-27T04:35:10.856128: step 3399, loss 0.777716, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 404/599 (start=25856, end=25920)\n",
      "2018-11-27T04:35:11.205081: step 3400, loss 0.802002, acc 0.8125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:35:17.656905: step 3400, loss 1.84409, acc 0.511324\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-3400\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 405/599 (start=25920, end=25984)\n",
      "2018-11-27T04:35:18.430111: step 3401, loss 1.03879, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 406/599 (start=25984, end=26048)\n",
      "2018-11-27T04:35:18.762072: step 3402, loss 0.858198, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 407/599 (start=26048, end=26112)\n",
      "2018-11-27T04:35:19.113700: step 3403, loss 0.608446, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 408/599 (start=26112, end=26176)\n",
      "2018-11-27T04:35:19.450511: step 3404, loss 0.740031, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 409/599 (start=26176, end=26240)\n",
      "2018-11-27T04:35:19.786770: step 3405, loss 0.714771, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 410/599 (start=26240, end=26304)\n",
      "2018-11-27T04:35:20.132996: step 3406, loss 0.588223, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 411/599 (start=26304, end=26368)\n",
      "2018-11-27T04:35:20.471794: step 3407, loss 0.794514, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 412/599 (start=26368, end=26432)\n",
      "2018-11-27T04:35:20.807697: step 3408, loss 0.859892, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 413/599 (start=26432, end=26496)\n",
      "2018-11-27T04:35:21.139546: step 3409, loss 0.736595, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 414/599 (start=26496, end=26560)\n",
      "2018-11-27T04:35:21.475481: step 3410, loss 0.586314, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 415/599 (start=26560, end=26624)\n",
      "2018-11-27T04:35:21.811756: step 3411, loss 0.75122, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 416/599 (start=26624, end=26688)\n",
      "2018-11-27T04:35:22.141394: step 3412, loss 0.734991, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 417/599 (start=26688, end=26752)\n",
      "2018-11-27T04:35:22.481723: step 3413, loss 0.780872, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 418/599 (start=26752, end=26816)\n",
      "2018-11-27T04:35:22.825628: step 3414, loss 0.921223, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 419/599 (start=26816, end=26880)\n",
      "2018-11-27T04:35:23.170128: step 3415, loss 0.671795, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 420/599 (start=26880, end=26944)\n",
      "2018-11-27T04:35:23.497712: step 3416, loss 1.01247, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 421/599 (start=26944, end=27008)\n",
      "2018-11-27T04:35:23.832337: step 3417, loss 0.600344, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 422/599 (start=27008, end=27072)\n",
      "2018-11-27T04:35:24.176538: step 3418, loss 0.878842, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 423/599 (start=27072, end=27136)\n",
      "2018-11-27T04:35:24.514676: step 3419, loss 0.909521, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 424/599 (start=27136, end=27200)\n",
      "2018-11-27T04:35:24.839654: step 3420, loss 0.577132, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 425/599 (start=27200, end=27264)\n",
      "2018-11-27T04:35:25.158336: step 3421, loss 0.932325, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 426/599 (start=27264, end=27328)\n",
      "2018-11-27T04:35:25.479401: step 3422, loss 0.605509, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 427/599 (start=27328, end=27392)\n",
      "2018-11-27T04:35:25.816789: step 3423, loss 0.845221, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 428/599 (start=27392, end=27456)\n",
      "2018-11-27T04:35:26.125207: step 3424, loss 0.823768, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 429/599 (start=27456, end=27520)\n",
      "2018-11-27T04:35:26.452435: step 3425, loss 0.705426, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 430/599 (start=27520, end=27584)\n",
      "2018-11-27T04:35:26.791142: step 3426, loss 0.751136, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 431/599 (start=27584, end=27648)\n",
      "2018-11-27T04:35:27.132279: step 3427, loss 0.777145, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 432/599 (start=27648, end=27712)\n",
      "2018-11-27T04:35:27.456994: step 3428, loss 0.729502, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 433/599 (start=27712, end=27776)\n",
      "2018-11-27T04:35:27.798606: step 3429, loss 0.776232, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 434/599 (start=27776, end=27840)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:35:28.141540: step 3430, loss 0.829208, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 435/599 (start=27840, end=27904)\n",
      "2018-11-27T04:35:28.489894: step 3431, loss 0.768331, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 436/599 (start=27904, end=27968)\n",
      "2018-11-27T04:35:28.828827: step 3432, loss 0.793297, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 437/599 (start=27968, end=28032)\n",
      "2018-11-27T04:35:29.162477: step 3433, loss 1.01296, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 438/599 (start=28032, end=28096)\n",
      "2018-11-27T04:35:29.474370: step 3434, loss 0.743653, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 439/599 (start=28096, end=28160)\n",
      "2018-11-27T04:35:29.794001: step 3435, loss 0.815833, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 440/599 (start=28160, end=28224)\n",
      "2018-11-27T04:35:30.134006: step 3436, loss 0.794398, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 441/599 (start=28224, end=28288)\n",
      "2018-11-27T04:35:30.456467: step 3437, loss 1.09791, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 442/599 (start=28288, end=28352)\n",
      "2018-11-27T04:35:30.787214: step 3438, loss 1.04018, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 443/599 (start=28352, end=28416)\n",
      "2018-11-27T04:35:31.115560: step 3439, loss 0.99434, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 444/599 (start=28416, end=28480)\n",
      "2018-11-27T04:35:31.439290: step 3440, loss 0.79363, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 445/599 (start=28480, end=28544)\n",
      "2018-11-27T04:35:31.784257: step 3441, loss 0.937922, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 446/599 (start=28544, end=28608)\n",
      "2018-11-27T04:35:32.112816: step 3442, loss 0.697977, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 447/599 (start=28608, end=28672)\n",
      "2018-11-27T04:35:32.433044: step 3443, loss 0.987929, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 448/599 (start=28672, end=28736)\n",
      "2018-11-27T04:35:32.745931: step 3444, loss 1.03806, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 449/599 (start=28736, end=28800)\n",
      "2018-11-27T04:35:33.080445: step 3445, loss 0.851687, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 450/599 (start=28800, end=28864)\n",
      "2018-11-27T04:35:33.421424: step 3446, loss 0.90287, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 451/599 (start=28864, end=28928)\n",
      "2018-11-27T04:35:33.747175: step 3447, loss 0.613348, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 452/599 (start=28928, end=28992)\n",
      "2018-11-27T04:35:34.096863: step 3448, loss 0.775953, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 453/599 (start=28992, end=29056)\n",
      "2018-11-27T04:35:34.434015: step 3449, loss 0.534281, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 454/599 (start=29056, end=29120)\n",
      "2018-11-27T04:35:34.773732: step 3450, loss 0.912809, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 455/599 (start=29120, end=29184)\n",
      "2018-11-27T04:35:35.088156: step 3451, loss 0.913613, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 456/599 (start=29184, end=29248)\n",
      "2018-11-27T04:35:35.431160: step 3452, loss 0.602873, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 457/599 (start=29248, end=29312)\n",
      "2018-11-27T04:35:35.741572: step 3453, loss 0.879042, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 458/599 (start=29312, end=29376)\n",
      "2018-11-27T04:35:36.088007: step 3454, loss 0.922954, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 459/599 (start=29376, end=29440)\n",
      "2018-11-27T04:35:36.426027: step 3455, loss 0.795702, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 460/599 (start=29440, end=29504)\n",
      "2018-11-27T04:35:36.734752: step 3456, loss 0.763075, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 461/599 (start=29504, end=29568)\n",
      "2018-11-27T04:35:37.100773: step 3457, loss 0.803725, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 462/599 (start=29568, end=29632)\n",
      "2018-11-27T04:35:37.442219: step 3458, loss 0.624007, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 463/599 (start=29632, end=29696)\n",
      "2018-11-27T04:35:37.766489: step 3459, loss 0.743436, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 464/599 (start=29696, end=29760)\n",
      "2018-11-27T04:35:38.082274: step 3460, loss 0.778911, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 465/599 (start=29760, end=29824)\n",
      "2018-11-27T04:35:38.403721: step 3461, loss 0.785267, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 466/599 (start=29824, end=29888)\n",
      "2018-11-27T04:35:38.719681: step 3462, loss 0.883777, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 467/599 (start=29888, end=29952)\n",
      "2018-11-27T04:35:39.050030: step 3463, loss 0.743631, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 468/599 (start=29952, end=30016)\n",
      "2018-11-27T04:35:39.391806: step 3464, loss 0.771114, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 469/599 (start=30016, end=30080)\n",
      "2018-11-27T04:35:39.725417: step 3465, loss 0.714136, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 470/599 (start=30080, end=30144)\n",
      "2018-11-27T04:35:40.047543: step 3466, loss 0.788068, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 471/599 (start=30144, end=30208)\n",
      "2018-11-27T04:35:40.364635: step 3467, loss 0.805918, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 472/599 (start=30208, end=30272)\n",
      "2018-11-27T04:35:40.701117: step 3468, loss 0.930603, acc 0.6875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 473/599 (start=30272, end=30336)\n",
      "2018-11-27T04:35:41.046658: step 3469, loss 0.97909, acc 0.703125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 474/599 (start=30336, end=30400)\n",
      "2018-11-27T04:35:41.394507: step 3470, loss 0.981316, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 475/599 (start=30400, end=30464)\n",
      "2018-11-27T04:35:41.729931: step 3471, loss 0.739711, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 476/599 (start=30464, end=30528)\n",
      "2018-11-27T04:35:42.062652: step 3472, loss 0.74652, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 477/599 (start=30528, end=30592)\n",
      "2018-11-27T04:35:42.409125: step 3473, loss 0.74356, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 478/599 (start=30592, end=30656)\n",
      "2018-11-27T04:35:42.744936: step 3474, loss 0.699279, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 479/599 (start=30656, end=30720)\n",
      "2018-11-27T04:35:43.061652: step 3475, loss 0.585469, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 480/599 (start=30720, end=30784)\n",
      "2018-11-27T04:35:43.400047: step 3476, loss 0.749305, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 481/599 (start=30784, end=30848)\n",
      "2018-11-27T04:35:43.732828: step 3477, loss 0.783059, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 482/599 (start=30848, end=30912)\n",
      "2018-11-27T04:35:44.071045: step 3478, loss 0.993766, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 483/599 (start=30912, end=30976)\n",
      "2018-11-27T04:35:44.415163: step 3479, loss 0.800274, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 484/599 (start=30976, end=31040)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:35:44.728098: step 3480, loss 0.707395, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 485/599 (start=31040, end=31104)\n",
      "2018-11-27T04:35:45.080302: step 3481, loss 0.876148, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 486/599 (start=31104, end=31168)\n",
      "2018-11-27T04:35:45.417044: step 3482, loss 0.960977, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 487/599 (start=31168, end=31232)\n",
      "2018-11-27T04:35:45.727050: step 3483, loss 0.790958, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 488/599 (start=31232, end=31296)\n",
      "2018-11-27T04:35:46.048868: step 3484, loss 0.812687, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 489/599 (start=31296, end=31360)\n",
      "2018-11-27T04:35:46.383682: step 3485, loss 0.651502, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 490/599 (start=31360, end=31424)\n",
      "2018-11-27T04:35:46.726686: step 3486, loss 0.686945, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 491/599 (start=31424, end=31488)\n",
      "2018-11-27T04:35:47.045158: step 3487, loss 0.880793, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 492/599 (start=31488, end=31552)\n",
      "2018-11-27T04:35:47.411561: step 3488, loss 0.785085, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 493/599 (start=31552, end=31616)\n",
      "2018-11-27T04:35:47.755120: step 3489, loss 0.871189, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 494/599 (start=31616, end=31680)\n",
      "2018-11-27T04:35:48.109847: step 3490, loss 0.655466, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 495/599 (start=31680, end=31744)\n",
      "2018-11-27T04:35:48.456680: step 3491, loss 0.666304, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 496/599 (start=31744, end=31808)\n",
      "2018-11-27T04:35:48.791335: step 3492, loss 0.745698, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 497/599 (start=31808, end=31872)\n",
      "2018-11-27T04:35:49.128440: step 3493, loss 0.747588, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 498/599 (start=31872, end=31936)\n",
      "2018-11-27T04:35:49.485929: step 3494, loss 0.53961, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 499/599 (start=31936, end=32000)\n",
      "2018-11-27T04:35:49.831549: step 3495, loss 0.873561, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 500/599 (start=32000, end=32064)\n",
      "2018-11-27T04:35:50.163512: step 3496, loss 0.821945, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 501/599 (start=32064, end=32128)\n",
      "2018-11-27T04:35:50.487576: step 3497, loss 0.918377, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 502/599 (start=32128, end=32192)\n",
      "2018-11-27T04:35:50.834229: step 3498, loss 0.614355, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 503/599 (start=32192, end=32256)\n",
      "2018-11-27T04:35:51.143950: step 3499, loss 0.70383, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 504/599 (start=32256, end=32320)\n",
      "2018-11-27T04:35:51.477815: step 3500, loss 0.702352, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:35:57.580542: step 3500, loss 1.83642, acc 0.511715\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-3500\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 505/599 (start=32320, end=32384)\n",
      "2018-11-27T04:35:58.297097: step 3501, loss 0.804032, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 506/599 (start=32384, end=32448)\n",
      "2018-11-27T04:35:58.656688: step 3502, loss 0.888411, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 507/599 (start=32448, end=32512)\n",
      "2018-11-27T04:35:58.993508: step 3503, loss 0.951654, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 508/599 (start=32512, end=32576)\n",
      "2018-11-27T04:35:59.311272: step 3504, loss 0.724156, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 509/599 (start=32576, end=32640)\n",
      "2018-11-27T04:35:59.647391: step 3505, loss 0.687108, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 510/599 (start=32640, end=32704)\n",
      "2018-11-27T04:35:59.964702: step 3506, loss 0.52834, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 511/599 (start=32704, end=32768)\n",
      "2018-11-27T04:36:00.312953: step 3507, loss 0.827966, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 512/599 (start=32768, end=32832)\n",
      "2018-11-27T04:36:00.624882: step 3508, loss 0.584663, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 513/599 (start=32832, end=32896)\n",
      "2018-11-27T04:36:00.966224: step 3509, loss 0.516295, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 514/599 (start=32896, end=32960)\n",
      "2018-11-27T04:36:01.296950: step 3510, loss 0.557664, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 515/599 (start=32960, end=33024)\n",
      "2018-11-27T04:36:01.635794: step 3511, loss 1.03382, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 516/599 (start=33024, end=33088)\n",
      "2018-11-27T04:36:01.965347: step 3512, loss 0.650191, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 517/599 (start=33088, end=33152)\n",
      "2018-11-27T04:36:02.315747: step 3513, loss 0.887903, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 518/599 (start=33152, end=33216)\n",
      "2018-11-27T04:36:02.668685: step 3514, loss 0.64088, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 519/599 (start=33216, end=33280)\n",
      "2018-11-27T04:36:03.002119: step 3515, loss 1.15641, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 520/599 (start=33280, end=33344)\n",
      "2018-11-27T04:36:03.337009: step 3516, loss 0.661633, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 521/599 (start=33344, end=33408)\n",
      "2018-11-27T04:36:03.691489: step 3517, loss 0.694941, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 522/599 (start=33408, end=33472)\n",
      "2018-11-27T04:36:04.037364: step 3518, loss 0.792958, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 523/599 (start=33472, end=33536)\n",
      "2018-11-27T04:36:04.346232: step 3519, loss 0.702676, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 524/599 (start=33536, end=33600)\n",
      "2018-11-27T04:36:04.666600: step 3520, loss 0.996838, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 525/599 (start=33600, end=33664)\n",
      "2018-11-27T04:36:04.988406: step 3521, loss 0.703405, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 526/599 (start=33664, end=33728)\n",
      "2018-11-27T04:36:05.309015: step 3522, loss 0.582467, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 527/599 (start=33728, end=33792)\n",
      "2018-11-27T04:36:05.611089: step 3523, loss 0.510778, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 528/599 (start=33792, end=33856)\n",
      "2018-11-27T04:36:05.922902: step 3524, loss 0.445957, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 529/599 (start=33856, end=33920)\n",
      "2018-11-27T04:36:06.253326: step 3525, loss 0.633264, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 530/599 (start=33920, end=33984)\n",
      "2018-11-27T04:36:06.569548: step 3526, loss 0.823441, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 531/599 (start=33984, end=34048)\n",
      "2018-11-27T04:36:06.878433: step 3527, loss 0.665715, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 532/599 (start=34048, end=34112)\n",
      "2018-11-27T04:36:07.225226: step 3528, loss 1.06672, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 533/599 (start=34112, end=34176)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:36:07.573596: step 3529, loss 0.843502, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 534/599 (start=34176, end=34240)\n",
      "2018-11-27T04:36:07.893557: step 3530, loss 0.712623, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 535/599 (start=34240, end=34304)\n",
      "2018-11-27T04:36:08.223763: step 3531, loss 0.887347, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 536/599 (start=34304, end=34368)\n",
      "2018-11-27T04:36:08.557794: step 3532, loss 0.561317, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 537/599 (start=34368, end=34432)\n",
      "2018-11-27T04:36:08.906375: step 3533, loss 0.548697, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 538/599 (start=34432, end=34496)\n",
      "2018-11-27T04:36:09.240696: step 3534, loss 0.697861, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 539/599 (start=34496, end=34560)\n",
      "2018-11-27T04:36:09.576864: step 3535, loss 0.670254, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 540/599 (start=34560, end=34624)\n",
      "2018-11-27T04:36:09.926773: step 3536, loss 1.05492, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 541/599 (start=34624, end=34688)\n",
      "2018-11-27T04:36:10.268661: step 3537, loss 0.744103, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 542/599 (start=34688, end=34752)\n",
      "2018-11-27T04:36:10.607184: step 3538, loss 1.09553, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 543/599 (start=34752, end=34816)\n",
      "2018-11-27T04:36:10.930081: step 3539, loss 0.819136, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 544/599 (start=34816, end=34880)\n",
      "2018-11-27T04:36:11.285116: step 3540, loss 0.892967, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 545/599 (start=34880, end=34944)\n",
      "2018-11-27T04:36:11.634880: step 3541, loss 0.859454, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 546/599 (start=34944, end=35008)\n",
      "2018-11-27T04:36:11.946925: step 3542, loss 0.630703, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 547/599 (start=35008, end=35072)\n",
      "2018-11-27T04:36:12.262126: step 3543, loss 0.769401, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 548/599 (start=35072, end=35136)\n",
      "2018-11-27T04:36:12.597110: step 3544, loss 0.746373, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 549/599 (start=35136, end=35200)\n",
      "2018-11-27T04:36:12.934116: step 3545, loss 0.670065, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 550/599 (start=35200, end=35264)\n",
      "2018-11-27T04:36:13.285828: step 3546, loss 0.892883, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 551/599 (start=35264, end=35328)\n",
      "2018-11-27T04:36:13.635925: step 3547, loss 0.92047, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 552/599 (start=35328, end=35392)\n",
      "2018-11-27T04:36:13.948380: step 3548, loss 0.698813, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 553/599 (start=35392, end=35456)\n",
      "2018-11-27T04:36:14.287252: step 3549, loss 0.659672, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 554/599 (start=35456, end=35520)\n",
      "2018-11-27T04:36:14.625312: step 3550, loss 0.962149, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 555/599 (start=35520, end=35584)\n",
      "2018-11-27T04:36:14.978271: step 3551, loss 0.813118, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 556/599 (start=35584, end=35648)\n",
      "2018-11-27T04:36:15.303510: step 3552, loss 0.620466, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 557/599 (start=35648, end=35712)\n",
      "2018-11-27T04:36:15.621679: step 3553, loss 0.96174, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 558/599 (start=35712, end=35776)\n",
      "2018-11-27T04:36:15.925843: step 3554, loss 0.808529, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 559/599 (start=35776, end=35840)\n",
      "2018-11-27T04:36:16.272858: step 3555, loss 0.8795, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 560/599 (start=35840, end=35904)\n",
      "2018-11-27T04:36:16.583541: step 3556, loss 1.09735, acc 0.71875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 561/599 (start=35904, end=35968)\n",
      "2018-11-27T04:36:16.931519: step 3557, loss 0.629119, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 562/599 (start=35968, end=36032)\n",
      "2018-11-27T04:36:17.279789: step 3558, loss 0.873711, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 563/599 (start=36032, end=36096)\n",
      "2018-11-27T04:36:17.613120: step 3559, loss 0.786292, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 564/599 (start=36096, end=36160)\n",
      "2018-11-27T04:36:17.952090: step 3560, loss 0.995783, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 565/599 (start=36160, end=36224)\n",
      "2018-11-27T04:36:18.289645: step 3561, loss 0.868037, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 566/599 (start=36224, end=36288)\n",
      "2018-11-27T04:36:18.636791: step 3562, loss 0.789586, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 567/599 (start=36288, end=36352)\n",
      "2018-11-27T04:36:18.975707: step 3563, loss 0.800707, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 568/599 (start=36352, end=36416)\n",
      "2018-11-27T04:36:19.302010: step 3564, loss 0.537215, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 569/599 (start=36416, end=36480)\n",
      "2018-11-27T04:36:19.629959: step 3565, loss 0.845623, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 570/599 (start=36480, end=36544)\n",
      "2018-11-27T04:36:19.957420: step 3566, loss 0.879012, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 571/599 (start=36544, end=36608)\n",
      "2018-11-27T04:36:20.290578: step 3567, loss 0.753249, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 572/599 (start=36608, end=36672)\n",
      "2018-11-27T04:36:20.643181: step 3568, loss 0.862084, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 573/599 (start=36672, end=36736)\n",
      "2018-11-27T04:36:20.968228: step 3569, loss 0.834435, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 574/599 (start=36736, end=36800)\n",
      "2018-11-27T04:36:21.301377: step 3570, loss 0.647661, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 575/599 (start=36800, end=36864)\n",
      "2018-11-27T04:36:21.632930: step 3571, loss 0.627351, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 576/599 (start=36864, end=36928)\n",
      "2018-11-27T04:36:21.957641: step 3572, loss 0.715619, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 577/599 (start=36928, end=36992)\n",
      "2018-11-27T04:36:22.270271: step 3573, loss 0.746292, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 578/599 (start=36992, end=37056)\n",
      "2018-11-27T04:36:22.602665: step 3574, loss 0.833081, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 579/599 (start=37056, end=37120)\n",
      "2018-11-27T04:36:22.962274: step 3575, loss 0.828773, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 580/599 (start=37120, end=37184)\n",
      "2018-11-27T04:36:23.293605: step 3576, loss 0.882939, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 581/599 (start=37184, end=37248)\n",
      "2018-11-27T04:36:23.641746: step 3577, loss 0.796661, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 582/599 (start=37248, end=37312)\n",
      "2018-11-27T04:36:23.975776: step 3578, loss 0.677676, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 583/599 (start=37312, end=37376)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:36:24.288549: step 3579, loss 0.925214, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 584/599 (start=37376, end=37440)\n",
      "2018-11-27T04:36:24.624357: step 3580, loss 0.748196, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 585/599 (start=37440, end=37504)\n",
      "2018-11-27T04:36:24.980070: step 3581, loss 0.881719, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 586/599 (start=37504, end=37568)\n",
      "2018-11-27T04:36:25.306166: step 3582, loss 0.925329, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 587/599 (start=37568, end=37632)\n",
      "2018-11-27T04:36:25.673673: step 3583, loss 0.591507, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 588/599 (start=37632, end=37696)\n",
      "2018-11-27T04:36:26.010005: step 3584, loss 0.762372, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 589/599 (start=37696, end=37760)\n",
      "2018-11-27T04:36:26.346265: step 3585, loss 0.658426, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 590/599 (start=37760, end=37824)\n",
      "2018-11-27T04:36:26.687223: step 3586, loss 0.619806, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 591/599 (start=37824, end=37888)\n",
      "2018-11-27T04:36:27.002631: step 3587, loss 0.920628, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 592/599 (start=37888, end=37952)\n",
      "2018-11-27T04:36:27.335573: step 3588, loss 0.733156, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 593/599 (start=37952, end=38016)\n",
      "2018-11-27T04:36:27.687472: step 3589, loss 0.660468, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 594/599 (start=38016, end=38080)\n",
      "2018-11-27T04:36:28.017767: step 3590, loss 0.55509, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 595/599 (start=38080, end=38144)\n",
      "2018-11-27T04:36:28.355231: step 3591, loss 0.573275, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 596/599 (start=38144, end=38208)\n",
      "2018-11-27T04:36:28.681773: step 3592, loss 0.965757, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 597/599 (start=38208, end=38272)\n",
      "2018-11-27T04:36:29.014638: step 3593, loss 1.01002, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 5/10, Batch 598/599 (start=38272, end=38281)\n",
      "2018-11-27T04:36:29.256109: step 3594, loss 1.44046, acc 0.555556\n",
      "***********************************************\n",
      "Epoch 6/10\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 0/599 (start=0, end=64)\n",
      "2018-11-27T04:36:29.601359: step 3595, loss 0.703613, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 1/599 (start=64, end=128)\n",
      "2018-11-27T04:36:29.938428: step 3596, loss 0.576206, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 2/599 (start=128, end=192)\n",
      "2018-11-27T04:36:30.268289: step 3597, loss 0.531306, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 3/599 (start=192, end=256)\n",
      "2018-11-27T04:36:30.616059: step 3598, loss 0.641097, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 4/599 (start=256, end=320)\n",
      "2018-11-27T04:36:30.928075: step 3599, loss 0.627804, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 5/599 (start=320, end=384)\n",
      "2018-11-27T04:36:31.252482: step 3600, loss 0.692876, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:36:37.578569: step 3600, loss 1.83986, acc 0.515555\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-3600\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 6/599 (start=384, end=448)\n",
      "2018-11-27T04:36:38.330375: step 3601, loss 0.631322, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 7/599 (start=448, end=512)\n",
      "2018-11-27T04:36:38.672284: step 3602, loss 0.49313, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 8/599 (start=512, end=576)\n",
      "2018-11-27T04:36:39.005279: step 3603, loss 0.876176, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 9/599 (start=576, end=640)\n",
      "2018-11-27T04:36:39.320153: step 3604, loss 0.52962, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 10/599 (start=640, end=704)\n",
      "2018-11-27T04:36:39.663735: step 3605, loss 0.604312, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 11/599 (start=704, end=768)\n",
      "2018-11-27T04:36:39.972129: step 3606, loss 0.619742, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 12/599 (start=768, end=832)\n",
      "2018-11-27T04:36:40.296902: step 3607, loss 0.555295, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 13/599 (start=832, end=896)\n",
      "2018-11-27T04:36:40.606008: step 3608, loss 0.797659, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 14/599 (start=896, end=960)\n",
      "2018-11-27T04:36:40.940215: step 3609, loss 0.64114, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 15/599 (start=960, end=1024)\n",
      "2018-11-27T04:36:41.250763: step 3610, loss 0.539679, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 16/599 (start=1024, end=1088)\n",
      "2018-11-27T04:36:41.591113: step 3611, loss 0.473901, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 17/599 (start=1088, end=1152)\n",
      "2018-11-27T04:36:41.901130: step 3612, loss 0.534252, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 18/599 (start=1152, end=1216)\n",
      "2018-11-27T04:36:42.248893: step 3613, loss 0.550509, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 19/599 (start=1216, end=1280)\n",
      "2018-11-27T04:36:42.587628: step 3614, loss 0.670649, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 20/599 (start=1280, end=1344)\n",
      "2018-11-27T04:36:42.928321: step 3615, loss 0.732972, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 21/599 (start=1344, end=1408)\n",
      "2018-11-27T04:36:43.257698: step 3616, loss 0.5163, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 22/599 (start=1408, end=1472)\n",
      "2018-11-27T04:36:43.582691: step 3617, loss 0.549774, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 23/599 (start=1472, end=1536)\n",
      "2018-11-27T04:36:43.925275: step 3618, loss 0.442523, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 24/599 (start=1536, end=1600)\n",
      "2018-11-27T04:36:44.237222: step 3619, loss 0.60017, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 25/599 (start=1600, end=1664)\n",
      "2018-11-27T04:36:44.555297: step 3620, loss 0.544997, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 26/599 (start=1664, end=1728)\n",
      "2018-11-27T04:36:44.870663: step 3621, loss 0.508865, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 27/599 (start=1728, end=1792)\n",
      "2018-11-27T04:36:45.206332: step 3622, loss 0.536296, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 28/599 (start=1792, end=1856)\n",
      "2018-11-27T04:36:45.541725: step 3623, loss 0.791651, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 29/599 (start=1856, end=1920)\n",
      "2018-11-27T04:36:45.859017: step 3624, loss 0.59309, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 30/599 (start=1920, end=1984)\n",
      "2018-11-27T04:36:46.181051: step 3625, loss 0.68549, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 31/599 (start=1984, end=2048)\n",
      "2018-11-27T04:36:46.522911: step 3626, loss 0.709595, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 32/599 (start=2048, end=2112)\n",
      "2018-11-27T04:36:46.854431: step 3627, loss 0.658678, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 33/599 (start=2112, end=2176)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:36:47.182790: step 3628, loss 0.836645, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 34/599 (start=2176, end=2240)\n",
      "2018-11-27T04:36:47.532481: step 3629, loss 0.462358, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 35/599 (start=2240, end=2304)\n",
      "2018-11-27T04:36:47.841213: step 3630, loss 0.471933, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 36/599 (start=2304, end=2368)\n",
      "2018-11-27T04:36:48.177596: step 3631, loss 0.630243, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 37/599 (start=2368, end=2432)\n",
      "2018-11-27T04:36:48.509351: step 3632, loss 0.73455, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 38/599 (start=2432, end=2496)\n",
      "2018-11-27T04:36:48.821056: step 3633, loss 0.6084, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 39/599 (start=2496, end=2560)\n",
      "2018-11-27T04:36:49.144304: step 3634, loss 0.749116, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 40/599 (start=2560, end=2624)\n",
      "2018-11-27T04:36:49.459120: step 3635, loss 0.487979, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 41/599 (start=2624, end=2688)\n",
      "2018-11-27T04:36:49.798977: step 3636, loss 0.788818, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 42/599 (start=2688, end=2752)\n",
      "2018-11-27T04:36:50.126960: step 3637, loss 0.744287, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 43/599 (start=2752, end=2816)\n",
      "2018-11-27T04:36:50.463038: step 3638, loss 0.488391, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 44/599 (start=2816, end=2880)\n",
      "2018-11-27T04:36:50.806577: step 3639, loss 0.741683, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 45/599 (start=2880, end=2944)\n",
      "2018-11-27T04:36:51.129935: step 3640, loss 0.536132, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 46/599 (start=2944, end=3008)\n",
      "2018-11-27T04:36:51.460737: step 3641, loss 0.532323, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 47/599 (start=3008, end=3072)\n",
      "2018-11-27T04:36:51.815346: step 3642, loss 0.585328, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 48/599 (start=3072, end=3136)\n",
      "2018-11-27T04:36:52.171323: step 3643, loss 0.544689, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 49/599 (start=3136, end=3200)\n",
      "2018-11-27T04:36:52.506411: step 3644, loss 0.677188, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 50/599 (start=3200, end=3264)\n",
      "2018-11-27T04:36:52.825079: step 3645, loss 0.474009, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 51/599 (start=3264, end=3328)\n",
      "2018-11-27T04:36:53.166105: step 3646, loss 0.500343, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 52/599 (start=3328, end=3392)\n",
      "2018-11-27T04:36:53.515455: step 3647, loss 0.386302, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 53/599 (start=3392, end=3456)\n",
      "2018-11-27T04:36:53.825139: step 3648, loss 0.676611, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 54/599 (start=3456, end=3520)\n",
      "2018-11-27T04:36:54.161795: step 3649, loss 0.769411, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 55/599 (start=3520, end=3584)\n",
      "2018-11-27T04:36:54.486253: step 3650, loss 0.583414, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 56/599 (start=3584, end=3648)\n",
      "2018-11-27T04:36:54.815138: step 3651, loss 0.722267, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 57/599 (start=3648, end=3712)\n",
      "2018-11-27T04:36:55.130465: step 3652, loss 0.5995, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 58/599 (start=3712, end=3776)\n",
      "2018-11-27T04:36:55.455688: step 3653, loss 0.52189, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 59/599 (start=3776, end=3840)\n",
      "2018-11-27T04:36:55.800133: step 3654, loss 0.790341, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 60/599 (start=3840, end=3904)\n",
      "2018-11-27T04:36:56.128374: step 3655, loss 0.472409, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 61/599 (start=3904, end=3968)\n",
      "2018-11-27T04:36:56.454352: step 3656, loss 0.733395, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 62/599 (start=3968, end=4032)\n",
      "2018-11-27T04:36:56.772687: step 3657, loss 0.54364, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 63/599 (start=4032, end=4096)\n",
      "2018-11-27T04:36:57.114093: step 3658, loss 0.522526, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 64/599 (start=4096, end=4160)\n",
      "2018-11-27T04:36:57.416564: step 3659, loss 0.74298, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 65/599 (start=4160, end=4224)\n",
      "2018-11-27T04:36:57.757135: step 3660, loss 0.467497, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 66/599 (start=4224, end=4288)\n",
      "2018-11-27T04:36:58.099157: step 3661, loss 1.02117, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 67/599 (start=4288, end=4352)\n",
      "2018-11-27T04:36:58.431187: step 3662, loss 0.579554, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 68/599 (start=4352, end=4416)\n",
      "2018-11-27T04:36:58.762538: step 3663, loss 0.55918, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 69/599 (start=4416, end=4480)\n",
      "2018-11-27T04:36:59.122721: step 3664, loss 0.731196, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 70/599 (start=4480, end=4544)\n",
      "2018-11-27T04:36:59.443196: step 3665, loss 0.766649, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 71/599 (start=4544, end=4608)\n",
      "2018-11-27T04:36:59.781105: step 3666, loss 0.771721, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 72/599 (start=4608, end=4672)\n",
      "2018-11-27T04:37:00.117635: step 3667, loss 0.399993, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 73/599 (start=4672, end=4736)\n",
      "2018-11-27T04:37:00.475591: step 3668, loss 0.704973, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 74/599 (start=4736, end=4800)\n",
      "2018-11-27T04:37:00.820264: step 3669, loss 0.821943, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 75/599 (start=4800, end=4864)\n",
      "2018-11-27T04:37:01.143622: step 3670, loss 0.732512, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 76/599 (start=4864, end=4928)\n",
      "2018-11-27T04:37:01.482770: step 3671, loss 0.642984, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 77/599 (start=4928, end=4992)\n",
      "2018-11-27T04:37:01.806794: step 3672, loss 0.61974, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 78/599 (start=4992, end=5056)\n",
      "2018-11-27T04:37:02.138602: step 3673, loss 0.682854, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 79/599 (start=5056, end=5120)\n",
      "2018-11-27T04:37:02.447443: step 3674, loss 0.781216, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 80/599 (start=5120, end=5184)\n",
      "2018-11-27T04:37:02.763867: step 3675, loss 0.703598, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 81/599 (start=5184, end=5248)\n",
      "2018-11-27T04:37:03.087326: step 3676, loss 0.788363, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 82/599 (start=5248, end=5312)\n",
      "2018-11-27T04:37:03.399255: step 3677, loss 0.551875, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 83/599 (start=5312, end=5376)\n",
      "2018-11-27T04:37:03.724895: step 3678, loss 0.90487, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 84/599 (start=5376, end=5440)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:37:04.048126: step 3679, loss 0.487613, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 85/599 (start=5440, end=5504)\n",
      "2018-11-27T04:37:04.370626: step 3680, loss 0.604664, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 86/599 (start=5504, end=5568)\n",
      "2018-11-27T04:37:04.684064: step 3681, loss 0.694924, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 87/599 (start=5568, end=5632)\n",
      "2018-11-27T04:37:05.025296: step 3682, loss 0.84405, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 88/599 (start=5632, end=5696)\n",
      "2018-11-27T04:37:05.339407: step 3683, loss 0.656371, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 89/599 (start=5696, end=5760)\n",
      "2018-11-27T04:37:05.651128: step 3684, loss 0.641731, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 90/599 (start=5760, end=5824)\n",
      "2018-11-27T04:37:05.980666: step 3685, loss 0.692727, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 91/599 (start=5824, end=5888)\n",
      "2018-11-27T04:37:06.309236: step 3686, loss 0.674832, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 92/599 (start=5888, end=5952)\n",
      "2018-11-27T04:37:06.656586: step 3687, loss 0.88885, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 93/599 (start=5952, end=6016)\n",
      "2018-11-27T04:37:06.982380: step 3688, loss 0.538954, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 94/599 (start=6016, end=6080)\n",
      "2018-11-27T04:37:07.319014: step 3689, loss 0.61725, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 95/599 (start=6080, end=6144)\n",
      "2018-11-27T04:37:07.663109: step 3690, loss 0.706611, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 96/599 (start=6144, end=6208)\n",
      "2018-11-27T04:37:08.012834: step 3691, loss 0.538455, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 97/599 (start=6208, end=6272)\n",
      "2018-11-27T04:37:08.322066: step 3692, loss 0.804987, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 98/599 (start=6272, end=6336)\n",
      "2018-11-27T04:37:08.648348: step 3693, loss 0.639427, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 99/599 (start=6336, end=6400)\n",
      "2018-11-27T04:37:09.000854: step 3694, loss 0.665687, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 100/599 (start=6400, end=6464)\n",
      "2018-11-27T04:37:09.321975: step 3695, loss 0.718567, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 101/599 (start=6464, end=6528)\n",
      "2018-11-27T04:37:09.634564: step 3696, loss 0.661361, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 102/599 (start=6528, end=6592)\n",
      "2018-11-27T04:37:09.990732: step 3697, loss 0.854426, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 103/599 (start=6592, end=6656)\n",
      "2018-11-27T04:37:10.333731: step 3698, loss 0.635286, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 104/599 (start=6656, end=6720)\n",
      "2018-11-27T04:37:10.670144: step 3699, loss 0.653708, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 105/599 (start=6720, end=6784)\n",
      "2018-11-27T04:37:11.020273: step 3700, loss 0.510313, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:37:17.163400: step 3700, loss 1.87426, acc 0.516182\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-3700\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 106/599 (start=6784, end=6848)\n",
      "2018-11-27T04:37:17.915475: step 3701, loss 0.70492, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 107/599 (start=6848, end=6912)\n",
      "2018-11-27T04:37:18.258030: step 3702, loss 0.579181, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 108/599 (start=6912, end=6976)\n",
      "2018-11-27T04:37:18.592491: step 3703, loss 0.924273, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 109/599 (start=6976, end=7040)\n",
      "2018-11-27T04:37:18.921092: step 3704, loss 0.61069, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 110/599 (start=7040, end=7104)\n",
      "2018-11-27T04:37:19.246054: step 3705, loss 0.683322, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 111/599 (start=7104, end=7168)\n",
      "2018-11-27T04:37:19.582922: step 3706, loss 0.70027, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 112/599 (start=7168, end=7232)\n",
      "2018-11-27T04:37:19.931678: step 3707, loss 0.525001, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 113/599 (start=7232, end=7296)\n",
      "2018-11-27T04:37:20.274429: step 3708, loss 0.723488, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 114/599 (start=7296, end=7360)\n",
      "2018-11-27T04:37:20.630316: step 3709, loss 0.725255, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 115/599 (start=7360, end=7424)\n",
      "2018-11-27T04:37:20.985703: step 3710, loss 0.780077, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 116/599 (start=7424, end=7488)\n",
      "2018-11-27T04:37:21.316240: step 3711, loss 0.758908, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 117/599 (start=7488, end=7552)\n",
      "2018-11-27T04:37:21.639470: step 3712, loss 0.672354, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 118/599 (start=7552, end=7616)\n",
      "2018-11-27T04:37:21.973188: step 3713, loss 0.611495, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 119/599 (start=7616, end=7680)\n",
      "2018-11-27T04:37:22.288843: step 3714, loss 0.781866, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 120/599 (start=7680, end=7744)\n",
      "2018-11-27T04:37:22.630769: step 3715, loss 0.589715, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 121/599 (start=7744, end=7808)\n",
      "2018-11-27T04:37:22.965281: step 3716, loss 0.650536, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 122/599 (start=7808, end=7872)\n",
      "2018-11-27T04:37:23.303492: step 3717, loss 0.681, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 123/599 (start=7872, end=7936)\n",
      "2018-11-27T04:37:23.642393: step 3718, loss 0.764622, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 124/599 (start=7936, end=8000)\n",
      "2018-11-27T04:37:23.974082: step 3719, loss 0.474643, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 125/599 (start=8000, end=8064)\n",
      "2018-11-27T04:37:24.328979: step 3720, loss 0.689353, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 126/599 (start=8064, end=8128)\n",
      "2018-11-27T04:37:24.671911: step 3721, loss 0.666166, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 127/599 (start=8128, end=8192)\n",
      "2018-11-27T04:37:25.018339: step 3722, loss 0.439521, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 128/599 (start=8192, end=8256)\n",
      "2018-11-27T04:37:25.331344: step 3723, loss 0.738213, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 129/599 (start=8256, end=8320)\n",
      "2018-11-27T04:37:25.662812: step 3724, loss 0.4449, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 130/599 (start=8320, end=8384)\n",
      "2018-11-27T04:37:26.003122: step 3725, loss 0.643821, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 131/599 (start=8384, end=8448)\n",
      "2018-11-27T04:37:26.339133: step 3726, loss 0.73298, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 132/599 (start=8448, end=8512)\n",
      "2018-11-27T04:37:26.694742: step 3727, loss 0.807416, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 133/599 (start=8512, end=8576)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:37:27.030547: step 3728, loss 0.816577, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 134/599 (start=8576, end=8640)\n",
      "2018-11-27T04:37:27.347000: step 3729, loss 0.477004, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 135/599 (start=8640, end=8704)\n",
      "2018-11-27T04:37:27.660283: step 3730, loss 0.558809, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 136/599 (start=8704, end=8768)\n",
      "2018-11-27T04:37:27.990290: step 3731, loss 0.564196, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 137/599 (start=8768, end=8832)\n",
      "2018-11-27T04:37:28.334804: step 3732, loss 0.678664, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 138/599 (start=8832, end=8896)\n",
      "2018-11-27T04:37:28.644694: step 3733, loss 0.676296, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 139/599 (start=8896, end=8960)\n",
      "2018-11-27T04:37:28.995058: step 3734, loss 0.520664, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 140/599 (start=8960, end=9024)\n",
      "2018-11-27T04:37:29.318826: step 3735, loss 0.443345, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 141/599 (start=9024, end=9088)\n",
      "2018-11-27T04:37:29.667292: step 3736, loss 0.862184, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 142/599 (start=9088, end=9152)\n",
      "2018-11-27T04:37:30.004602: step 3737, loss 0.823786, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 143/599 (start=9152, end=9216)\n",
      "2018-11-27T04:37:30.349029: step 3738, loss 0.888823, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 144/599 (start=9216, end=9280)\n",
      "2018-11-27T04:37:30.694122: step 3739, loss 0.477317, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 145/599 (start=9280, end=9344)\n",
      "2018-11-27T04:37:31.033938: step 3740, loss 0.878057, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 146/599 (start=9344, end=9408)\n",
      "2018-11-27T04:37:31.371955: step 3741, loss 0.834826, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 147/599 (start=9408, end=9472)\n",
      "2018-11-27T04:37:31.709799: step 3742, loss 0.825321, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 148/599 (start=9472, end=9536)\n",
      "2018-11-27T04:37:32.040945: step 3743, loss 0.847258, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 149/599 (start=9536, end=9600)\n",
      "2018-11-27T04:37:32.367474: step 3744, loss 0.865676, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 150/599 (start=9600, end=9664)\n",
      "2018-11-27T04:37:32.684506: step 3745, loss 0.597773, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 151/599 (start=9664, end=9728)\n",
      "2018-11-27T04:37:33.029620: step 3746, loss 0.751353, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 152/599 (start=9728, end=9792)\n",
      "2018-11-27T04:37:33.337746: step 3747, loss 0.721916, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 153/599 (start=9792, end=9856)\n",
      "2018-11-27T04:37:33.672390: step 3748, loss 0.61171, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 154/599 (start=9856, end=9920)\n",
      "2018-11-27T04:37:33.991363: step 3749, loss 0.583267, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 155/599 (start=9920, end=9984)\n",
      "2018-11-27T04:37:34.333889: step 3750, loss 0.657603, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 156/599 (start=9984, end=10048)\n",
      "2018-11-27T04:37:34.671695: step 3751, loss 0.878843, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 157/599 (start=10048, end=10112)\n",
      "2018-11-27T04:37:35.018728: step 3752, loss 0.808985, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 158/599 (start=10112, end=10176)\n",
      "2018-11-27T04:37:35.327652: step 3753, loss 0.694973, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 159/599 (start=10176, end=10240)\n",
      "2018-11-27T04:37:35.665739: step 3754, loss 0.746346, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 160/599 (start=10240, end=10304)\n",
      "2018-11-27T04:37:35.997780: step 3755, loss 0.613918, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 161/599 (start=10304, end=10368)\n",
      "2018-11-27T04:37:36.344406: step 3756, loss 0.65722, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 162/599 (start=10368, end=10432)\n",
      "2018-11-27T04:37:36.695961: step 3757, loss 0.782605, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 163/599 (start=10432, end=10496)\n",
      "2018-11-27T04:37:37.029097: step 3758, loss 0.623009, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 164/599 (start=10496, end=10560)\n",
      "2018-11-27T04:37:37.362911: step 3759, loss 0.47743, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 165/599 (start=10560, end=10624)\n",
      "2018-11-27T04:37:37.704521: step 3760, loss 0.625139, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 166/599 (start=10624, end=10688)\n",
      "2018-11-27T04:37:38.045016: step 3761, loss 0.545097, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 167/599 (start=10688, end=10752)\n",
      "2018-11-27T04:37:38.381414: step 3762, loss 0.754548, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 168/599 (start=10752, end=10816)\n",
      "2018-11-27T04:37:38.724154: step 3763, loss 0.719121, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 169/599 (start=10816, end=10880)\n",
      "2018-11-27T04:37:39.066029: step 3764, loss 0.815004, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 170/599 (start=10880, end=10944)\n",
      "2018-11-27T04:37:39.375983: step 3765, loss 0.681857, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 171/599 (start=10944, end=11008)\n",
      "2018-11-27T04:37:39.716688: step 3766, loss 0.463894, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 172/599 (start=11008, end=11072)\n",
      "2018-11-27T04:37:40.066270: step 3767, loss 0.583884, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 173/599 (start=11072, end=11136)\n",
      "2018-11-27T04:37:40.385089: step 3768, loss 0.603797, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 174/599 (start=11136, end=11200)\n",
      "2018-11-27T04:37:40.709949: step 3769, loss 0.878841, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 175/599 (start=11200, end=11264)\n",
      "2018-11-27T04:37:41.025794: step 3770, loss 0.763298, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 176/599 (start=11264, end=11328)\n",
      "2018-11-27T04:37:41.357153: step 3771, loss 0.450042, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 177/599 (start=11328, end=11392)\n",
      "2018-11-27T04:37:41.682030: step 3772, loss 0.522138, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 178/599 (start=11392, end=11456)\n",
      "2018-11-27T04:37:42.017804: step 3773, loss 0.8606, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 179/599 (start=11456, end=11520)\n",
      "2018-11-27T04:37:42.356999: step 3774, loss 0.726675, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 180/599 (start=11520, end=11584)\n",
      "2018-11-27T04:37:42.703128: step 3775, loss 0.644949, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 181/599 (start=11584, end=11648)\n",
      "2018-11-27T04:37:43.018834: step 3776, loss 0.870147, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 182/599 (start=11648, end=11712)\n",
      "2018-11-27T04:37:43.330336: step 3777, loss 0.569903, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 183/599 (start=11712, end=11776)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:37:43.684332: step 3778, loss 0.723232, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 184/599 (start=11776, end=11840)\n",
      "2018-11-27T04:37:44.029194: step 3779, loss 0.582851, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 185/599 (start=11840, end=11904)\n",
      "2018-11-27T04:37:44.354548: step 3780, loss 0.642152, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 186/599 (start=11904, end=11968)\n",
      "2018-11-27T04:37:44.679516: step 3781, loss 0.674007, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 187/599 (start=11968, end=12032)\n",
      "2018-11-27T04:37:45.019544: step 3782, loss 0.750293, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 188/599 (start=12032, end=12096)\n",
      "2018-11-27T04:37:45.347073: step 3783, loss 0.695768, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 189/599 (start=12096, end=12160)\n",
      "2018-11-27T04:37:45.689853: step 3784, loss 0.590257, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 190/599 (start=12160, end=12224)\n",
      "2018-11-27T04:37:46.020168: step 3785, loss 0.613515, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 191/599 (start=12224, end=12288)\n",
      "2018-11-27T04:37:46.363443: step 3786, loss 0.783113, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 192/599 (start=12288, end=12352)\n",
      "2018-11-27T04:37:46.684374: step 3787, loss 0.758667, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 193/599 (start=12352, end=12416)\n",
      "2018-11-27T04:37:47.002713: step 3788, loss 0.646867, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 194/599 (start=12416, end=12480)\n",
      "2018-11-27T04:37:47.341762: step 3789, loss 0.891252, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 195/599 (start=12480, end=12544)\n",
      "2018-11-27T04:37:47.685294: step 3790, loss 0.597772, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 196/599 (start=12544, end=12608)\n",
      "2018-11-27T04:37:48.027198: step 3791, loss 0.83413, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 197/599 (start=12608, end=12672)\n",
      "2018-11-27T04:37:48.355239: step 3792, loss 0.558383, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 198/599 (start=12672, end=12736)\n",
      "2018-11-27T04:37:48.700507: step 3793, loss 0.646322, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 199/599 (start=12736, end=12800)\n",
      "2018-11-27T04:37:49.043581: step 3794, loss 0.480865, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 200/599 (start=12800, end=12864)\n",
      "2018-11-27T04:37:49.388946: step 3795, loss 0.561794, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 201/599 (start=12864, end=12928)\n",
      "2018-11-27T04:37:49.715265: step 3796, loss 0.605657, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 202/599 (start=12928, end=12992)\n",
      "2018-11-27T04:37:50.065206: step 3797, loss 0.755384, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 203/599 (start=12992, end=13056)\n",
      "2018-11-27T04:37:50.405223: step 3798, loss 0.948863, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 204/599 (start=13056, end=13120)\n",
      "2018-11-27T04:37:50.740486: step 3799, loss 0.638725, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 205/599 (start=13120, end=13184)\n",
      "2018-11-27T04:37:51.070437: step 3800, loss 0.548309, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:37:57.459267: step 3800, loss 1.87674, acc 0.513126\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-3800\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 206/599 (start=13184, end=13248)\n",
      "2018-11-27T04:37:58.198625: step 3801, loss 0.460904, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 207/599 (start=13248, end=13312)\n",
      "2018-11-27T04:37:58.547159: step 3802, loss 0.624894, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 208/599 (start=13312, end=13376)\n",
      "2018-11-27T04:37:58.883129: step 3803, loss 0.775101, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 209/599 (start=13376, end=13440)\n",
      "2018-11-27T04:37:59.221322: step 3804, loss 0.739788, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 210/599 (start=13440, end=13504)\n",
      "2018-11-27T04:37:59.535558: step 3805, loss 0.496236, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 211/599 (start=13504, end=13568)\n",
      "2018-11-27T04:37:59.861666: step 3806, loss 0.867905, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 212/599 (start=13568, end=13632)\n",
      "2018-11-27T04:38:00.186782: step 3807, loss 0.567161, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 213/599 (start=13632, end=13696)\n",
      "2018-11-27T04:38:00.521062: step 3808, loss 0.710646, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 214/599 (start=13696, end=13760)\n",
      "2018-11-27T04:38:00.861799: step 3809, loss 0.671841, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 215/599 (start=13760, end=13824)\n",
      "2018-11-27T04:38:01.204282: step 3810, loss 0.52502, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 216/599 (start=13824, end=13888)\n",
      "2018-11-27T04:38:01.551361: step 3811, loss 0.64711, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 217/599 (start=13888, end=13952)\n",
      "2018-11-27T04:38:01.873473: step 3812, loss 0.695002, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 218/599 (start=13952, end=14016)\n",
      "2018-11-27T04:38:02.199553: step 3813, loss 0.598363, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 219/599 (start=14016, end=14080)\n",
      "2018-11-27T04:38:02.515471: step 3814, loss 0.639095, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 220/599 (start=14080, end=14144)\n",
      "2018-11-27T04:38:02.870862: step 3815, loss 0.801869, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 221/599 (start=14144, end=14208)\n",
      "2018-11-27T04:38:03.219800: step 3816, loss 0.549778, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 222/599 (start=14208, end=14272)\n",
      "2018-11-27T04:38:03.547798: step 3817, loss 0.743581, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 223/599 (start=14272, end=14336)\n",
      "2018-11-27T04:38:03.887416: step 3818, loss 0.755086, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 224/599 (start=14336, end=14400)\n",
      "2018-11-27T04:38:04.207656: step 3819, loss 0.746182, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 225/599 (start=14400, end=14464)\n",
      "2018-11-27T04:38:04.529274: step 3820, loss 0.711337, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 226/599 (start=14464, end=14528)\n",
      "2018-11-27T04:38:04.832386: step 3821, loss 0.726248, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 227/599 (start=14528, end=14592)\n",
      "2018-11-27T04:38:05.179363: step 3822, loss 0.65563, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 228/599 (start=14592, end=14656)\n",
      "2018-11-27T04:38:05.501911: step 3823, loss 0.740163, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 229/599 (start=14656, end=14720)\n",
      "2018-11-27T04:38:05.843383: step 3824, loss 0.544069, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 230/599 (start=14720, end=14784)\n",
      "2018-11-27T04:38:06.181349: step 3825, loss 0.569898, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 231/599 (start=14784, end=14848)\n",
      "2018-11-27T04:38:06.505433: step 3826, loss 0.513383, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 232/599 (start=14848, end=14912)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:38:06.828183: step 3827, loss 0.59304, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 233/599 (start=14912, end=14976)\n",
      "2018-11-27T04:38:07.161256: step 3828, loss 0.542296, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 234/599 (start=14976, end=15040)\n",
      "2018-11-27T04:38:07.503367: step 3829, loss 0.653972, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 235/599 (start=15040, end=15104)\n",
      "2018-11-27T04:38:07.815947: step 3830, loss 0.468557, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 236/599 (start=15104, end=15168)\n",
      "2018-11-27T04:38:08.140035: step 3831, loss 0.637086, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 237/599 (start=15168, end=15232)\n",
      "2018-11-27T04:38:08.468787: step 3832, loss 0.467211, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 238/599 (start=15232, end=15296)\n",
      "2018-11-27T04:38:08.797911: step 3833, loss 0.891072, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 239/599 (start=15296, end=15360)\n",
      "2018-11-27T04:38:09.121108: step 3834, loss 0.603905, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 240/599 (start=15360, end=15424)\n",
      "2018-11-27T04:38:09.439849: step 3835, loss 0.949369, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 241/599 (start=15424, end=15488)\n",
      "2018-11-27T04:38:09.751036: step 3836, loss 0.812486, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 242/599 (start=15488, end=15552)\n",
      "2018-11-27T04:38:10.084250: step 3837, loss 0.528421, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 243/599 (start=15552, end=15616)\n",
      "2018-11-27T04:38:10.416370: step 3838, loss 0.621244, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 244/599 (start=15616, end=15680)\n",
      "2018-11-27T04:38:10.731466: step 3839, loss 0.672683, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 245/599 (start=15680, end=15744)\n",
      "2018-11-27T04:38:11.040339: step 3840, loss 0.863964, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 246/599 (start=15744, end=15808)\n",
      "2018-11-27T04:38:11.356572: step 3841, loss 0.60363, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 247/599 (start=15808, end=15872)\n",
      "2018-11-27T04:38:11.700158: step 3842, loss 0.578416, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 248/599 (start=15872, end=15936)\n",
      "2018-11-27T04:38:12.034887: step 3843, loss 0.732099, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 249/599 (start=15936, end=16000)\n",
      "2018-11-27T04:38:12.378417: step 3844, loss 0.917996, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 250/599 (start=16000, end=16064)\n",
      "2018-11-27T04:38:12.723878: step 3845, loss 0.656441, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 251/599 (start=16064, end=16128)\n",
      "2018-11-27T04:38:13.056123: step 3846, loss 0.439729, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 252/599 (start=16128, end=16192)\n",
      "2018-11-27T04:38:13.379012: step 3847, loss 0.853717, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 253/599 (start=16192, end=16256)\n",
      "2018-11-27T04:38:13.719789: step 3848, loss 0.600112, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 254/599 (start=16256, end=16320)\n",
      "2018-11-27T04:38:14.053651: step 3849, loss 0.817125, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 255/599 (start=16320, end=16384)\n",
      "2018-11-27T04:38:14.366747: step 3850, loss 0.626624, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 256/599 (start=16384, end=16448)\n",
      "2018-11-27T04:38:14.691252: step 3851, loss 0.511554, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 257/599 (start=16448, end=16512)\n",
      "2018-11-27T04:38:15.001694: step 3852, loss 0.751176, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 258/599 (start=16512, end=16576)\n",
      "2018-11-27T04:38:15.340827: step 3853, loss 0.655824, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 259/599 (start=16576, end=16640)\n",
      "2018-11-27T04:38:15.669350: step 3854, loss 0.697775, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 260/599 (start=16640, end=16704)\n",
      "2018-11-27T04:38:16.005971: step 3855, loss 0.596695, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 261/599 (start=16704, end=16768)\n",
      "2018-11-27T04:38:16.352170: step 3856, loss 0.635003, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 262/599 (start=16768, end=16832)\n",
      "2018-11-27T04:38:16.693815: step 3857, loss 0.654695, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 263/599 (start=16832, end=16896)\n",
      "2018-11-27T04:38:17.019178: step 3858, loss 0.56617, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 264/599 (start=16896, end=16960)\n",
      "2018-11-27T04:38:17.339731: step 3859, loss 0.578051, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 265/599 (start=16960, end=17024)\n",
      "2018-11-27T04:38:17.672395: step 3860, loss 0.814843, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 266/599 (start=17024, end=17088)\n",
      "2018-11-27T04:38:17.997779: step 3861, loss 0.480978, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 267/599 (start=17088, end=17152)\n",
      "2018-11-27T04:38:18.315761: step 3862, loss 0.527857, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 268/599 (start=17152, end=17216)\n",
      "2018-11-27T04:38:18.646631: step 3863, loss 0.646347, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 269/599 (start=17216, end=17280)\n",
      "2018-11-27T04:38:18.982075: step 3864, loss 0.685962, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 270/599 (start=17280, end=17344)\n",
      "2018-11-27T04:38:19.323283: step 3865, loss 0.548438, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 271/599 (start=17344, end=17408)\n",
      "2018-11-27T04:38:19.651842: step 3866, loss 0.78514, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 272/599 (start=17408, end=17472)\n",
      "2018-11-27T04:38:20.002279: step 3867, loss 0.737725, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 273/599 (start=17472, end=17536)\n",
      "2018-11-27T04:38:20.342309: step 3868, loss 0.781489, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 274/599 (start=17536, end=17600)\n",
      "2018-11-27T04:38:20.652038: step 3869, loss 0.642868, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 275/599 (start=17600, end=17664)\n",
      "2018-11-27T04:38:20.976387: step 3870, loss 0.73468, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 276/599 (start=17664, end=17728)\n",
      "2018-11-27T04:38:21.300832: step 3871, loss 0.588627, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 277/599 (start=17728, end=17792)\n",
      "2018-11-27T04:38:21.628631: step 3872, loss 0.560106, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 278/599 (start=17792, end=17856)\n",
      "2018-11-27T04:38:21.990914: step 3873, loss 0.848854, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 279/599 (start=17856, end=17920)\n",
      "2018-11-27T04:38:22.322091: step 3874, loss 0.756784, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 280/599 (start=17920, end=17984)\n",
      "2018-11-27T04:38:22.661970: step 3875, loss 0.704475, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 281/599 (start=17984, end=18048)\n",
      "2018-11-27T04:38:22.993997: step 3876, loss 0.782726, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 282/599 (start=18048, end=18112)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:38:23.341126: step 3877, loss 0.608377, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 283/599 (start=18112, end=18176)\n",
      "2018-11-27T04:38:23.677080: step 3878, loss 0.523566, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 284/599 (start=18176, end=18240)\n",
      "2018-11-27T04:38:24.009190: step 3879, loss 0.742464, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 285/599 (start=18240, end=18304)\n",
      "2018-11-27T04:38:24.347103: step 3880, loss 0.691836, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 286/599 (start=18304, end=18368)\n",
      "2018-11-27T04:38:24.684688: step 3881, loss 0.710372, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 287/599 (start=18368, end=18432)\n",
      "2018-11-27T04:38:25.018982: step 3882, loss 0.960972, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 288/599 (start=18432, end=18496)\n",
      "2018-11-27T04:38:25.352359: step 3883, loss 0.576277, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 289/599 (start=18496, end=18560)\n",
      "2018-11-27T04:38:25.678136: step 3884, loss 0.83489, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 290/599 (start=18560, end=18624)\n",
      "2018-11-27T04:38:26.015926: step 3885, loss 0.732455, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 291/599 (start=18624, end=18688)\n",
      "2018-11-27T04:38:26.331656: step 3886, loss 0.612933, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 292/599 (start=18688, end=18752)\n",
      "2018-11-27T04:38:26.672706: step 3887, loss 0.703236, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 293/599 (start=18752, end=18816)\n",
      "2018-11-27T04:38:27.017719: step 3888, loss 0.659859, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 294/599 (start=18816, end=18880)\n",
      "2018-11-27T04:38:27.360423: step 3889, loss 0.485012, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 295/599 (start=18880, end=18944)\n",
      "2018-11-27T04:38:27.687771: step 3890, loss 0.751395, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 296/599 (start=18944, end=19008)\n",
      "2018-11-27T04:38:28.030089: step 3891, loss 0.462985, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 297/599 (start=19008, end=19072)\n",
      "2018-11-27T04:38:28.335730: step 3892, loss 0.598152, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 298/599 (start=19072, end=19136)\n",
      "2018-11-27T04:38:28.677228: step 3893, loss 0.656493, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 299/599 (start=19136, end=19200)\n",
      "2018-11-27T04:38:28.987269: step 3894, loss 0.712547, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 300/599 (start=19200, end=19264)\n",
      "2018-11-27T04:38:29.304692: step 3895, loss 0.824982, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 301/599 (start=19264, end=19328)\n",
      "2018-11-27T04:38:29.638739: step 3896, loss 0.716214, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 302/599 (start=19328, end=19392)\n",
      "2018-11-27T04:38:29.967790: step 3897, loss 0.541473, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 303/599 (start=19392, end=19456)\n",
      "2018-11-27T04:38:30.293393: step 3898, loss 0.706591, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 304/599 (start=19456, end=19520)\n",
      "2018-11-27T04:38:30.635512: step 3899, loss 0.545919, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 305/599 (start=19520, end=19584)\n",
      "2018-11-27T04:38:30.990457: step 3900, loss 0.906575, acc 0.796875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:38:37.316392: step 3900, loss 1.88904, acc 0.516104\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-3900\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 306/599 (start=19584, end=19648)\n",
      "2018-11-27T04:38:38.065571: step 3901, loss 0.762732, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 307/599 (start=19648, end=19712)\n",
      "2018-11-27T04:38:38.388599: step 3902, loss 0.62884, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 308/599 (start=19712, end=19776)\n",
      "2018-11-27T04:38:38.715624: step 3903, loss 0.494383, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 309/599 (start=19776, end=19840)\n",
      "2018-11-27T04:38:39.047466: step 3904, loss 0.780169, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 310/599 (start=19840, end=19904)\n",
      "2018-11-27T04:38:39.387203: step 3905, loss 0.660037, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 311/599 (start=19904, end=19968)\n",
      "2018-11-27T04:38:39.706712: step 3906, loss 0.622685, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 312/599 (start=19968, end=20032)\n",
      "2018-11-27T04:38:40.027802: step 3907, loss 0.641345, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 313/599 (start=20032, end=20096)\n",
      "2018-11-27T04:38:40.372519: step 3908, loss 0.893914, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 314/599 (start=20096, end=20160)\n",
      "2018-11-27T04:38:40.692465: step 3909, loss 0.691941, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 315/599 (start=20160, end=20224)\n",
      "2018-11-27T04:38:41.019222: step 3910, loss 0.709092, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 316/599 (start=20224, end=20288)\n",
      "2018-11-27T04:38:41.363111: step 3911, loss 0.757891, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 317/599 (start=20288, end=20352)\n",
      "2018-11-27T04:38:41.687725: step 3912, loss 0.480887, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 318/599 (start=20352, end=20416)\n",
      "2018-11-27T04:38:42.012690: step 3913, loss 0.567896, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 319/599 (start=20416, end=20480)\n",
      "2018-11-27T04:38:42.358938: step 3914, loss 0.766329, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 320/599 (start=20480, end=20544)\n",
      "2018-11-27T04:38:42.676932: step 3915, loss 0.688325, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 321/599 (start=20544, end=20608)\n",
      "2018-11-27T04:38:43.033672: step 3916, loss 0.608808, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 322/599 (start=20608, end=20672)\n",
      "2018-11-27T04:38:43.360820: step 3917, loss 0.773362, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 323/599 (start=20672, end=20736)\n",
      "2018-11-27T04:38:43.684679: step 3918, loss 0.754424, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 324/599 (start=20736, end=20800)\n",
      "2018-11-27T04:38:44.006479: step 3919, loss 0.535035, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 325/599 (start=20800, end=20864)\n",
      "2018-11-27T04:38:44.345797: step 3920, loss 0.680067, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 326/599 (start=20864, end=20928)\n",
      "2018-11-27T04:38:44.686375: step 3921, loss 0.71822, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 327/599 (start=20928, end=20992)\n",
      "2018-11-27T04:38:45.034708: step 3922, loss 0.770141, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 328/599 (start=20992, end=21056)\n",
      "2018-11-27T04:38:45.356211: step 3923, loss 0.761638, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 329/599 (start=21056, end=21120)\n",
      "2018-11-27T04:38:45.691992: step 3924, loss 0.720763, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 330/599 (start=21120, end=21184)\n",
      "2018-11-27T04:38:46.030255: step 3925, loss 0.712582, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 331/599 (start=21184, end=21248)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:38:46.363195: step 3926, loss 0.553439, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 332/599 (start=21248, end=21312)\n",
      "2018-11-27T04:38:46.693124: step 3927, loss 0.680449, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 333/599 (start=21312, end=21376)\n",
      "2018-11-27T04:38:46.999424: step 3928, loss 0.723075, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 334/599 (start=21376, end=21440)\n",
      "2018-11-27T04:38:47.337827: step 3929, loss 0.586459, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 335/599 (start=21440, end=21504)\n",
      "2018-11-27T04:38:47.651881: step 3930, loss 0.930695, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 336/599 (start=21504, end=21568)\n",
      "2018-11-27T04:38:47.985920: step 3931, loss 0.483058, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 337/599 (start=21568, end=21632)\n",
      "2018-11-27T04:38:48.333714: step 3932, loss 0.496129, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 338/599 (start=21632, end=21696)\n",
      "2018-11-27T04:38:48.664646: step 3933, loss 0.637426, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 339/599 (start=21696, end=21760)\n",
      "2018-11-27T04:38:49.024034: step 3934, loss 0.679149, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 340/599 (start=21760, end=21824)\n",
      "2018-11-27T04:38:49.344846: step 3935, loss 0.765579, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 341/599 (start=21824, end=21888)\n",
      "2018-11-27T04:38:49.701959: step 3936, loss 0.588191, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 342/599 (start=21888, end=21952)\n",
      "2018-11-27T04:38:50.022717: step 3937, loss 0.567776, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 343/599 (start=21952, end=22016)\n",
      "2018-11-27T04:38:50.357034: step 3938, loss 0.71876, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 344/599 (start=22016, end=22080)\n",
      "2018-11-27T04:38:50.690695: step 3939, loss 0.626936, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 345/599 (start=22080, end=22144)\n",
      "2018-11-27T04:38:51.024237: step 3940, loss 0.552913, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 346/599 (start=22144, end=22208)\n",
      "2018-11-27T04:38:51.368543: step 3941, loss 0.567594, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 347/599 (start=22208, end=22272)\n",
      "2018-11-27T04:38:51.700450: step 3942, loss 0.754598, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 348/599 (start=22272, end=22336)\n",
      "2018-11-27T04:38:52.048540: step 3943, loss 0.781996, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 349/599 (start=22336, end=22400)\n",
      "2018-11-27T04:38:52.396899: step 3944, loss 0.511019, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 350/599 (start=22400, end=22464)\n",
      "2018-11-27T04:38:52.705428: step 3945, loss 0.581582, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 351/599 (start=22464, end=22528)\n",
      "2018-11-27T04:38:53.023721: step 3946, loss 0.757292, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 352/599 (start=22528, end=22592)\n",
      "2018-11-27T04:38:53.359732: step 3947, loss 0.728537, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 353/599 (start=22592, end=22656)\n",
      "2018-11-27T04:38:53.698367: step 3948, loss 0.542341, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 354/599 (start=22656, end=22720)\n",
      "2018-11-27T04:38:54.021726: step 3949, loss 0.82365, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 355/599 (start=22720, end=22784)\n",
      "2018-11-27T04:38:54.368064: step 3950, loss 0.621158, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 356/599 (start=22784, end=22848)\n",
      "2018-11-27T04:38:54.681219: step 3951, loss 0.759771, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 357/599 (start=22848, end=22912)\n",
      "2018-11-27T04:38:55.006899: step 3952, loss 0.598741, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 358/599 (start=22912, end=22976)\n",
      "2018-11-27T04:38:55.345171: step 3953, loss 0.661054, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 359/599 (start=22976, end=23040)\n",
      "2018-11-27T04:38:55.703802: step 3954, loss 0.530625, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 360/599 (start=23040, end=23104)\n",
      "2018-11-27T04:38:56.047791: step 3955, loss 0.5615, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 361/599 (start=23104, end=23168)\n",
      "2018-11-27T04:38:56.375194: step 3956, loss 0.588389, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 362/599 (start=23168, end=23232)\n",
      "2018-11-27T04:38:56.719919: step 3957, loss 0.540688, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 363/599 (start=23232, end=23296)\n",
      "2018-11-27T04:38:57.065428: step 3958, loss 0.550493, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 364/599 (start=23296, end=23360)\n",
      "2018-11-27T04:38:57.420110: step 3959, loss 0.687066, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 365/599 (start=23360, end=23424)\n",
      "2018-11-27T04:38:57.736394: step 3960, loss 0.698781, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 366/599 (start=23424, end=23488)\n",
      "2018-11-27T04:38:58.083047: step 3961, loss 0.644318, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 367/599 (start=23488, end=23552)\n",
      "2018-11-27T04:38:58.402386: step 3962, loss 0.856112, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 368/599 (start=23552, end=23616)\n",
      "2018-11-27T04:38:58.736122: step 3963, loss 0.761659, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 369/599 (start=23616, end=23680)\n",
      "2018-11-27T04:38:59.067006: step 3964, loss 0.570572, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 370/599 (start=23680, end=23744)\n",
      "2018-11-27T04:38:59.401866: step 3965, loss 0.566068, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 371/599 (start=23744, end=23808)\n",
      "2018-11-27T04:38:59.744784: step 3966, loss 0.549929, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 372/599 (start=23808, end=23872)\n",
      "2018-11-27T04:39:00.077688: step 3967, loss 0.761478, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 373/599 (start=23872, end=23936)\n",
      "2018-11-27T04:39:00.436630: step 3968, loss 0.66048, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 374/599 (start=23936, end=24000)\n",
      "2018-11-27T04:39:00.776550: step 3969, loss 0.559752, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 375/599 (start=24000, end=24064)\n",
      "2018-11-27T04:39:01.102326: step 3970, loss 0.858145, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 376/599 (start=24064, end=24128)\n",
      "2018-11-27T04:39:01.415209: step 3971, loss 0.609825, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 377/599 (start=24128, end=24192)\n",
      "2018-11-27T04:39:01.751634: step 3972, loss 0.709425, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 378/599 (start=24192, end=24256)\n",
      "2018-11-27T04:39:02.102002: step 3973, loss 0.766996, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 379/599 (start=24256, end=24320)\n",
      "2018-11-27T04:39:02.432084: step 3974, loss 0.564603, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 380/599 (start=24320, end=24384)\n",
      "2018-11-27T04:39:02.734913: step 3975, loss 0.741517, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 381/599 (start=24384, end=24448)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:39:03.077254: step 3976, loss 0.623696, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 382/599 (start=24448, end=24512)\n",
      "2018-11-27T04:39:03.417877: step 3977, loss 0.700052, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 383/599 (start=24512, end=24576)\n",
      "2018-11-27T04:39:03.760361: step 3978, loss 0.888876, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 384/599 (start=24576, end=24640)\n",
      "2018-11-27T04:39:04.078882: step 3979, loss 0.503477, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 385/599 (start=24640, end=24704)\n",
      "2018-11-27T04:39:04.420227: step 3980, loss 0.647867, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 386/599 (start=24704, end=24768)\n",
      "2018-11-27T04:39:04.757068: step 3981, loss 0.546774, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 387/599 (start=24768, end=24832)\n",
      "2018-11-27T04:39:05.091542: step 3982, loss 0.525607, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 388/599 (start=24832, end=24896)\n",
      "2018-11-27T04:39:05.412153: step 3983, loss 0.531925, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 389/599 (start=24896, end=24960)\n",
      "2018-11-27T04:39:05.732720: step 3984, loss 0.577349, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 390/599 (start=24960, end=25024)\n",
      "2018-11-27T04:39:06.046833: step 3985, loss 0.734574, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 391/599 (start=25024, end=25088)\n",
      "2018-11-27T04:39:06.357058: step 3986, loss 0.526855, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 392/599 (start=25088, end=25152)\n",
      "2018-11-27T04:39:06.701964: step 3987, loss 0.70701, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 393/599 (start=25152, end=25216)\n",
      "2018-11-27T04:39:07.055091: step 3988, loss 0.476488, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 394/599 (start=25216, end=25280)\n",
      "2018-11-27T04:39:07.384448: step 3989, loss 0.811024, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 395/599 (start=25280, end=25344)\n",
      "2018-11-27T04:39:07.705444: step 3990, loss 0.667186, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 396/599 (start=25344, end=25408)\n",
      "2018-11-27T04:39:08.047287: step 3991, loss 0.568599, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 397/599 (start=25408, end=25472)\n",
      "2018-11-27T04:39:08.355085: step 3992, loss 0.690658, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 398/599 (start=25472, end=25536)\n",
      "2018-11-27T04:39:08.687182: step 3993, loss 0.540121, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 399/599 (start=25536, end=25600)\n",
      "2018-11-27T04:39:09.007580: step 3994, loss 0.638591, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 400/599 (start=25600, end=25664)\n",
      "2018-11-27T04:39:09.346534: step 3995, loss 0.647743, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 401/599 (start=25664, end=25728)\n",
      "2018-11-27T04:39:09.679455: step 3996, loss 0.708742, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 402/599 (start=25728, end=25792)\n",
      "2018-11-27T04:39:09.996690: step 3997, loss 0.848618, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 403/599 (start=25792, end=25856)\n",
      "2018-11-27T04:39:10.328805: step 3998, loss 0.743984, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 404/599 (start=25856, end=25920)\n",
      "2018-11-27T04:39:10.669113: step 3999, loss 0.788764, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 405/599 (start=25920, end=25984)\n",
      "2018-11-27T04:39:11.001214: step 4000, loss 0.658721, acc 0.796875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:39:17.385589: step 4000, loss 1.8869, acc 0.514066\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-4000\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 406/599 (start=25984, end=26048)\n",
      "2018-11-27T04:39:18.133013: step 4001, loss 0.614346, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 407/599 (start=26048, end=26112)\n",
      "2018-11-27T04:39:18.466969: step 4002, loss 0.714041, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 408/599 (start=26112, end=26176)\n",
      "2018-11-27T04:39:18.824373: step 4003, loss 0.623561, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 409/599 (start=26176, end=26240)\n",
      "2018-11-27T04:39:19.157123: step 4004, loss 0.844418, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 410/599 (start=26240, end=26304)\n",
      "2018-11-27T04:39:19.466512: step 4005, loss 0.801872, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 411/599 (start=26304, end=26368)\n",
      "2018-11-27T04:39:19.808204: step 4006, loss 0.486264, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 412/599 (start=26368, end=26432)\n",
      "2018-11-27T04:39:20.159268: step 4007, loss 0.587848, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 413/599 (start=26432, end=26496)\n",
      "2018-11-27T04:39:20.500716: step 4008, loss 0.50851, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 414/599 (start=26496, end=26560)\n",
      "2018-11-27T04:39:20.815559: step 4009, loss 0.731624, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 415/599 (start=26560, end=26624)\n",
      "2018-11-27T04:39:21.136849: step 4010, loss 0.754731, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 416/599 (start=26624, end=26688)\n",
      "2018-11-27T04:39:21.476768: step 4011, loss 0.792823, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 417/599 (start=26688, end=26752)\n",
      "2018-11-27T04:39:21.801051: step 4012, loss 0.737599, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 418/599 (start=26752, end=26816)\n",
      "2018-11-27T04:39:22.137827: step 4013, loss 0.719853, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 419/599 (start=26816, end=26880)\n",
      "2018-11-27T04:39:22.454700: step 4014, loss 0.799252, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 420/599 (start=26880, end=26944)\n",
      "2018-11-27T04:39:22.792155: step 4015, loss 0.620863, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 421/599 (start=26944, end=27008)\n",
      "2018-11-27T04:39:23.137199: step 4016, loss 0.577652, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 422/599 (start=27008, end=27072)\n",
      "2018-11-27T04:39:23.457243: step 4017, loss 0.709167, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 423/599 (start=27072, end=27136)\n",
      "2018-11-27T04:39:23.789816: step 4018, loss 0.635682, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 424/599 (start=27136, end=27200)\n",
      "2018-11-27T04:39:24.116921: step 4019, loss 0.656224, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 425/599 (start=27200, end=27264)\n",
      "2018-11-27T04:39:24.456565: step 4020, loss 0.557826, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 426/599 (start=27264, end=27328)\n",
      "2018-11-27T04:39:24.796183: step 4021, loss 0.604217, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 427/599 (start=27328, end=27392)\n",
      "2018-11-27T04:39:25.149814: step 4022, loss 0.753738, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 428/599 (start=27392, end=27456)\n",
      "2018-11-27T04:39:25.478157: step 4023, loss 0.683354, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 429/599 (start=27456, end=27520)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:39:25.805477: step 4024, loss 0.577595, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 430/599 (start=27520, end=27584)\n",
      "2018-11-27T04:39:26.153748: step 4025, loss 0.68651, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 431/599 (start=27584, end=27648)\n",
      "2018-11-27T04:39:26.482789: step 4026, loss 0.597944, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 432/599 (start=27648, end=27712)\n",
      "2018-11-27T04:39:26.821825: step 4027, loss 0.709608, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 433/599 (start=27712, end=27776)\n",
      "2018-11-27T04:39:27.159071: step 4028, loss 0.536669, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 434/599 (start=27776, end=27840)\n",
      "2018-11-27T04:39:27.475146: step 4029, loss 0.726572, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 435/599 (start=27840, end=27904)\n",
      "2018-11-27T04:39:27.817565: step 4030, loss 0.678951, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 436/599 (start=27904, end=27968)\n",
      "2018-11-27T04:39:28.162328: step 4031, loss 0.594508, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 437/599 (start=27968, end=28032)\n",
      "2018-11-27T04:39:28.493310: step 4032, loss 0.878319, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 438/599 (start=28032, end=28096)\n",
      "2018-11-27T04:39:28.827673: step 4033, loss 0.529534, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 439/599 (start=28096, end=28160)\n",
      "2018-11-27T04:39:29.162970: step 4034, loss 0.562231, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 440/599 (start=28160, end=28224)\n",
      "2018-11-27T04:39:29.476516: step 4035, loss 0.936983, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 441/599 (start=28224, end=28288)\n",
      "2018-11-27T04:39:29.815502: step 4036, loss 0.752547, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 442/599 (start=28288, end=28352)\n",
      "2018-11-27T04:39:30.128100: step 4037, loss 0.714276, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 443/599 (start=28352, end=28416)\n",
      "2018-11-27T04:39:30.449666: step 4038, loss 0.875169, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 444/599 (start=28416, end=28480)\n",
      "2018-11-27T04:39:30.771422: step 4039, loss 0.491921, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 445/599 (start=28480, end=28544)\n",
      "2018-11-27T04:39:31.115089: step 4040, loss 0.607923, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 446/599 (start=28544, end=28608)\n",
      "2018-11-27T04:39:31.448026: step 4041, loss 0.553423, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 447/599 (start=28608, end=28672)\n",
      "2018-11-27T04:39:31.792460: step 4042, loss 0.643286, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 448/599 (start=28672, end=28736)\n",
      "2018-11-27T04:39:32.133448: step 4043, loss 0.812841, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 449/599 (start=28736, end=28800)\n",
      "2018-11-27T04:39:32.440011: step 4044, loss 0.545044, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 450/599 (start=28800, end=28864)\n",
      "2018-11-27T04:39:32.771581: step 4045, loss 0.893855, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 451/599 (start=28864, end=28928)\n",
      "2018-11-27T04:39:33.105718: step 4046, loss 0.721249, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 452/599 (start=28928, end=28992)\n",
      "2018-11-27T04:39:33.422594: step 4047, loss 0.654809, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 453/599 (start=28992, end=29056)\n",
      "2018-11-27T04:39:33.762358: step 4048, loss 0.678631, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 454/599 (start=29056, end=29120)\n",
      "2018-11-27T04:39:34.114928: step 4049, loss 0.874687, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 455/599 (start=29120, end=29184)\n",
      "2018-11-27T04:39:34.447596: step 4050, loss 0.768107, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 456/599 (start=29184, end=29248)\n",
      "2018-11-27T04:39:34.776124: step 4051, loss 0.457705, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 457/599 (start=29248, end=29312)\n",
      "2018-11-27T04:39:35.092235: step 4052, loss 0.783552, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 458/599 (start=29312, end=29376)\n",
      "2018-11-27T04:39:35.433097: step 4053, loss 0.426121, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 459/599 (start=29376, end=29440)\n",
      "2018-11-27T04:39:35.773256: step 4054, loss 0.872439, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 460/599 (start=29440, end=29504)\n",
      "2018-11-27T04:39:36.083886: step 4055, loss 0.734164, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 461/599 (start=29504, end=29568)\n",
      "2018-11-27T04:39:36.422371: step 4056, loss 0.832143, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 462/599 (start=29568, end=29632)\n",
      "2018-11-27T04:39:36.760496: step 4057, loss 0.777908, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 463/599 (start=29632, end=29696)\n",
      "2018-11-27T04:39:37.085092: step 4058, loss 0.662877, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 464/599 (start=29696, end=29760)\n",
      "2018-11-27T04:39:37.392795: step 4059, loss 0.859274, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 465/599 (start=29760, end=29824)\n",
      "2018-11-27T04:39:37.752022: step 4060, loss 0.482073, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 466/599 (start=29824, end=29888)\n",
      "2018-11-27T04:39:38.105643: step 4061, loss 0.62576, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 467/599 (start=29888, end=29952)\n",
      "2018-11-27T04:39:38.439504: step 4062, loss 0.614103, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 468/599 (start=29952, end=30016)\n",
      "2018-11-27T04:39:38.779535: step 4063, loss 0.836412, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 469/599 (start=30016, end=30080)\n",
      "2018-11-27T04:39:39.124283: step 4064, loss 0.793379, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 470/599 (start=30080, end=30144)\n",
      "2018-11-27T04:39:39.457848: step 4065, loss 0.572471, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 471/599 (start=30144, end=30208)\n",
      "2018-11-27T04:39:39.796054: step 4066, loss 0.6771, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 472/599 (start=30208, end=30272)\n",
      "2018-11-27T04:39:40.140952: step 4067, loss 0.77303, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 473/599 (start=30272, end=30336)\n",
      "2018-11-27T04:39:40.473230: step 4068, loss 0.567128, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 474/599 (start=30336, end=30400)\n",
      "2018-11-27T04:39:40.812187: step 4069, loss 0.742223, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 475/599 (start=30400, end=30464)\n",
      "2018-11-27T04:39:41.120591: step 4070, loss 0.855113, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 476/599 (start=30464, end=30528)\n",
      "2018-11-27T04:39:41.464029: step 4071, loss 0.722725, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 477/599 (start=30528, end=30592)\n",
      "2018-11-27T04:39:41.805504: step 4072, loss 0.613413, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 478/599 (start=30592, end=30656)\n",
      "2018-11-27T04:39:42.148559: step 4073, loss 0.596091, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 479/599 (start=30656, end=30720)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:39:42.486433: step 4074, loss 0.633767, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 480/599 (start=30720, end=30784)\n",
      "2018-11-27T04:39:42.826181: step 4075, loss 0.607854, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 481/599 (start=30784, end=30848)\n",
      "2018-11-27T04:39:43.146539: step 4076, loss 0.895902, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 482/599 (start=30848, end=30912)\n",
      "2018-11-27T04:39:43.486802: step 4077, loss 0.701911, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 483/599 (start=30912, end=30976)\n",
      "2018-11-27T04:39:43.824630: step 4078, loss 0.954548, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 484/599 (start=30976, end=31040)\n",
      "2018-11-27T04:39:44.163481: step 4079, loss 0.70135, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 485/599 (start=31040, end=31104)\n",
      "2018-11-27T04:39:44.496879: step 4080, loss 0.836751, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 486/599 (start=31104, end=31168)\n",
      "2018-11-27T04:39:44.812135: step 4081, loss 0.625409, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 487/599 (start=31168, end=31232)\n",
      "2018-11-27T04:39:45.159080: step 4082, loss 0.783557, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 488/599 (start=31232, end=31296)\n",
      "2018-11-27T04:39:45.490372: step 4083, loss 0.696346, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 489/599 (start=31296, end=31360)\n",
      "2018-11-27T04:39:45.825200: step 4084, loss 0.528385, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 490/599 (start=31360, end=31424)\n",
      "2018-11-27T04:39:46.166011: step 4085, loss 0.543785, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 491/599 (start=31424, end=31488)\n",
      "2018-11-27T04:39:46.501396: step 4086, loss 0.656869, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 492/599 (start=31488, end=31552)\n",
      "2018-11-27T04:39:46.839060: step 4087, loss 0.617796, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 493/599 (start=31552, end=31616)\n",
      "2018-11-27T04:39:47.149795: step 4088, loss 0.675951, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 494/599 (start=31616, end=31680)\n",
      "2018-11-27T04:39:47.491233: step 4089, loss 0.443799, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 495/599 (start=31680, end=31744)\n",
      "2018-11-27T04:39:47.830101: step 4090, loss 0.53038, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 496/599 (start=31744, end=31808)\n",
      "2018-11-27T04:39:48.137814: step 4091, loss 0.74485, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 497/599 (start=31808, end=31872)\n",
      "2018-11-27T04:39:48.452842: step 4092, loss 0.57641, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 498/599 (start=31872, end=31936)\n",
      "2018-11-27T04:39:48.796539: step 4093, loss 0.591325, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 499/599 (start=31936, end=32000)\n",
      "2018-11-27T04:39:49.138505: step 4094, loss 0.684145, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 500/599 (start=32000, end=32064)\n",
      "2018-11-27T04:39:49.471113: step 4095, loss 0.676925, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 501/599 (start=32064, end=32128)\n",
      "2018-11-27T04:39:49.821609: step 4096, loss 0.499784, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 502/599 (start=32128, end=32192)\n",
      "2018-11-27T04:39:50.172090: step 4097, loss 0.600113, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 503/599 (start=32192, end=32256)\n",
      "2018-11-27T04:39:50.509855: step 4098, loss 0.676981, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 504/599 (start=32256, end=32320)\n",
      "2018-11-27T04:39:50.846042: step 4099, loss 0.646587, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 505/599 (start=32320, end=32384)\n",
      "2018-11-27T04:39:51.185057: step 4100, loss 0.568507, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:39:57.374654: step 4100, loss 1.89519, acc 0.517985\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-4100\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 506/599 (start=32384, end=32448)\n",
      "2018-11-27T04:39:58.133368: step 4101, loss 0.879815, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 507/599 (start=32448, end=32512)\n",
      "2018-11-27T04:39:58.474223: step 4102, loss 0.760228, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 508/599 (start=32512, end=32576)\n",
      "2018-11-27T04:39:58.808845: step 4103, loss 0.730377, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 509/599 (start=32576, end=32640)\n",
      "2018-11-27T04:39:59.139323: step 4104, loss 0.740346, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 510/599 (start=32640, end=32704)\n",
      "2018-11-27T04:39:59.455244: step 4105, loss 0.696323, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 511/599 (start=32704, end=32768)\n",
      "2018-11-27T04:39:59.790589: step 4106, loss 0.831893, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 512/599 (start=32768, end=32832)\n",
      "2018-11-27T04:40:00.114216: step 4107, loss 1.09209, acc 0.734375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 513/599 (start=32832, end=32896)\n",
      "2018-11-27T04:40:00.441969: step 4108, loss 0.657628, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 514/599 (start=32896, end=32960)\n",
      "2018-11-27T04:40:00.753682: step 4109, loss 0.621833, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 515/599 (start=32960, end=33024)\n",
      "2018-11-27T04:40:01.102421: step 4110, loss 0.924654, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 516/599 (start=33024, end=33088)\n",
      "2018-11-27T04:40:01.412463: step 4111, loss 0.598114, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 517/599 (start=33088, end=33152)\n",
      "2018-11-27T04:40:01.736072: step 4112, loss 0.540189, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 518/599 (start=33152, end=33216)\n",
      "2018-11-27T04:40:02.060724: step 4113, loss 0.764515, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 519/599 (start=33216, end=33280)\n",
      "2018-11-27T04:40:02.400457: step 4114, loss 0.699709, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 520/599 (start=33280, end=33344)\n",
      "2018-11-27T04:40:02.726946: step 4115, loss 0.460918, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 521/599 (start=33344, end=33408)\n",
      "2018-11-27T04:40:03.064582: step 4116, loss 0.599893, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 522/599 (start=33408, end=33472)\n",
      "2018-11-27T04:40:03.386455: step 4117, loss 0.815334, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 523/599 (start=33472, end=33536)\n",
      "2018-11-27T04:40:03.729518: step 4118, loss 0.505972, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 524/599 (start=33536, end=33600)\n",
      "2018-11-27T04:40:04.059117: step 4119, loss 0.736646, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 525/599 (start=33600, end=33664)\n",
      "2018-11-27T04:40:04.377429: step 4120, loss 0.834249, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 526/599 (start=33664, end=33728)\n",
      "2018-11-27T04:40:04.728371: step 4121, loss 0.595908, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 527/599 (start=33728, end=33792)\n",
      "2018-11-27T04:40:05.062646: step 4122, loss 0.799055, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 528/599 (start=33792, end=33856)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:40:05.416274: step 4123, loss 0.778166, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 529/599 (start=33856, end=33920)\n",
      "2018-11-27T04:40:05.772145: step 4124, loss 0.576863, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 530/599 (start=33920, end=33984)\n",
      "2018-11-27T04:40:06.109617: step 4125, loss 0.736503, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 531/599 (start=33984, end=34048)\n",
      "2018-11-27T04:40:06.425800: step 4126, loss 0.636251, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 532/599 (start=34048, end=34112)\n",
      "2018-11-27T04:40:06.765376: step 4127, loss 0.583551, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 533/599 (start=34112, end=34176)\n",
      "2018-11-27T04:40:07.076938: step 4128, loss 0.617402, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 534/599 (start=34176, end=34240)\n",
      "2018-11-27T04:40:07.395379: step 4129, loss 0.480284, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 535/599 (start=34240, end=34304)\n",
      "2018-11-27T04:40:07.714895: step 4130, loss 0.5848, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 536/599 (start=34304, end=34368)\n",
      "2018-11-27T04:40:08.049830: step 4131, loss 0.804853, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 537/599 (start=34368, end=34432)\n",
      "2018-11-27T04:40:08.396697: step 4132, loss 0.711747, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 538/599 (start=34432, end=34496)\n",
      "2018-11-27T04:40:08.737813: step 4133, loss 0.550249, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 539/599 (start=34496, end=34560)\n",
      "2018-11-27T04:40:09.083605: step 4134, loss 0.683819, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 540/599 (start=34560, end=34624)\n",
      "2018-11-27T04:40:09.429878: step 4135, loss 0.814649, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 541/599 (start=34624, end=34688)\n",
      "2018-11-27T04:40:09.771925: step 4136, loss 0.582107, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 542/599 (start=34688, end=34752)\n",
      "2018-11-27T04:40:10.096950: step 4137, loss 0.754773, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 543/599 (start=34752, end=34816)\n",
      "2018-11-27T04:40:10.442989: step 4138, loss 0.59993, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 544/599 (start=34816, end=34880)\n",
      "2018-11-27T04:40:10.785212: step 4139, loss 0.906708, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 545/599 (start=34880, end=34944)\n",
      "2018-11-27T04:40:11.099495: step 4140, loss 0.669545, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 546/599 (start=34944, end=35008)\n",
      "2018-11-27T04:40:11.452952: step 4141, loss 0.669882, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 547/599 (start=35008, end=35072)\n",
      "2018-11-27T04:40:11.800514: step 4142, loss 0.55635, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 548/599 (start=35072, end=35136)\n",
      "2018-11-27T04:40:12.120231: step 4143, loss 0.731702, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 549/599 (start=35136, end=35200)\n",
      "2018-11-27T04:40:12.450929: step 4144, loss 0.698166, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 550/599 (start=35200, end=35264)\n",
      "2018-11-27T04:40:12.765674: step 4145, loss 0.821788, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 551/599 (start=35264, end=35328)\n",
      "2018-11-27T04:40:13.077101: step 4146, loss 0.661446, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 552/599 (start=35328, end=35392)\n",
      "2018-11-27T04:40:13.412115: step 4147, loss 0.631054, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 553/599 (start=35392, end=35456)\n",
      "2018-11-27T04:40:13.732903: step 4148, loss 0.705844, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 554/599 (start=35456, end=35520)\n",
      "2018-11-27T04:40:14.065236: step 4149, loss 0.618824, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 555/599 (start=35520, end=35584)\n",
      "2018-11-27T04:40:14.413667: step 4150, loss 0.555622, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 556/599 (start=35584, end=35648)\n",
      "2018-11-27T04:40:14.749295: step 4151, loss 0.589433, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 557/599 (start=35648, end=35712)\n",
      "2018-11-27T04:40:15.086976: step 4152, loss 0.917033, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 558/599 (start=35712, end=35776)\n",
      "2018-11-27T04:40:15.432991: step 4153, loss 0.714508, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 559/599 (start=35776, end=35840)\n",
      "2018-11-27T04:40:15.782945: step 4154, loss 0.596157, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 560/599 (start=35840, end=35904)\n",
      "2018-11-27T04:40:16.124214: step 4155, loss 0.629611, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 561/599 (start=35904, end=35968)\n",
      "2018-11-27T04:40:16.431451: step 4156, loss 0.811193, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 562/599 (start=35968, end=36032)\n",
      "2018-11-27T04:40:16.780669: step 4157, loss 0.935044, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 563/599 (start=36032, end=36096)\n",
      "2018-11-27T04:40:17.099252: step 4158, loss 0.730833, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 564/599 (start=36096, end=36160)\n",
      "2018-11-27T04:40:17.451092: step 4159, loss 0.79716, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 565/599 (start=36160, end=36224)\n",
      "2018-11-27T04:40:17.790295: step 4160, loss 0.742347, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 566/599 (start=36224, end=36288)\n",
      "2018-11-27T04:40:18.122499: step 4161, loss 0.927312, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 567/599 (start=36288, end=36352)\n",
      "2018-11-27T04:40:18.461575: step 4162, loss 0.656424, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 568/599 (start=36352, end=36416)\n",
      "2018-11-27T04:40:18.805667: step 4163, loss 0.732325, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 569/599 (start=36416, end=36480)\n",
      "2018-11-27T04:40:19.126016: step 4164, loss 0.579079, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 570/599 (start=36480, end=36544)\n",
      "2018-11-27T04:40:19.465384: step 4165, loss 0.755163, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 571/599 (start=36544, end=36608)\n",
      "2018-11-27T04:40:19.812376: step 4166, loss 0.614529, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 572/599 (start=36608, end=36672)\n",
      "2018-11-27T04:40:20.144596: step 4167, loss 1.08071, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 573/599 (start=36672, end=36736)\n",
      "2018-11-27T04:40:20.475201: step 4168, loss 0.73888, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 574/599 (start=36736, end=36800)\n",
      "2018-11-27T04:40:20.809300: step 4169, loss 0.672034, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 575/599 (start=36800, end=36864)\n",
      "2018-11-27T04:40:21.158134: step 4170, loss 0.632814, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 576/599 (start=36864, end=36928)\n",
      "2018-11-27T04:40:21.488566: step 4171, loss 0.84967, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 577/599 (start=36928, end=36992)\n",
      "2018-11-27T04:40:21.828737: step 4172, loss 0.693248, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 578/599 (start=36992, end=37056)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:40:22.158616: step 4173, loss 0.734199, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 579/599 (start=37056, end=37120)\n",
      "2018-11-27T04:40:22.470158: step 4174, loss 0.576358, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 580/599 (start=37120, end=37184)\n",
      "2018-11-27T04:40:22.816648: step 4175, loss 0.589832, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 581/599 (start=37184, end=37248)\n",
      "2018-11-27T04:40:23.125399: step 4176, loss 0.551075, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 582/599 (start=37248, end=37312)\n",
      "2018-11-27T04:40:23.464415: step 4177, loss 0.642127, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 583/599 (start=37312, end=37376)\n",
      "2018-11-27T04:40:23.808460: step 4178, loss 0.825702, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 584/599 (start=37376, end=37440)\n",
      "2018-11-27T04:40:24.119042: step 4179, loss 0.732205, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 585/599 (start=37440, end=37504)\n",
      "2018-11-27T04:40:24.449546: step 4180, loss 0.717948, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 586/599 (start=37504, end=37568)\n",
      "2018-11-27T04:40:24.771901: step 4181, loss 0.827949, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 587/599 (start=37568, end=37632)\n",
      "2018-11-27T04:40:25.093786: step 4182, loss 0.654301, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 588/599 (start=37632, end=37696)\n",
      "2018-11-27T04:40:25.410160: step 4183, loss 0.718891, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 589/599 (start=37696, end=37760)\n",
      "2018-11-27T04:40:25.729978: step 4184, loss 0.695363, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 590/599 (start=37760, end=37824)\n",
      "2018-11-27T04:40:26.073375: step 4185, loss 0.83378, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 591/599 (start=37824, end=37888)\n",
      "2018-11-27T04:40:26.434150: step 4186, loss 0.682074, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 592/599 (start=37888, end=37952)\n",
      "2018-11-27T04:40:26.782250: step 4187, loss 0.736188, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 593/599 (start=37952, end=38016)\n",
      "2018-11-27T04:40:27.132433: step 4188, loss 0.792095, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 594/599 (start=38016, end=38080)\n",
      "2018-11-27T04:40:27.457340: step 4189, loss 0.80561, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 595/599 (start=38080, end=38144)\n",
      "2018-11-27T04:40:27.773956: step 4190, loss 0.936511, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 596/599 (start=38144, end=38208)\n",
      "2018-11-27T04:40:28.105399: step 4191, loss 0.832869, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 597/599 (start=38208, end=38272)\n",
      "2018-11-27T04:40:28.464250: step 4192, loss 0.716692, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 6/10, Batch 598/599 (start=38272, end=38281)\n",
      "2018-11-27T04:40:28.706163: step 4193, loss 0.357821, acc 1\n",
      "***********************************************\n",
      "Epoch 7/10\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 0/599 (start=0, end=64)\n",
      "2018-11-27T04:40:29.059466: step 4194, loss 0.621607, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 1/599 (start=64, end=128)\n",
      "2018-11-27T04:40:29.413344: step 4195, loss 0.699374, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 2/599 (start=128, end=192)\n",
      "2018-11-27T04:40:29.756534: step 4196, loss 0.56993, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 3/599 (start=192, end=256)\n",
      "2018-11-27T04:40:30.082767: step 4197, loss 0.313758, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 4/599 (start=256, end=320)\n",
      "2018-11-27T04:40:30.420925: step 4198, loss 0.623241, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 5/599 (start=320, end=384)\n",
      "2018-11-27T04:40:30.748579: step 4199, loss 0.509647, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 6/599 (start=384, end=448)\n",
      "2018-11-27T04:40:31.098552: step 4200, loss 0.606524, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:40:37.448706: step 4200, loss 1.91702, acc 0.516731\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-4200\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 7/599 (start=448, end=512)\n",
      "2018-11-27T04:40:38.203839: step 4201, loss 0.720149, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 8/599 (start=512, end=576)\n",
      "2018-11-27T04:40:38.545032: step 4202, loss 0.544553, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 9/599 (start=576, end=640)\n",
      "2018-11-27T04:40:38.862464: step 4203, loss 0.693808, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 10/599 (start=640, end=704)\n",
      "2018-11-27T04:40:39.199466: step 4204, loss 0.527693, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 11/599 (start=704, end=768)\n",
      "2018-11-27T04:40:39.538341: step 4205, loss 0.651326, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 12/599 (start=768, end=832)\n",
      "2018-11-27T04:40:39.856275: step 4206, loss 0.626351, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 13/599 (start=832, end=896)\n",
      "2018-11-27T04:40:40.207492: step 4207, loss 0.49295, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 14/599 (start=896, end=960)\n",
      "2018-11-27T04:40:40.543780: step 4208, loss 0.663705, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 15/599 (start=960, end=1024)\n",
      "2018-11-27T04:40:40.862641: step 4209, loss 0.687447, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 16/599 (start=1024, end=1088)\n",
      "2018-11-27T04:40:41.200331: step 4210, loss 0.560793, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 17/599 (start=1088, end=1152)\n",
      "2018-11-27T04:40:41.548690: step 4211, loss 0.712994, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 18/599 (start=1152, end=1216)\n",
      "2018-11-27T04:40:41.892671: step 4212, loss 0.615441, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 19/599 (start=1216, end=1280)\n",
      "2018-11-27T04:40:42.204743: step 4213, loss 0.609275, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 20/599 (start=1280, end=1344)\n",
      "2018-11-27T04:40:42.536196: step 4214, loss 0.430889, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 21/599 (start=1344, end=1408)\n",
      "2018-11-27T04:40:42.882588: step 4215, loss 0.48121, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 22/599 (start=1408, end=1472)\n",
      "2018-11-27T04:40:43.202615: step 4216, loss 0.615954, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 23/599 (start=1472, end=1536)\n",
      "2018-11-27T04:40:43.530638: step 4217, loss 0.513683, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 24/599 (start=1536, end=1600)\n",
      "2018-11-27T04:40:43.857771: step 4218, loss 0.65177, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 25/599 (start=1600, end=1664)\n",
      "2018-11-27T04:40:44.193245: step 4219, loss 0.432511, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 26/599 (start=1664, end=1728)\n",
      "2018-11-27T04:40:44.521766: step 4220, loss 0.633764, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 27/599 (start=1728, end=1792)\n",
      "2018-11-27T04:40:44.842122: step 4221, loss 0.462715, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 28/599 (start=1792, end=1856)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:40:45.155272: step 4222, loss 0.442221, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 29/599 (start=1856, end=1920)\n",
      "2018-11-27T04:40:45.494299: step 4223, loss 0.429734, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 30/599 (start=1920, end=1984)\n",
      "2018-11-27T04:40:45.829497: step 4224, loss 0.589512, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 31/599 (start=1984, end=2048)\n",
      "2018-11-27T04:40:46.131042: step 4225, loss 0.572959, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 32/599 (start=2048, end=2112)\n",
      "2018-11-27T04:40:46.455244: step 4226, loss 0.485222, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 33/599 (start=2112, end=2176)\n",
      "2018-11-27T04:40:46.778036: step 4227, loss 0.422744, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 34/599 (start=2176, end=2240)\n",
      "2018-11-27T04:40:47.105885: step 4228, loss 0.546656, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 35/599 (start=2240, end=2304)\n",
      "2018-11-27T04:40:47.436022: step 4229, loss 0.626685, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 36/599 (start=2304, end=2368)\n",
      "2018-11-27T04:40:47.767891: step 4230, loss 0.548273, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 37/599 (start=2368, end=2432)\n",
      "2018-11-27T04:40:48.104506: step 4231, loss 0.462244, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 38/599 (start=2432, end=2496)\n",
      "2018-11-27T04:40:48.412523: step 4232, loss 0.80092, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 39/599 (start=2496, end=2560)\n",
      "2018-11-27T04:40:48.758033: step 4233, loss 0.406441, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 40/599 (start=2560, end=2624)\n",
      "2018-11-27T04:40:49.085182: step 4234, loss 0.849678, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 41/599 (start=2624, end=2688)\n",
      "2018-11-27T04:40:49.424242: step 4235, loss 0.392721, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 42/599 (start=2688, end=2752)\n",
      "2018-11-27T04:40:49.756746: step 4236, loss 0.623482, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 43/599 (start=2752, end=2816)\n",
      "2018-11-27T04:40:50.107555: step 4237, loss 0.61325, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 44/599 (start=2816, end=2880)\n",
      "2018-11-27T04:40:50.452215: step 4238, loss 0.355203, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 45/599 (start=2880, end=2944)\n",
      "2018-11-27T04:40:50.789773: step 4239, loss 0.614673, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 46/599 (start=2944, end=3008)\n",
      "2018-11-27T04:40:51.116996: step 4240, loss 0.455611, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 47/599 (start=3008, end=3072)\n",
      "2018-11-27T04:40:51.470169: step 4241, loss 0.535447, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 48/599 (start=3072, end=3136)\n",
      "2018-11-27T04:40:51.815596: step 4242, loss 0.64529, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 49/599 (start=3136, end=3200)\n",
      "2018-11-27T04:40:52.136344: step 4243, loss 0.685798, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 50/599 (start=3200, end=3264)\n",
      "2018-11-27T04:40:52.477740: step 4244, loss 0.447397, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 51/599 (start=3264, end=3328)\n",
      "2018-11-27T04:40:52.799854: step 4245, loss 0.513375, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 52/599 (start=3328, end=3392)\n",
      "2018-11-27T04:40:53.130894: step 4246, loss 0.59783, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 53/599 (start=3392, end=3456)\n",
      "2018-11-27T04:40:53.471706: step 4247, loss 0.542356, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 54/599 (start=3456, end=3520)\n",
      "2018-11-27T04:40:53.809103: step 4248, loss 0.593453, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 55/599 (start=3520, end=3584)\n",
      "2018-11-27T04:40:54.132179: step 4249, loss 0.586533, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 56/599 (start=3584, end=3648)\n",
      "2018-11-27T04:40:54.464360: step 4250, loss 0.422361, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 57/599 (start=3648, end=3712)\n",
      "2018-11-27T04:40:54.770702: step 4251, loss 0.550372, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 58/599 (start=3712, end=3776)\n",
      "2018-11-27T04:40:55.107834: step 4252, loss 0.549627, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 59/599 (start=3776, end=3840)\n",
      "2018-11-27T04:40:55.425846: step 4253, loss 0.670901, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 60/599 (start=3840, end=3904)\n",
      "2018-11-27T04:40:55.759565: step 4254, loss 0.483182, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 61/599 (start=3904, end=3968)\n",
      "2018-11-27T04:40:56.079178: step 4255, loss 0.656382, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 62/599 (start=3968, end=4032)\n",
      "2018-11-27T04:40:56.399063: step 4256, loss 0.542576, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 63/599 (start=4032, end=4096)\n",
      "2018-11-27T04:40:56.732520: step 4257, loss 0.696807, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 64/599 (start=4096, end=4160)\n",
      "2018-11-27T04:40:57.083107: step 4258, loss 0.55446, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 65/599 (start=4160, end=4224)\n",
      "2018-11-27T04:40:57.425679: step 4259, loss 0.453717, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 66/599 (start=4224, end=4288)\n",
      "2018-11-27T04:40:57.738768: step 4260, loss 0.528246, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 67/599 (start=4288, end=4352)\n",
      "2018-11-27T04:40:58.057153: step 4261, loss 0.458186, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 68/599 (start=4352, end=4416)\n",
      "2018-11-27T04:40:58.380094: step 4262, loss 0.799851, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 69/599 (start=4416, end=4480)\n",
      "2018-11-27T04:40:58.729901: step 4263, loss 0.478735, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 70/599 (start=4480, end=4544)\n",
      "2018-11-27T04:40:59.063876: step 4264, loss 0.626815, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 71/599 (start=4544, end=4608)\n",
      "2018-11-27T04:40:59.381000: step 4265, loss 0.450716, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 72/599 (start=4608, end=4672)\n",
      "2018-11-27T04:40:59.702625: step 4266, loss 0.583884, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 73/599 (start=4672, end=4736)\n",
      "2018-11-27T04:41:00.035224: step 4267, loss 0.548994, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 74/599 (start=4736, end=4800)\n",
      "2018-11-27T04:41:00.376379: step 4268, loss 0.667275, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 75/599 (start=4800, end=4864)\n",
      "2018-11-27T04:41:00.711338: step 4269, loss 0.566092, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 76/599 (start=4864, end=4928)\n",
      "2018-11-27T04:41:01.066765: step 4270, loss 0.544124, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 77/599 (start=4928, end=4992)\n",
      "2018-11-27T04:41:01.411559: step 4271, loss 0.514959, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 78/599 (start=4992, end=5056)\n",
      "2018-11-27T04:41:01.731617: step 4272, loss 0.628099, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 79/599 (start=5056, end=5120)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:41:02.072673: step 4273, loss 0.709288, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 80/599 (start=5120, end=5184)\n",
      "2018-11-27T04:41:02.405272: step 4274, loss 0.671645, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 81/599 (start=5184, end=5248)\n",
      "2018-11-27T04:41:02.724438: step 4275, loss 0.494376, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 82/599 (start=5248, end=5312)\n",
      "2018-11-27T04:41:03.060744: step 4276, loss 0.396945, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 83/599 (start=5312, end=5376)\n",
      "2018-11-27T04:41:03.392364: step 4277, loss 0.465366, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 84/599 (start=5376, end=5440)\n",
      "2018-11-27T04:41:03.731865: step 4278, loss 0.535206, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 85/599 (start=5440, end=5504)\n",
      "2018-11-27T04:41:04.052972: step 4279, loss 0.47942, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 86/599 (start=5504, end=5568)\n",
      "2018-11-27T04:41:04.376556: step 4280, loss 0.592028, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 87/599 (start=5568, end=5632)\n",
      "2018-11-27T04:41:04.715885: step 4281, loss 0.412713, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 88/599 (start=5632, end=5696)\n",
      "2018-11-27T04:41:05.043062: step 4282, loss 0.448025, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 89/599 (start=5696, end=5760)\n",
      "2018-11-27T04:41:05.374373: step 4283, loss 0.487964, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 90/599 (start=5760, end=5824)\n",
      "2018-11-27T04:41:05.700126: step 4284, loss 0.493912, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 91/599 (start=5824, end=5888)\n",
      "2018-11-27T04:41:06.030816: step 4285, loss 0.532757, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 92/599 (start=5888, end=5952)\n",
      "2018-11-27T04:41:06.361660: step 4286, loss 0.521179, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 93/599 (start=5952, end=6016)\n",
      "2018-11-27T04:41:06.679476: step 4287, loss 0.442986, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 94/599 (start=6016, end=6080)\n",
      "2018-11-27T04:41:07.010976: step 4288, loss 0.53273, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 95/599 (start=6080, end=6144)\n",
      "2018-11-27T04:41:07.349513: step 4289, loss 0.453786, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 96/599 (start=6144, end=6208)\n",
      "2018-11-27T04:41:07.666476: step 4290, loss 0.592037, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 97/599 (start=6208, end=6272)\n",
      "2018-11-27T04:41:07.984244: step 4291, loss 0.73675, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 98/599 (start=6272, end=6336)\n",
      "2018-11-27T04:41:08.330601: step 4292, loss 0.54387, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 99/599 (start=6336, end=6400)\n",
      "2018-11-27T04:41:08.645515: step 4293, loss 0.520113, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 100/599 (start=6400, end=6464)\n",
      "2018-11-27T04:41:08.958756: step 4294, loss 0.48163, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 101/599 (start=6464, end=6528)\n",
      "2018-11-27T04:41:09.305188: step 4295, loss 0.800287, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 102/599 (start=6528, end=6592)\n",
      "2018-11-27T04:41:09.662228: step 4296, loss 0.573967, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 103/599 (start=6592, end=6656)\n",
      "2018-11-27T04:41:09.994866: step 4297, loss 0.440566, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 104/599 (start=6656, end=6720)\n",
      "2018-11-27T04:41:10.336787: step 4298, loss 0.774873, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 105/599 (start=6720, end=6784)\n",
      "2018-11-27T04:41:10.680218: step 4299, loss 0.492247, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 106/599 (start=6784, end=6848)\n",
      "2018-11-27T04:41:11.018483: step 4300, loss 0.571543, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:41:17.389646: step 4300, loss 1.89966, acc 0.507954\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-4300\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 107/599 (start=6848, end=6912)\n",
      "2018-11-27T04:41:18.146888: step 4301, loss 0.727966, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 108/599 (start=6912, end=6976)\n",
      "2018-11-27T04:41:18.461390: step 4302, loss 0.602592, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 109/599 (start=6976, end=7040)\n",
      "2018-11-27T04:41:18.799911: step 4303, loss 0.587981, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 110/599 (start=7040, end=7104)\n",
      "2018-11-27T04:41:19.118999: step 4304, loss 0.589782, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 111/599 (start=7104, end=7168)\n",
      "2018-11-27T04:41:19.432060: step 4305, loss 0.702064, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 112/599 (start=7168, end=7232)\n",
      "2018-11-27T04:41:19.788951: step 4306, loss 0.420378, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 113/599 (start=7232, end=7296)\n",
      "2018-11-27T04:41:20.136364: step 4307, loss 0.475642, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 114/599 (start=7296, end=7360)\n",
      "2018-11-27T04:41:20.462164: step 4308, loss 0.587518, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 115/599 (start=7360, end=7424)\n",
      "2018-11-27T04:41:20.792344: step 4309, loss 0.636859, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 116/599 (start=7424, end=7488)\n",
      "2018-11-27T04:41:21.108739: step 4310, loss 0.702367, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 117/599 (start=7488, end=7552)\n",
      "2018-11-27T04:41:21.423006: step 4311, loss 0.581314, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 118/599 (start=7552, end=7616)\n",
      "2018-11-27T04:41:21.765374: step 4312, loss 0.467485, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 119/599 (start=7616, end=7680)\n",
      "2018-11-27T04:41:22.097430: step 4313, loss 0.58573, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 120/599 (start=7680, end=7744)\n",
      "2018-11-27T04:41:22.431400: step 4314, loss 0.452417, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 121/599 (start=7744, end=7808)\n",
      "2018-11-27T04:41:22.770765: step 4315, loss 0.54674, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 122/599 (start=7808, end=7872)\n",
      "2018-11-27T04:41:23.111532: step 4316, loss 0.633639, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 123/599 (start=7872, end=7936)\n",
      "2018-11-27T04:41:23.455249: step 4317, loss 0.502452, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 124/599 (start=7936, end=8000)\n",
      "2018-11-27T04:41:23.767404: step 4318, loss 0.634283, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 125/599 (start=8000, end=8064)\n",
      "2018-11-27T04:41:24.112522: step 4319, loss 0.840668, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 126/599 (start=8064, end=8128)\n",
      "2018-11-27T04:41:24.454552: step 4320, loss 0.638667, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 127/599 (start=8128, end=8192)\n",
      "2018-11-27T04:41:24.798698: step 4321, loss 0.723592, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 128/599 (start=8192, end=8256)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:41:25.143922: step 4322, loss 0.502849, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 129/599 (start=8256, end=8320)\n",
      "2018-11-27T04:41:25.482602: step 4323, loss 0.55639, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 130/599 (start=8320, end=8384)\n",
      "2018-11-27T04:41:25.812746: step 4324, loss 0.811029, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 131/599 (start=8384, end=8448)\n",
      "2018-11-27T04:41:26.143729: step 4325, loss 0.64561, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 132/599 (start=8448, end=8512)\n",
      "2018-11-27T04:41:26.485624: step 4326, loss 0.43153, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 133/599 (start=8512, end=8576)\n",
      "2018-11-27T04:41:26.815859: step 4327, loss 0.635482, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 134/599 (start=8576, end=8640)\n",
      "2018-11-27T04:41:27.153977: step 4328, loss 0.487873, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 135/599 (start=8640, end=8704)\n",
      "2018-11-27T04:41:27.488641: step 4329, loss 0.699293, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 136/599 (start=8704, end=8768)\n",
      "2018-11-27T04:41:27.805774: step 4330, loss 0.676152, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 137/599 (start=8768, end=8832)\n",
      "2018-11-27T04:41:28.135833: step 4331, loss 0.481614, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 138/599 (start=8832, end=8896)\n",
      "2018-11-27T04:41:28.477619: step 4332, loss 0.610945, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 139/599 (start=8896, end=8960)\n",
      "2018-11-27T04:41:28.797777: step 4333, loss 0.593003, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 140/599 (start=8960, end=9024)\n",
      "2018-11-27T04:41:29.134252: step 4334, loss 0.585332, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 141/599 (start=9024, end=9088)\n",
      "2018-11-27T04:41:29.478051: step 4335, loss 0.4067, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 142/599 (start=9088, end=9152)\n",
      "2018-11-27T04:41:29.816414: step 4336, loss 0.539794, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 143/599 (start=9152, end=9216)\n",
      "2018-11-27T04:41:30.141119: step 4337, loss 0.425221, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 144/599 (start=9216, end=9280)\n",
      "2018-11-27T04:41:30.479913: step 4338, loss 0.833044, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 145/599 (start=9280, end=9344)\n",
      "2018-11-27T04:41:30.797982: step 4339, loss 0.646622, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 146/599 (start=9344, end=9408)\n",
      "2018-11-27T04:41:31.153144: step 4340, loss 0.717292, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 147/599 (start=9408, end=9472)\n",
      "2018-11-27T04:41:31.488336: step 4341, loss 0.50326, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 148/599 (start=9472, end=9536)\n",
      "2018-11-27T04:41:31.814993: step 4342, loss 0.595574, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 149/599 (start=9536, end=9600)\n",
      "2018-11-27T04:41:32.138253: step 4343, loss 0.682841, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 150/599 (start=9600, end=9664)\n",
      "2018-11-27T04:41:32.485112: step 4344, loss 0.449854, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 151/599 (start=9664, end=9728)\n",
      "2018-11-27T04:41:32.805048: step 4345, loss 0.46704, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 152/599 (start=9728, end=9792)\n",
      "2018-11-27T04:41:33.142797: step 4346, loss 0.566347, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 153/599 (start=9792, end=9856)\n",
      "2018-11-27T04:41:33.459490: step 4347, loss 0.568227, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 154/599 (start=9856, end=9920)\n",
      "2018-11-27T04:41:33.799018: step 4348, loss 0.603809, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 155/599 (start=9920, end=9984)\n",
      "2018-11-27T04:41:34.112344: step 4349, loss 0.703902, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 156/599 (start=9984, end=10048)\n",
      "2018-11-27T04:41:34.453505: step 4350, loss 0.476822, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 157/599 (start=10048, end=10112)\n",
      "2018-11-27T04:41:34.801984: step 4351, loss 0.624097, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 158/599 (start=10112, end=10176)\n",
      "2018-11-27T04:41:35.143230: step 4352, loss 0.514851, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 159/599 (start=10176, end=10240)\n",
      "2018-11-27T04:41:35.493924: step 4353, loss 0.603512, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 160/599 (start=10240, end=10304)\n",
      "2018-11-27T04:41:35.816230: step 4354, loss 0.621898, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 161/599 (start=10304, end=10368)\n",
      "2018-11-27T04:41:36.137294: step 4355, loss 0.377466, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 162/599 (start=10368, end=10432)\n",
      "2018-11-27T04:41:36.453729: step 4356, loss 0.667439, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 163/599 (start=10432, end=10496)\n",
      "2018-11-27T04:41:36.780353: step 4357, loss 0.68659, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 164/599 (start=10496, end=10560)\n",
      "2018-11-27T04:41:37.096718: step 4358, loss 0.539324, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 165/599 (start=10560, end=10624)\n",
      "2018-11-27T04:41:37.405493: step 4359, loss 0.659793, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 166/599 (start=10624, end=10688)\n",
      "2018-11-27T04:41:37.713441: step 4360, loss 0.553306, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 167/599 (start=10688, end=10752)\n",
      "2018-11-27T04:41:38.056488: step 4361, loss 0.609769, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 168/599 (start=10752, end=10816)\n",
      "2018-11-27T04:41:38.397565: step 4362, loss 0.48222, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 169/599 (start=10816, end=10880)\n",
      "2018-11-27T04:41:38.749120: step 4363, loss 0.567999, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 170/599 (start=10880, end=10944)\n",
      "2018-11-27T04:41:39.083932: step 4364, loss 0.571501, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 171/599 (start=10944, end=11008)\n",
      "2018-11-27T04:41:39.434268: step 4365, loss 0.63281, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 172/599 (start=11008, end=11072)\n",
      "2018-11-27T04:41:39.778775: step 4366, loss 0.691281, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 173/599 (start=11072, end=11136)\n",
      "2018-11-27T04:41:40.102961: step 4367, loss 0.498189, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 174/599 (start=11136, end=11200)\n",
      "2018-11-27T04:41:40.447309: step 4368, loss 0.58957, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 175/599 (start=11200, end=11264)\n",
      "2018-11-27T04:41:40.794765: step 4369, loss 0.650685, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 176/599 (start=11264, end=11328)\n",
      "2018-11-27T04:41:41.125324: step 4370, loss 0.566196, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 177/599 (start=11328, end=11392)\n",
      "2018-11-27T04:41:41.462783: step 4371, loss 0.684225, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 178/599 (start=11392, end=11456)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:41:41.805499: step 4372, loss 0.709879, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 179/599 (start=11456, end=11520)\n",
      "2018-11-27T04:41:42.141669: step 4373, loss 0.776527, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 180/599 (start=11520, end=11584)\n",
      "2018-11-27T04:41:42.479178: step 4374, loss 0.692919, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 181/599 (start=11584, end=11648)\n",
      "2018-11-27T04:41:42.830677: step 4375, loss 0.561003, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 182/599 (start=11648, end=11712)\n",
      "2018-11-27T04:41:43.159222: step 4376, loss 0.506325, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 183/599 (start=11712, end=11776)\n",
      "2018-11-27T04:41:43.470790: step 4377, loss 0.672221, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 184/599 (start=11776, end=11840)\n",
      "2018-11-27T04:41:43.810729: step 4378, loss 0.54892, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 185/599 (start=11840, end=11904)\n",
      "2018-11-27T04:41:44.135198: step 4379, loss 0.664794, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 186/599 (start=11904, end=11968)\n",
      "2018-11-27T04:41:44.456138: step 4380, loss 0.478003, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 187/599 (start=11968, end=12032)\n",
      "2018-11-27T04:41:44.810978: step 4381, loss 0.595067, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 188/599 (start=12032, end=12096)\n",
      "2018-11-27T04:41:45.146590: step 4382, loss 0.544872, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 189/599 (start=12096, end=12160)\n",
      "2018-11-27T04:41:45.487449: step 4383, loss 0.557745, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 190/599 (start=12160, end=12224)\n",
      "2018-11-27T04:41:45.820059: step 4384, loss 0.552101, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 191/599 (start=12224, end=12288)\n",
      "2018-11-27T04:41:46.159240: step 4385, loss 0.722764, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 192/599 (start=12288, end=12352)\n",
      "2018-11-27T04:41:46.491828: step 4386, loss 0.599293, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 193/599 (start=12352, end=12416)\n",
      "2018-11-27T04:41:46.813736: step 4387, loss 0.697988, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 194/599 (start=12416, end=12480)\n",
      "2018-11-27T04:41:47.149263: step 4388, loss 0.601608, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 195/599 (start=12480, end=12544)\n",
      "2018-11-27T04:41:47.454080: step 4389, loss 0.375441, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 196/599 (start=12544, end=12608)\n",
      "2018-11-27T04:41:47.795418: step 4390, loss 0.657729, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 197/599 (start=12608, end=12672)\n",
      "2018-11-27T04:41:48.131946: step 4391, loss 0.452197, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 198/599 (start=12672, end=12736)\n",
      "2018-11-27T04:41:48.487354: step 4392, loss 0.503403, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 199/599 (start=12736, end=12800)\n",
      "2018-11-27T04:41:48.824790: step 4393, loss 0.497057, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 200/599 (start=12800, end=12864)\n",
      "2018-11-27T04:41:49.173196: step 4394, loss 0.414945, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 201/599 (start=12864, end=12928)\n",
      "2018-11-27T04:41:49.524625: step 4395, loss 0.500922, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 202/599 (start=12928, end=12992)\n",
      "2018-11-27T04:41:49.875203: step 4396, loss 0.482718, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 203/599 (start=12992, end=13056)\n",
      "2018-11-27T04:41:50.216401: step 4397, loss 0.697862, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 204/599 (start=13056, end=13120)\n",
      "2018-11-27T04:41:50.561401: step 4398, loss 0.493834, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 205/599 (start=13120, end=13184)\n",
      "2018-11-27T04:41:50.881766: step 4399, loss 0.670464, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 206/599 (start=13184, end=13248)\n",
      "2018-11-27T04:41:51.233317: step 4400, loss 0.808379, acc 0.859375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:41:57.677025: step 4400, loss 1.92778, acc 0.510462\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-4400\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 207/599 (start=13248, end=13312)\n",
      "2018-11-27T04:41:58.439790: step 4401, loss 0.440486, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 208/599 (start=13312, end=13376)\n",
      "2018-11-27T04:41:58.793518: step 4402, loss 0.665599, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 209/599 (start=13376, end=13440)\n",
      "2018-11-27T04:41:59.133953: step 4403, loss 0.757818, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 210/599 (start=13440, end=13504)\n",
      "2018-11-27T04:41:59.462401: step 4404, loss 0.584112, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 211/599 (start=13504, end=13568)\n",
      "2018-11-27T04:41:59.779307: step 4405, loss 0.586268, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 212/599 (start=13568, end=13632)\n",
      "2018-11-27T04:42:00.119841: step 4406, loss 0.606831, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 213/599 (start=13632, end=13696)\n",
      "2018-11-27T04:42:00.429769: step 4407, loss 0.664179, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 214/599 (start=13696, end=13760)\n",
      "2018-11-27T04:42:00.765057: step 4408, loss 0.782572, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 215/599 (start=13760, end=13824)\n",
      "2018-11-27T04:42:01.081242: step 4409, loss 0.552656, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 216/599 (start=13824, end=13888)\n",
      "2018-11-27T04:42:01.427236: step 4410, loss 0.371216, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 217/599 (start=13888, end=13952)\n",
      "2018-11-27T04:42:01.743157: step 4411, loss 0.586414, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 218/599 (start=13952, end=14016)\n",
      "2018-11-27T04:42:02.067711: step 4412, loss 0.640086, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 219/599 (start=14016, end=14080)\n",
      "2018-11-27T04:42:02.399597: step 4413, loss 0.417903, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 220/599 (start=14080, end=14144)\n",
      "2018-11-27T04:42:02.732207: step 4414, loss 0.603937, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 221/599 (start=14144, end=14208)\n",
      "2018-11-27T04:42:03.076854: step 4415, loss 0.535109, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 222/599 (start=14208, end=14272)\n",
      "2018-11-27T04:42:03.415670: step 4416, loss 0.498986, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 223/599 (start=14272, end=14336)\n",
      "2018-11-27T04:42:03.757920: step 4417, loss 0.571461, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 224/599 (start=14336, end=14400)\n",
      "2018-11-27T04:42:04.092934: step 4418, loss 0.528333, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 225/599 (start=14400, end=14464)\n",
      "2018-11-27T04:42:04.427753: step 4419, loss 0.357548, acc 0.984375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 226/599 (start=14464, end=14528)\n",
      "2018-11-27T04:42:04.747521: step 4420, loss 0.779058, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 227/599 (start=14528, end=14592)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:42:05.085108: step 4421, loss 0.751916, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 228/599 (start=14592, end=14656)\n",
      "2018-11-27T04:42:05.426924: step 4422, loss 0.616303, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 229/599 (start=14656, end=14720)\n",
      "2018-11-27T04:42:05.764147: step 4423, loss 0.542911, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 230/599 (start=14720, end=14784)\n",
      "2018-11-27T04:42:06.086520: step 4424, loss 0.502731, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 231/599 (start=14784, end=14848)\n",
      "2018-11-27T04:42:06.413969: step 4425, loss 0.591011, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 232/599 (start=14848, end=14912)\n",
      "2018-11-27T04:42:06.769836: step 4426, loss 0.650616, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 233/599 (start=14912, end=14976)\n",
      "2018-11-27T04:42:07.109340: step 4427, loss 0.62858, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 234/599 (start=14976, end=15040)\n",
      "2018-11-27T04:42:07.424850: step 4428, loss 0.619151, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 235/599 (start=15040, end=15104)\n",
      "2018-11-27T04:42:07.756150: step 4429, loss 0.489935, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 236/599 (start=15104, end=15168)\n",
      "2018-11-27T04:42:08.079498: step 4430, loss 0.515327, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 237/599 (start=15168, end=15232)\n",
      "2018-11-27T04:42:08.409709: step 4431, loss 0.892372, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 238/599 (start=15232, end=15296)\n",
      "2018-11-27T04:42:08.744474: step 4432, loss 0.53609, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 239/599 (start=15296, end=15360)\n",
      "2018-11-27T04:42:09.076578: step 4433, loss 0.614115, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 240/599 (start=15360, end=15424)\n",
      "2018-11-27T04:42:09.409043: step 4434, loss 0.620991, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 241/599 (start=15424, end=15488)\n",
      "2018-11-27T04:42:09.726637: step 4435, loss 0.609801, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 242/599 (start=15488, end=15552)\n",
      "2018-11-27T04:42:10.058872: step 4436, loss 0.736043, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 243/599 (start=15552, end=15616)\n",
      "2018-11-27T04:42:10.394565: step 4437, loss 0.493359, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 244/599 (start=15616, end=15680)\n",
      "2018-11-27T04:42:10.708720: step 4438, loss 0.729244, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 245/599 (start=15680, end=15744)\n",
      "2018-11-27T04:42:11.053380: step 4439, loss 0.379781, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 246/599 (start=15744, end=15808)\n",
      "2018-11-27T04:42:11.385629: step 4440, loss 0.507755, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 247/599 (start=15808, end=15872)\n",
      "2018-11-27T04:42:11.718627: step 4441, loss 0.68093, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 248/599 (start=15872, end=15936)\n",
      "2018-11-27T04:42:12.036364: step 4442, loss 0.726025, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 249/599 (start=15936, end=16000)\n",
      "2018-11-27T04:42:12.352474: step 4443, loss 0.699001, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 250/599 (start=16000, end=16064)\n",
      "2018-11-27T04:42:12.659754: step 4444, loss 0.527156, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 251/599 (start=16064, end=16128)\n",
      "2018-11-27T04:42:12.978202: step 4445, loss 0.64934, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 252/599 (start=16128, end=16192)\n",
      "2018-11-27T04:42:13.300079: step 4446, loss 0.508606, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 253/599 (start=16192, end=16256)\n",
      "2018-11-27T04:42:13.612278: step 4447, loss 0.566881, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 254/599 (start=16256, end=16320)\n",
      "2018-11-27T04:42:13.953318: step 4448, loss 0.69997, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 255/599 (start=16320, end=16384)\n",
      "2018-11-27T04:42:14.279708: step 4449, loss 0.620935, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 256/599 (start=16384, end=16448)\n",
      "2018-11-27T04:42:14.619554: step 4450, loss 0.779452, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 257/599 (start=16448, end=16512)\n",
      "2018-11-27T04:42:14.947311: step 4451, loss 0.536512, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 258/599 (start=16512, end=16576)\n",
      "2018-11-27T04:42:15.290521: step 4452, loss 0.711325, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 259/599 (start=16576, end=16640)\n",
      "2018-11-27T04:42:15.626471: step 4453, loss 0.623356, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 260/599 (start=16640, end=16704)\n",
      "2018-11-27T04:42:15.940416: step 4454, loss 0.611486, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 261/599 (start=16704, end=16768)\n",
      "2018-11-27T04:42:16.265948: step 4455, loss 0.690776, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 262/599 (start=16768, end=16832)\n",
      "2018-11-27T04:42:16.585740: step 4456, loss 0.660429, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 263/599 (start=16832, end=16896)\n",
      "2018-11-27T04:42:16.915138: step 4457, loss 0.673582, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 264/599 (start=16896, end=16960)\n",
      "2018-11-27T04:42:17.216660: step 4458, loss 0.486342, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 265/599 (start=16960, end=17024)\n",
      "2018-11-27T04:42:17.534788: step 4459, loss 0.608495, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 266/599 (start=17024, end=17088)\n",
      "2018-11-27T04:42:17.864618: step 4460, loss 0.897811, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 267/599 (start=17088, end=17152)\n",
      "2018-11-27T04:42:18.212573: step 4461, loss 0.549554, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 268/599 (start=17152, end=17216)\n",
      "2018-11-27T04:42:18.528213: step 4462, loss 0.566616, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 269/599 (start=17216, end=17280)\n",
      "2018-11-27T04:42:18.852060: step 4463, loss 0.638645, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 270/599 (start=17280, end=17344)\n",
      "2018-11-27T04:42:19.183746: step 4464, loss 0.668559, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 271/599 (start=17344, end=17408)\n",
      "2018-11-27T04:42:19.503645: step 4465, loss 0.749457, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 272/599 (start=17408, end=17472)\n",
      "2018-11-27T04:42:19.815525: step 4466, loss 0.568641, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 273/599 (start=17472, end=17536)\n",
      "2018-11-27T04:42:20.149039: step 4467, loss 0.531076, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 274/599 (start=17536, end=17600)\n",
      "2018-11-27T04:42:20.492418: step 4468, loss 0.71788, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 275/599 (start=17600, end=17664)\n",
      "2018-11-27T04:42:20.824342: step 4469, loss 0.542781, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 276/599 (start=17664, end=17728)\n",
      "2018-11-27T04:42:21.133975: step 4470, loss 0.475515, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 277/599 (start=17728, end=17792)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:42:21.480208: step 4471, loss 0.59191, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 278/599 (start=17792, end=17856)\n",
      "2018-11-27T04:42:21.820314: step 4472, loss 0.556485, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 279/599 (start=17856, end=17920)\n",
      "2018-11-27T04:42:22.134838: step 4473, loss 0.399486, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 280/599 (start=17920, end=17984)\n",
      "2018-11-27T04:42:22.473944: step 4474, loss 0.489098, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 281/599 (start=17984, end=18048)\n",
      "2018-11-27T04:42:22.814532: step 4475, loss 0.739203, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 282/599 (start=18048, end=18112)\n",
      "2018-11-27T04:42:23.128226: step 4476, loss 0.71235, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 283/599 (start=18112, end=18176)\n",
      "2018-11-27T04:42:23.437789: step 4477, loss 0.57593, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 284/599 (start=18176, end=18240)\n",
      "2018-11-27T04:42:23.777214: step 4478, loss 0.646336, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 285/599 (start=18240, end=18304)\n",
      "2018-11-27T04:42:24.133291: step 4479, loss 0.627587, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 286/599 (start=18304, end=18368)\n",
      "2018-11-27T04:42:24.444844: step 4480, loss 0.596963, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 287/599 (start=18368, end=18432)\n",
      "2018-11-27T04:42:24.773691: step 4481, loss 0.589497, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 288/599 (start=18432, end=18496)\n",
      "2018-11-27T04:42:25.101913: step 4482, loss 0.636067, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 289/599 (start=18496, end=18560)\n",
      "2018-11-27T04:42:25.454363: step 4483, loss 0.715352, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 290/599 (start=18560, end=18624)\n",
      "2018-11-27T04:42:25.773192: step 4484, loss 0.508719, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 291/599 (start=18624, end=18688)\n",
      "2018-11-27T04:42:26.097988: step 4485, loss 0.417944, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 292/599 (start=18688, end=18752)\n",
      "2018-11-27T04:42:26.422425: step 4486, loss 0.437496, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 293/599 (start=18752, end=18816)\n",
      "2018-11-27T04:42:26.734989: step 4487, loss 0.758876, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 294/599 (start=18816, end=18880)\n",
      "2018-11-27T04:42:27.059630: step 4488, loss 0.592675, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 295/599 (start=18880, end=18944)\n",
      "2018-11-27T04:42:27.392329: step 4489, loss 0.444147, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 296/599 (start=18944, end=19008)\n",
      "2018-11-27T04:42:27.710918: step 4490, loss 0.731443, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 297/599 (start=19008, end=19072)\n",
      "2018-11-27T04:42:28.045091: step 4491, loss 0.64792, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 298/599 (start=19072, end=19136)\n",
      "2018-11-27T04:42:28.391921: step 4492, loss 0.6835, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 299/599 (start=19136, end=19200)\n",
      "2018-11-27T04:42:28.701324: step 4493, loss 0.547916, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 300/599 (start=19200, end=19264)\n",
      "2018-11-27T04:42:29.051751: step 4494, loss 0.634178, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 301/599 (start=19264, end=19328)\n",
      "2018-11-27T04:42:29.368194: step 4495, loss 0.863429, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 302/599 (start=19328, end=19392)\n",
      "2018-11-27T04:42:29.695742: step 4496, loss 0.500006, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 303/599 (start=19392, end=19456)\n",
      "2018-11-27T04:42:30.031608: step 4497, loss 0.729826, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 304/599 (start=19456, end=19520)\n",
      "2018-11-27T04:42:30.361914: step 4498, loss 0.532826, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 305/599 (start=19520, end=19584)\n",
      "2018-11-27T04:42:30.691563: step 4499, loss 0.395961, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 306/599 (start=19584, end=19648)\n",
      "2018-11-27T04:42:31.015794: step 4500, loss 0.766143, acc 0.828125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:42:37.117472: step 4500, loss 1.93332, acc 0.499647\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-4500\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 307/599 (start=19648, end=19712)\n",
      "2018-11-27T04:42:37.867396: step 4501, loss 0.771459, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 308/599 (start=19712, end=19776)\n",
      "2018-11-27T04:42:38.189709: step 4502, loss 0.565927, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 309/599 (start=19776, end=19840)\n",
      "2018-11-27T04:42:38.530705: step 4503, loss 0.523973, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 310/599 (start=19840, end=19904)\n",
      "2018-11-27T04:42:38.862905: step 4504, loss 0.474282, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 311/599 (start=19904, end=19968)\n",
      "2018-11-27T04:42:39.197150: step 4505, loss 0.591908, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 312/599 (start=19968, end=20032)\n",
      "2018-11-27T04:42:39.533154: step 4506, loss 0.550346, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 313/599 (start=20032, end=20096)\n",
      "2018-11-27T04:42:39.872465: step 4507, loss 0.634046, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 314/599 (start=20096, end=20160)\n",
      "2018-11-27T04:42:40.183471: step 4508, loss 0.565027, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 315/599 (start=20160, end=20224)\n",
      "2018-11-27T04:42:40.506572: step 4509, loss 0.635777, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 316/599 (start=20224, end=20288)\n",
      "2018-11-27T04:42:40.850902: step 4510, loss 0.483276, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 317/599 (start=20288, end=20352)\n",
      "2018-11-27T04:42:41.184511: step 4511, loss 0.768003, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 318/599 (start=20352, end=20416)\n",
      "2018-11-27T04:42:41.504420: step 4512, loss 0.600188, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 319/599 (start=20416, end=20480)\n",
      "2018-11-27T04:42:41.825781: step 4513, loss 0.698383, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 320/599 (start=20480, end=20544)\n",
      "2018-11-27T04:42:42.141457: step 4514, loss 0.719984, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 321/599 (start=20544, end=20608)\n",
      "2018-11-27T04:42:42.451900: step 4515, loss 0.627268, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 322/599 (start=20608, end=20672)\n",
      "2018-11-27T04:42:42.768233: step 4516, loss 0.501475, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 323/599 (start=20672, end=20736)\n",
      "2018-11-27T04:42:43.105365: step 4517, loss 0.703565, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 324/599 (start=20736, end=20800)\n",
      "2018-11-27T04:42:43.436604: step 4518, loss 0.615976, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 325/599 (start=20800, end=20864)\n",
      "2018-11-27T04:42:43.775897: step 4519, loss 0.635668, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 326/599 (start=20864, end=20928)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:42:44.102897: step 4520, loss 0.497521, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 327/599 (start=20928, end=20992)\n",
      "2018-11-27T04:42:44.441574: step 4521, loss 0.678452, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 328/599 (start=20992, end=21056)\n",
      "2018-11-27T04:42:44.762576: step 4522, loss 0.585182, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 329/599 (start=21056, end=21120)\n",
      "2018-11-27T04:42:45.102291: step 4523, loss 0.564362, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 330/599 (start=21120, end=21184)\n",
      "2018-11-27T04:42:45.428177: step 4524, loss 0.556, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 331/599 (start=21184, end=21248)\n",
      "2018-11-27T04:42:45.767573: step 4525, loss 0.768234, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 332/599 (start=21248, end=21312)\n",
      "2018-11-27T04:42:46.102911: step 4526, loss 0.518732, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 333/599 (start=21312, end=21376)\n",
      "2018-11-27T04:42:46.446596: step 4527, loss 0.614958, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 334/599 (start=21376, end=21440)\n",
      "2018-11-27T04:42:46.785616: step 4528, loss 0.430821, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 335/599 (start=21440, end=21504)\n",
      "2018-11-27T04:42:47.104261: step 4529, loss 0.680374, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 336/599 (start=21504, end=21568)\n",
      "2018-11-27T04:42:47.433327: step 4530, loss 0.710736, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 337/599 (start=21568, end=21632)\n",
      "2018-11-27T04:42:47.777398: step 4531, loss 0.652131, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 338/599 (start=21632, end=21696)\n",
      "2018-11-27T04:42:48.110611: step 4532, loss 0.607588, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 339/599 (start=21696, end=21760)\n",
      "2018-11-27T04:42:48.443854: step 4533, loss 0.62764, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 340/599 (start=21760, end=21824)\n",
      "2018-11-27T04:42:48.755554: step 4534, loss 0.620637, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 341/599 (start=21824, end=21888)\n",
      "2018-11-27T04:42:49.082416: step 4535, loss 0.634113, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 342/599 (start=21888, end=21952)\n",
      "2018-11-27T04:42:49.389604: step 4536, loss 0.61947, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 343/599 (start=21952, end=22016)\n",
      "2018-11-27T04:42:49.727590: step 4537, loss 0.762931, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 344/599 (start=22016, end=22080)\n",
      "2018-11-27T04:42:50.048138: step 4538, loss 0.438462, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 345/599 (start=22080, end=22144)\n",
      "2018-11-27T04:42:50.361774: step 4539, loss 0.465911, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 346/599 (start=22144, end=22208)\n",
      "2018-11-27T04:42:50.678496: step 4540, loss 0.557062, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 347/599 (start=22208, end=22272)\n",
      "2018-11-27T04:42:51.011698: step 4541, loss 0.489867, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 348/599 (start=22272, end=22336)\n",
      "2018-11-27T04:42:51.341493: step 4542, loss 0.533416, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 349/599 (start=22336, end=22400)\n",
      "2018-11-27T04:42:51.696417: step 4543, loss 0.521343, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 350/599 (start=22400, end=22464)\n",
      "2018-11-27T04:42:52.006742: step 4544, loss 0.763509, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 351/599 (start=22464, end=22528)\n",
      "2018-11-27T04:42:52.331349: step 4545, loss 0.620578, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 352/599 (start=22528, end=22592)\n",
      "2018-11-27T04:42:52.658494: step 4546, loss 0.682614, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 353/599 (start=22592, end=22656)\n",
      "2018-11-27T04:42:53.000270: step 4547, loss 0.784113, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 354/599 (start=22656, end=22720)\n",
      "2018-11-27T04:42:53.310361: step 4548, loss 0.595521, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 355/599 (start=22720, end=22784)\n",
      "2018-11-27T04:42:53.619226: step 4549, loss 0.604184, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 356/599 (start=22784, end=22848)\n",
      "2018-11-27T04:42:53.936096: step 4550, loss 0.776346, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 357/599 (start=22848, end=22912)\n",
      "2018-11-27T04:42:54.256219: step 4551, loss 0.43459, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 358/599 (start=22912, end=22976)\n",
      "2018-11-27T04:42:54.599673: step 4552, loss 0.54908, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 359/599 (start=22976, end=23040)\n",
      "2018-11-27T04:42:54.948731: step 4553, loss 0.791581, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 360/599 (start=23040, end=23104)\n",
      "2018-11-27T04:42:55.293447: step 4554, loss 0.571084, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 361/599 (start=23104, end=23168)\n",
      "2018-11-27T04:42:55.621963: step 4555, loss 0.479245, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 362/599 (start=23168, end=23232)\n",
      "2018-11-27T04:42:55.957398: step 4556, loss 0.703766, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 363/599 (start=23232, end=23296)\n",
      "2018-11-27T04:42:56.300369: step 4557, loss 0.519604, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 364/599 (start=23296, end=23360)\n",
      "2018-11-27T04:42:56.634610: step 4558, loss 0.586406, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 365/599 (start=23360, end=23424)\n",
      "2018-11-27T04:42:56.971755: step 4559, loss 0.655915, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 366/599 (start=23424, end=23488)\n",
      "2018-11-27T04:42:57.294293: step 4560, loss 0.484897, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 367/599 (start=23488, end=23552)\n",
      "2018-11-27T04:42:57.630402: step 4561, loss 0.608823, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 368/599 (start=23552, end=23616)\n",
      "2018-11-27T04:42:57.968133: step 4562, loss 0.621849, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 369/599 (start=23616, end=23680)\n",
      "2018-11-27T04:42:58.277948: step 4563, loss 0.621522, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 370/599 (start=23680, end=23744)\n",
      "2018-11-27T04:42:58.615715: step 4564, loss 0.551781, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 371/599 (start=23744, end=23808)\n",
      "2018-11-27T04:42:58.944079: step 4565, loss 0.549037, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 372/599 (start=23808, end=23872)\n",
      "2018-11-27T04:42:59.276353: step 4566, loss 0.599822, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 373/599 (start=23872, end=23936)\n",
      "2018-11-27T04:42:59.620048: step 4567, loss 0.643591, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 374/599 (start=23936, end=24000)\n",
      "2018-11-27T04:42:59.963213: step 4568, loss 0.604739, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 375/599 (start=24000, end=24064)\n",
      "2018-11-27T04:43:00.304600: step 4569, loss 0.468522, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 376/599 (start=24064, end=24128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:43:00.642813: step 4570, loss 0.756683, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 377/599 (start=24128, end=24192)\n",
      "2018-11-27T04:43:00.998589: step 4571, loss 0.606368, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 378/599 (start=24192, end=24256)\n",
      "2018-11-27T04:43:01.365793: step 4572, loss 0.534228, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 379/599 (start=24256, end=24320)\n",
      "2018-11-27T04:43:01.686071: step 4573, loss 0.609565, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 380/599 (start=24320, end=24384)\n",
      "2018-11-27T04:43:02.022269: step 4574, loss 0.500814, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 381/599 (start=24384, end=24448)\n",
      "2018-11-27T04:43:02.362584: step 4575, loss 0.573709, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 382/599 (start=24448, end=24512)\n",
      "2018-11-27T04:43:02.705903: step 4576, loss 0.611593, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 383/599 (start=24512, end=24576)\n",
      "2018-11-27T04:43:03.021877: step 4577, loss 0.641495, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 384/599 (start=24576, end=24640)\n",
      "2018-11-27T04:43:03.369717: step 4578, loss 0.449277, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 385/599 (start=24640, end=24704)\n",
      "2018-11-27T04:43:03.712618: step 4579, loss 0.510051, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 386/599 (start=24704, end=24768)\n",
      "2018-11-27T04:43:04.040823: step 4580, loss 0.575793, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 387/599 (start=24768, end=24832)\n",
      "2018-11-27T04:43:04.372448: step 4581, loss 0.677064, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 388/599 (start=24832, end=24896)\n",
      "2018-11-27T04:43:04.743178: step 4582, loss 0.502924, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 389/599 (start=24896, end=24960)\n",
      "2018-11-27T04:43:05.099756: step 4583, loss 0.710471, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 390/599 (start=24960, end=25024)\n",
      "2018-11-27T04:43:05.414018: step 4584, loss 0.597533, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 391/599 (start=25024, end=25088)\n",
      "2018-11-27T04:43:05.752835: step 4585, loss 1.02185, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 392/599 (start=25088, end=25152)\n",
      "2018-11-27T04:43:06.095539: step 4586, loss 0.607597, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 393/599 (start=25152, end=25216)\n",
      "2018-11-27T04:43:06.428331: step 4587, loss 0.770106, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 394/599 (start=25216, end=25280)\n",
      "2018-11-27T04:43:06.761573: step 4588, loss 0.722208, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 395/599 (start=25280, end=25344)\n",
      "2018-11-27T04:43:07.081519: step 4589, loss 0.571188, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 396/599 (start=25344, end=25408)\n",
      "2018-11-27T04:43:07.425695: step 4590, loss 0.598484, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 397/599 (start=25408, end=25472)\n",
      "2018-11-27T04:43:07.762892: step 4591, loss 0.510052, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 398/599 (start=25472, end=25536)\n",
      "2018-11-27T04:43:08.109243: step 4592, loss 0.554086, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 399/599 (start=25536, end=25600)\n",
      "2018-11-27T04:43:08.434925: step 4593, loss 0.609861, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 400/599 (start=25600, end=25664)\n",
      "2018-11-27T04:43:08.753082: step 4594, loss 0.527872, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 401/599 (start=25664, end=25728)\n",
      "2018-11-27T04:43:09.060099: step 4595, loss 0.563623, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 402/599 (start=25728, end=25792)\n",
      "2018-11-27T04:43:09.381068: step 4596, loss 0.490292, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 403/599 (start=25792, end=25856)\n",
      "2018-11-27T04:43:09.739109: step 4597, loss 0.665252, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 404/599 (start=25856, end=25920)\n",
      "2018-11-27T04:43:10.064311: step 4598, loss 0.548967, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 405/599 (start=25920, end=25984)\n",
      "2018-11-27T04:43:10.395479: step 4599, loss 0.486871, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 406/599 (start=25984, end=26048)\n",
      "2018-11-27T04:43:10.732795: step 4600, loss 0.585901, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:43:16.928373: step 4600, loss 1.94395, acc 0.512264\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-4600\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 407/599 (start=26048, end=26112)\n",
      "2018-11-27T04:43:17.695116: step 4601, loss 0.61428, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 408/599 (start=26112, end=26176)\n",
      "2018-11-27T04:43:18.007613: step 4602, loss 0.683179, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 409/599 (start=26176, end=26240)\n",
      "2018-11-27T04:43:18.349763: step 4603, loss 0.609714, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 410/599 (start=26240, end=26304)\n",
      "2018-11-27T04:43:18.675133: step 4604, loss 0.525437, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 411/599 (start=26304, end=26368)\n",
      "2018-11-27T04:43:18.989876: step 4605, loss 0.594095, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 412/599 (start=26368, end=26432)\n",
      "2018-11-27T04:43:19.324440: step 4606, loss 0.527483, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 413/599 (start=26432, end=26496)\n",
      "2018-11-27T04:43:19.666330: step 4607, loss 0.681991, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 414/599 (start=26496, end=26560)\n",
      "2018-11-27T04:43:19.978001: step 4608, loss 0.806843, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 415/599 (start=26560, end=26624)\n",
      "2018-11-27T04:43:20.336302: step 4609, loss 0.613726, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 416/599 (start=26624, end=26688)\n",
      "2018-11-27T04:43:20.665165: step 4610, loss 1.00655, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 417/599 (start=26688, end=26752)\n",
      "2018-11-27T04:43:21.008399: step 4611, loss 0.535352, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 418/599 (start=26752, end=26816)\n",
      "2018-11-27T04:43:21.348983: step 4612, loss 0.711806, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 419/599 (start=26816, end=26880)\n",
      "2018-11-27T04:43:21.685640: step 4613, loss 0.597388, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 420/599 (start=26880, end=26944)\n",
      "2018-11-27T04:43:22.012838: step 4614, loss 0.623479, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 421/599 (start=26944, end=27008)\n",
      "2018-11-27T04:43:22.347129: step 4615, loss 0.63216, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 422/599 (start=27008, end=27072)\n",
      "2018-11-27T04:43:22.671702: step 4616, loss 0.737397, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 423/599 (start=27072, end=27136)\n",
      "2018-11-27T04:43:23.017457: step 4617, loss 0.687439, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 424/599 (start=27136, end=27200)\n",
      "2018-11-27T04:43:23.334947: step 4618, loss 0.591299, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 425/599 (start=27200, end=27264)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:43:23.673995: step 4619, loss 0.828581, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 426/599 (start=27264, end=27328)\n",
      "2018-11-27T04:43:23.994116: step 4620, loss 0.57307, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 427/599 (start=27328, end=27392)\n",
      "2018-11-27T04:43:24.309385: step 4621, loss 0.378304, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 428/599 (start=27392, end=27456)\n",
      "2018-11-27T04:43:24.629113: step 4622, loss 0.655761, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 429/599 (start=27456, end=27520)\n",
      "2018-11-27T04:43:24.973702: step 4623, loss 0.618979, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 430/599 (start=27520, end=27584)\n",
      "2018-11-27T04:43:25.290012: step 4624, loss 0.514675, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 431/599 (start=27584, end=27648)\n",
      "2018-11-27T04:43:25.603366: step 4625, loss 0.724233, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 432/599 (start=27648, end=27712)\n",
      "2018-11-27T04:43:25.929100: step 4626, loss 0.583834, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 433/599 (start=27712, end=27776)\n",
      "2018-11-27T04:43:26.265843: step 4627, loss 0.716842, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 434/599 (start=27776, end=27840)\n",
      "2018-11-27T04:43:26.615298: step 4628, loss 0.694963, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 435/599 (start=27840, end=27904)\n",
      "2018-11-27T04:43:26.942848: step 4629, loss 0.539374, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 436/599 (start=27904, end=27968)\n",
      "2018-11-27T04:43:27.281070: step 4630, loss 0.441152, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 437/599 (start=27968, end=28032)\n",
      "2018-11-27T04:43:27.621894: step 4631, loss 0.495905, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 438/599 (start=28032, end=28096)\n",
      "2018-11-27T04:43:27.966974: step 4632, loss 0.92319, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 439/599 (start=28096, end=28160)\n",
      "2018-11-27T04:43:28.294508: step 4633, loss 0.474222, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 440/599 (start=28160, end=28224)\n",
      "2018-11-27T04:43:28.639844: step 4634, loss 0.639194, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 441/599 (start=28224, end=28288)\n",
      "2018-11-27T04:43:28.986443: step 4635, loss 0.685808, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 442/599 (start=28288, end=28352)\n",
      "2018-11-27T04:43:29.296831: step 4636, loss 0.471532, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 443/599 (start=28352, end=28416)\n",
      "2018-11-27T04:43:29.634392: step 4637, loss 0.607191, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 444/599 (start=28416, end=28480)\n",
      "2018-11-27T04:43:29.956297: step 4638, loss 0.689275, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 445/599 (start=28480, end=28544)\n",
      "2018-11-27T04:43:30.288344: step 4639, loss 0.846894, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 446/599 (start=28544, end=28608)\n",
      "2018-11-27T04:43:30.627969: step 4640, loss 0.509423, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 447/599 (start=28608, end=28672)\n",
      "2018-11-27T04:43:30.961474: step 4641, loss 0.691535, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 448/599 (start=28672, end=28736)\n",
      "2018-11-27T04:43:31.319374: step 4642, loss 0.506878, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 449/599 (start=28736, end=28800)\n",
      "2018-11-27T04:43:31.622684: step 4643, loss 0.576087, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 450/599 (start=28800, end=28864)\n",
      "2018-11-27T04:43:31.925658: step 4644, loss 0.530103, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 451/599 (start=28864, end=28928)\n",
      "2018-11-27T04:43:32.264249: step 4645, loss 0.537552, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 452/599 (start=28928, end=28992)\n",
      "2018-11-27T04:43:32.609805: step 4646, loss 0.591047, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 453/599 (start=28992, end=29056)\n",
      "2018-11-27T04:43:32.943884: step 4647, loss 0.427825, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 454/599 (start=29056, end=29120)\n",
      "2018-11-27T04:43:33.258671: step 4648, loss 0.792093, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 455/599 (start=29120, end=29184)\n",
      "2018-11-27T04:43:33.598852: step 4649, loss 0.580313, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 456/599 (start=29184, end=29248)\n",
      "2018-11-27T04:43:33.932839: step 4650, loss 0.733362, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 457/599 (start=29248, end=29312)\n",
      "2018-11-27T04:43:34.275145: step 4651, loss 0.603915, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 458/599 (start=29312, end=29376)\n",
      "2018-11-27T04:43:34.601228: step 4652, loss 0.730798, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 459/599 (start=29376, end=29440)\n",
      "2018-11-27T04:43:34.944369: step 4653, loss 0.702335, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 460/599 (start=29440, end=29504)\n",
      "2018-11-27T04:43:35.277011: step 4654, loss 0.714585, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 461/599 (start=29504, end=29568)\n",
      "2018-11-27T04:43:35.608728: step 4655, loss 0.669931, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 462/599 (start=29568, end=29632)\n",
      "2018-11-27T04:43:35.955134: step 4656, loss 0.638906, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 463/599 (start=29632, end=29696)\n",
      "2018-11-27T04:43:36.274865: step 4657, loss 0.746744, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 464/599 (start=29696, end=29760)\n",
      "2018-11-27T04:43:36.611726: step 4658, loss 0.542811, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 465/599 (start=29760, end=29824)\n",
      "2018-11-27T04:43:36.962393: step 4659, loss 0.619469, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 466/599 (start=29824, end=29888)\n",
      "2018-11-27T04:43:37.295171: step 4660, loss 0.585696, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 467/599 (start=29888, end=29952)\n",
      "2018-11-27T04:43:37.637074: step 4661, loss 0.491002, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 468/599 (start=29952, end=30016)\n",
      "2018-11-27T04:43:37.945141: step 4662, loss 0.692367, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 469/599 (start=30016, end=30080)\n",
      "2018-11-27T04:43:38.272909: step 4663, loss 0.806904, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 470/599 (start=30080, end=30144)\n",
      "2018-11-27T04:43:38.602889: step 4664, loss 0.523823, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 471/599 (start=30144, end=30208)\n",
      "2018-11-27T04:43:38.921133: step 4665, loss 0.518882, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 472/599 (start=30208, end=30272)\n",
      "2018-11-27T04:43:39.252791: step 4666, loss 0.673565, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 473/599 (start=30272, end=30336)\n",
      "2018-11-27T04:43:39.585067: step 4667, loss 0.734521, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 474/599 (start=30336, end=30400)\n",
      "2018-11-27T04:43:39.906743: step 4668, loss 0.530442, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 475/599 (start=30400, end=30464)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:43:40.223325: step 4669, loss 0.35789, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 476/599 (start=30464, end=30528)\n",
      "2018-11-27T04:43:40.540424: step 4670, loss 0.727454, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 477/599 (start=30528, end=30592)\n",
      "2018-11-27T04:43:40.843048: step 4671, loss 0.576802, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 478/599 (start=30592, end=30656)\n",
      "2018-11-27T04:43:41.186783: step 4672, loss 0.392107, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 479/599 (start=30656, end=30720)\n",
      "2018-11-27T04:43:41.499114: step 4673, loss 0.6576, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 480/599 (start=30720, end=30784)\n",
      "2018-11-27T04:43:41.821022: step 4674, loss 0.653366, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 481/599 (start=30784, end=30848)\n",
      "2018-11-27T04:43:42.160825: step 4675, loss 0.680822, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 482/599 (start=30848, end=30912)\n",
      "2018-11-27T04:43:42.492175: step 4676, loss 0.475037, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 483/599 (start=30912, end=30976)\n",
      "2018-11-27T04:43:42.802209: step 4677, loss 0.758775, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 484/599 (start=30976, end=31040)\n",
      "2018-11-27T04:43:43.123870: step 4678, loss 0.794372, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 485/599 (start=31040, end=31104)\n",
      "2018-11-27T04:43:43.469973: step 4679, loss 0.566661, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 486/599 (start=31104, end=31168)\n",
      "2018-11-27T04:43:43.801093: step 4680, loss 0.7417, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 487/599 (start=31168, end=31232)\n",
      "2018-11-27T04:43:44.120207: step 4681, loss 0.766562, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 488/599 (start=31232, end=31296)\n",
      "2018-11-27T04:43:44.459206: step 4682, loss 0.681038, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 489/599 (start=31296, end=31360)\n",
      "2018-11-27T04:43:44.811567: step 4683, loss 0.675941, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 490/599 (start=31360, end=31424)\n",
      "2018-11-27T04:43:45.135854: step 4684, loss 0.705003, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 491/599 (start=31424, end=31488)\n",
      "2018-11-27T04:43:45.454794: step 4685, loss 0.884668, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 492/599 (start=31488, end=31552)\n",
      "2018-11-27T04:43:45.770909: step 4686, loss 0.863992, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 493/599 (start=31552, end=31616)\n",
      "2018-11-27T04:43:46.114649: step 4687, loss 0.503108, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 494/599 (start=31616, end=31680)\n",
      "2018-11-27T04:43:46.453645: step 4688, loss 0.775894, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 495/599 (start=31680, end=31744)\n",
      "2018-11-27T04:43:46.791988: step 4689, loss 0.555253, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 496/599 (start=31744, end=31808)\n",
      "2018-11-27T04:43:47.133331: step 4690, loss 0.503963, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 497/599 (start=31808, end=31872)\n",
      "2018-11-27T04:43:47.487592: step 4691, loss 0.589061, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 498/599 (start=31872, end=31936)\n",
      "2018-11-27T04:43:47.834725: step 4692, loss 0.59457, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 499/599 (start=31936, end=32000)\n",
      "2018-11-27T04:43:48.149575: step 4693, loss 0.539297, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 500/599 (start=32000, end=32064)\n",
      "2018-11-27T04:43:48.481859: step 4694, loss 0.530313, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 501/599 (start=32064, end=32128)\n",
      "2018-11-27T04:43:48.793725: step 4695, loss 0.560122, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 502/599 (start=32128, end=32192)\n",
      "2018-11-27T04:43:49.131787: step 4696, loss 1.03674, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 503/599 (start=32192, end=32256)\n",
      "2018-11-27T04:43:49.458260: step 4697, loss 0.607699, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 504/599 (start=32256, end=32320)\n",
      "2018-11-27T04:43:49.794794: step 4698, loss 0.530327, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 505/599 (start=32320, end=32384)\n",
      "2018-11-27T04:43:50.129722: step 4699, loss 0.765773, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 506/599 (start=32384, end=32448)\n",
      "2018-11-27T04:43:50.473974: step 4700, loss 0.699218, acc 0.859375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:43:56.946649: step 4700, loss 1.95569, acc 0.521903\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-4700\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 507/599 (start=32448, end=32512)\n",
      "2018-11-27T04:43:57.675192: step 4701, loss 0.576372, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 508/599 (start=32512, end=32576)\n",
      "2018-11-27T04:43:58.024278: step 4702, loss 0.65512, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 509/599 (start=32576, end=32640)\n",
      "2018-11-27T04:43:58.371200: step 4703, loss 0.69602, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 510/599 (start=32640, end=32704)\n",
      "2018-11-27T04:43:58.695256: step 4704, loss 0.426633, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 511/599 (start=32704, end=32768)\n",
      "2018-11-27T04:43:59.039227: step 4705, loss 0.654864, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 512/599 (start=32768, end=32832)\n",
      "2018-11-27T04:43:59.381158: step 4706, loss 0.544366, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 513/599 (start=32832, end=32896)\n",
      "2018-11-27T04:43:59.732159: step 4707, loss 0.618529, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 514/599 (start=32896, end=32960)\n",
      "2018-11-27T04:44:00.088868: step 4708, loss 0.800849, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 515/599 (start=32960, end=33024)\n",
      "2018-11-27T04:44:00.428443: step 4709, loss 0.581236, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 516/599 (start=33024, end=33088)\n",
      "2018-11-27T04:44:00.744614: step 4710, loss 0.596511, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 517/599 (start=33088, end=33152)\n",
      "2018-11-27T04:44:01.067645: step 4711, loss 0.73556, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 518/599 (start=33152, end=33216)\n",
      "2018-11-27T04:44:01.413822: step 4712, loss 0.756037, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 519/599 (start=33216, end=33280)\n",
      "2018-11-27T04:44:01.748008: step 4713, loss 0.592287, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 520/599 (start=33280, end=33344)\n",
      "2018-11-27T04:44:02.075129: step 4714, loss 0.605332, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 521/599 (start=33344, end=33408)\n",
      "2018-11-27T04:44:02.395102: step 4715, loss 0.646292, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 522/599 (start=33408, end=33472)\n",
      "2018-11-27T04:44:02.720532: step 4716, loss 0.771611, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 523/599 (start=33472, end=33536)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:44:03.044795: step 4717, loss 0.560085, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 524/599 (start=33536, end=33600)\n",
      "2018-11-27T04:44:03.360467: step 4718, loss 0.452626, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 525/599 (start=33600, end=33664)\n",
      "2018-11-27T04:44:03.693830: step 4719, loss 0.49404, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 526/599 (start=33664, end=33728)\n",
      "2018-11-27T04:44:04.028498: step 4720, loss 0.448434, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 527/599 (start=33728, end=33792)\n",
      "2018-11-27T04:44:04.361494: step 4721, loss 0.542402, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 528/599 (start=33792, end=33856)\n",
      "2018-11-27T04:44:04.711357: step 4722, loss 0.507701, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 529/599 (start=33856, end=33920)\n",
      "2018-11-27T04:44:05.035025: step 4723, loss 0.444831, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 530/599 (start=33920, end=33984)\n",
      "2018-11-27T04:44:05.388931: step 4724, loss 0.734034, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 531/599 (start=33984, end=34048)\n",
      "2018-11-27T04:44:05.713124: step 4725, loss 0.760622, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 532/599 (start=34048, end=34112)\n",
      "2018-11-27T04:44:06.045919: step 4726, loss 0.568003, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 533/599 (start=34112, end=34176)\n",
      "2018-11-27T04:44:06.357580: step 4727, loss 0.469498, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 534/599 (start=34176, end=34240)\n",
      "2018-11-27T04:44:06.707265: step 4728, loss 0.687304, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 535/599 (start=34240, end=34304)\n",
      "2018-11-27T04:44:07.049614: step 4729, loss 0.49698, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 536/599 (start=34304, end=34368)\n",
      "2018-11-27T04:44:07.386759: step 4730, loss 0.859579, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 537/599 (start=34368, end=34432)\n",
      "2018-11-27T04:44:07.716056: step 4731, loss 0.754467, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 538/599 (start=34432, end=34496)\n",
      "2018-11-27T04:44:08.032087: step 4732, loss 0.489456, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 539/599 (start=34496, end=34560)\n",
      "2018-11-27T04:44:08.349101: step 4733, loss 0.676572, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 540/599 (start=34560, end=34624)\n",
      "2018-11-27T04:44:08.689401: step 4734, loss 0.334109, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 541/599 (start=34624, end=34688)\n",
      "2018-11-27T04:44:09.026325: step 4735, loss 0.628163, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 542/599 (start=34688, end=34752)\n",
      "2018-11-27T04:44:09.360756: step 4736, loss 0.68993, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 543/599 (start=34752, end=34816)\n",
      "2018-11-27T04:44:09.708375: step 4737, loss 0.639171, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 544/599 (start=34816, end=34880)\n",
      "2018-11-27T04:44:10.044720: step 4738, loss 0.790051, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 545/599 (start=34880, end=34944)\n",
      "2018-11-27T04:44:10.360630: step 4739, loss 0.672604, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 546/599 (start=34944, end=35008)\n",
      "2018-11-27T04:44:10.683689: step 4740, loss 0.49547, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 547/599 (start=35008, end=35072)\n",
      "2018-11-27T04:44:11.019960: step 4741, loss 0.725601, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 548/599 (start=35072, end=35136)\n",
      "2018-11-27T04:44:11.354097: step 4742, loss 0.73946, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 549/599 (start=35136, end=35200)\n",
      "2018-11-27T04:44:11.687208: step 4743, loss 0.60245, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 550/599 (start=35200, end=35264)\n",
      "2018-11-27T04:44:12.003760: step 4744, loss 0.857409, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 551/599 (start=35264, end=35328)\n",
      "2018-11-27T04:44:12.351700: step 4745, loss 0.71202, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 552/599 (start=35328, end=35392)\n",
      "2018-11-27T04:44:12.686966: step 4746, loss 0.845294, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 553/599 (start=35392, end=35456)\n",
      "2018-11-27T04:44:13.042281: step 4747, loss 0.760165, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 554/599 (start=35456, end=35520)\n",
      "2018-11-27T04:44:13.384352: step 4748, loss 0.755952, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 555/599 (start=35520, end=35584)\n",
      "2018-11-27T04:44:13.728018: step 4749, loss 0.610812, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 556/599 (start=35584, end=35648)\n",
      "2018-11-27T04:44:14.071537: step 4750, loss 0.704541, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 557/599 (start=35648, end=35712)\n",
      "2018-11-27T04:44:14.412777: step 4751, loss 0.601518, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 558/599 (start=35712, end=35776)\n",
      "2018-11-27T04:44:14.730515: step 4752, loss 0.556426, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 559/599 (start=35776, end=35840)\n",
      "2018-11-27T04:44:15.036914: step 4753, loss 0.539639, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 560/599 (start=35840, end=35904)\n",
      "2018-11-27T04:44:15.363313: step 4754, loss 0.494504, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 561/599 (start=35904, end=35968)\n",
      "2018-11-27T04:44:15.682623: step 4755, loss 0.581414, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 562/599 (start=35968, end=36032)\n",
      "2018-11-27T04:44:16.036070: step 4756, loss 0.647126, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 563/599 (start=36032, end=36096)\n",
      "2018-11-27T04:44:16.352194: step 4757, loss 0.650784, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 564/599 (start=36096, end=36160)\n",
      "2018-11-27T04:44:16.702511: step 4758, loss 0.664287, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 565/599 (start=36160, end=36224)\n",
      "2018-11-27T04:44:17.027381: step 4759, loss 0.78218, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 566/599 (start=36224, end=36288)\n",
      "2018-11-27T04:44:17.361943: step 4760, loss 0.549265, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 567/599 (start=36288, end=36352)\n",
      "2018-11-27T04:44:17.695354: step 4761, loss 0.521871, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 568/599 (start=36352, end=36416)\n",
      "2018-11-27T04:44:18.039394: step 4762, loss 0.685565, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 569/599 (start=36416, end=36480)\n",
      "2018-11-27T04:44:18.368694: step 4763, loss 0.65016, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 570/599 (start=36480, end=36544)\n",
      "2018-11-27T04:44:18.706316: step 4764, loss 0.611136, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 571/599 (start=36544, end=36608)\n",
      "2018-11-27T04:44:19.053628: step 4765, loss 0.531171, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 572/599 (start=36608, end=36672)\n",
      "2018-11-27T04:44:19.366443: step 4766, loss 0.528233, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 573/599 (start=36672, end=36736)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:44:19.717496: step 4767, loss 0.640728, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 574/599 (start=36736, end=36800)\n",
      "2018-11-27T04:44:20.056022: step 4768, loss 0.418821, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 575/599 (start=36800, end=36864)\n",
      "2018-11-27T04:44:20.381342: step 4769, loss 0.711866, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 576/599 (start=36864, end=36928)\n",
      "2018-11-27T04:44:20.712572: step 4770, loss 0.516897, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 577/599 (start=36928, end=36992)\n",
      "2018-11-27T04:44:21.032255: step 4771, loss 0.500476, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 578/599 (start=36992, end=37056)\n",
      "2018-11-27T04:44:21.336530: step 4772, loss 0.578996, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 579/599 (start=37056, end=37120)\n",
      "2018-11-27T04:44:21.696149: step 4773, loss 0.629168, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 580/599 (start=37120, end=37184)\n",
      "2018-11-27T04:44:22.042212: step 4774, loss 0.71618, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 581/599 (start=37184, end=37248)\n",
      "2018-11-27T04:44:22.365258: step 4775, loss 0.602089, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 582/599 (start=37248, end=37312)\n",
      "2018-11-27T04:44:22.710236: step 4776, loss 0.505231, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 583/599 (start=37312, end=37376)\n",
      "2018-11-27T04:44:23.039090: step 4777, loss 0.498576, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 584/599 (start=37376, end=37440)\n",
      "2018-11-27T04:44:23.395387: step 4778, loss 0.628338, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 585/599 (start=37440, end=37504)\n",
      "2018-11-27T04:44:23.704350: step 4779, loss 0.75288, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 586/599 (start=37504, end=37568)\n",
      "2018-11-27T04:44:24.069715: step 4780, loss 0.694705, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 587/599 (start=37568, end=37632)\n",
      "2018-11-27T04:44:24.413832: step 4781, loss 0.550324, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 588/599 (start=37632, end=37696)\n",
      "2018-11-27T04:44:24.750933: step 4782, loss 0.624817, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 589/599 (start=37696, end=37760)\n",
      "2018-11-27T04:44:25.101300: step 4783, loss 0.904077, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 590/599 (start=37760, end=37824)\n",
      "2018-11-27T04:44:25.440852: step 4784, loss 0.84895, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 591/599 (start=37824, end=37888)\n",
      "2018-11-27T04:44:25.753452: step 4785, loss 0.627596, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 592/599 (start=37888, end=37952)\n",
      "2018-11-27T04:44:26.078012: step 4786, loss 0.499116, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 593/599 (start=37952, end=38016)\n",
      "2018-11-27T04:44:26.405921: step 4787, loss 0.780908, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 594/599 (start=38016, end=38080)\n",
      "2018-11-27T04:44:26.738372: step 4788, loss 0.586816, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 595/599 (start=38080, end=38144)\n",
      "2018-11-27T04:44:27.077250: step 4789, loss 0.68269, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 596/599 (start=38144, end=38208)\n",
      "2018-11-27T04:44:27.403128: step 4790, loss 0.668216, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 597/599 (start=38208, end=38272)\n",
      "2018-11-27T04:44:27.727763: step 4791, loss 0.716263, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 7/10, Batch 598/599 (start=38272, end=38281)\n",
      "2018-11-27T04:44:27.966205: step 4792, loss 0.574327, acc 0.888889\n",
      "***********************************************\n",
      "Epoch 8/10\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 0/599 (start=0, end=64)\n",
      "2018-11-27T04:44:28.291886: step 4793, loss 0.469982, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 1/599 (start=64, end=128)\n",
      "2018-11-27T04:44:28.643784: step 4794, loss 0.499462, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 2/599 (start=128, end=192)\n",
      "2018-11-27T04:44:28.967940: step 4795, loss 0.437975, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 3/599 (start=192, end=256)\n",
      "2018-11-27T04:44:29.318268: step 4796, loss 0.588061, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 4/599 (start=256, end=320)\n",
      "2018-11-27T04:44:29.629980: step 4797, loss 0.479251, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 5/599 (start=320, end=384)\n",
      "2018-11-27T04:44:29.972144: step 4798, loss 0.442982, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 6/599 (start=384, end=448)\n",
      "2018-11-27T04:44:30.304084: step 4799, loss 0.562158, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 7/599 (start=448, end=512)\n",
      "2018-11-27T04:44:30.622628: step 4800, loss 0.577252, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:44:36.772119: step 4800, loss 1.93334, acc 0.507562\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-4800\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 8/599 (start=512, end=576)\n",
      "2018-11-27T04:44:37.512060: step 4801, loss 0.517904, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 9/599 (start=576, end=640)\n",
      "2018-11-27T04:44:37.848410: step 4802, loss 0.354558, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 10/599 (start=640, end=704)\n",
      "2018-11-27T04:44:38.195508: step 4803, loss 0.46112, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 11/599 (start=704, end=768)\n",
      "2018-11-27T04:44:38.520395: step 4804, loss 0.460656, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 12/599 (start=768, end=832)\n",
      "2018-11-27T04:44:38.868544: step 4805, loss 0.528071, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 13/599 (start=832, end=896)\n",
      "2018-11-27T04:44:39.211040: step 4806, loss 0.471794, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 14/599 (start=896, end=960)\n",
      "2018-11-27T04:44:39.534053: step 4807, loss 0.658669, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 15/599 (start=960, end=1024)\n",
      "2018-11-27T04:44:39.885154: step 4808, loss 0.528583, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 16/599 (start=1024, end=1088)\n",
      "2018-11-27T04:44:40.216556: step 4809, loss 0.683249, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 17/599 (start=1088, end=1152)\n",
      "2018-11-27T04:44:40.529754: step 4810, loss 0.496027, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 18/599 (start=1152, end=1216)\n",
      "2018-11-27T04:44:40.861054: step 4811, loss 0.448992, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 19/599 (start=1216, end=1280)\n",
      "2018-11-27T04:44:41.179806: step 4812, loss 0.612186, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 20/599 (start=1280, end=1344)\n",
      "2018-11-27T04:44:41.496900: step 4813, loss 0.299386, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 21/599 (start=1344, end=1408)\n",
      "2018-11-27T04:44:41.832335: step 4814, loss 0.556069, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 22/599 (start=1408, end=1472)\n",
      "2018-11-27T04:44:42.149706: step 4815, loss 0.523427, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 23/599 (start=1472, end=1536)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:44:42.468490: step 4816, loss 0.748091, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 24/599 (start=1536, end=1600)\n",
      "2018-11-27T04:44:42.784579: step 4817, loss 0.677263, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 25/599 (start=1600, end=1664)\n",
      "2018-11-27T04:44:43.124369: step 4818, loss 0.499692, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 26/599 (start=1664, end=1728)\n",
      "2018-11-27T04:44:43.460860: step 4819, loss 0.624011, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 27/599 (start=1728, end=1792)\n",
      "2018-11-27T04:44:43.768593: step 4820, loss 0.636988, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 28/599 (start=1792, end=1856)\n",
      "2018-11-27T04:44:44.097023: step 4821, loss 0.466936, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 29/599 (start=1856, end=1920)\n",
      "2018-11-27T04:44:44.434713: step 4822, loss 0.647616, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 30/599 (start=1920, end=1984)\n",
      "2018-11-27T04:44:44.778750: step 4823, loss 0.419315, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 31/599 (start=1984, end=2048)\n",
      "2018-11-27T04:44:45.119217: step 4824, loss 0.480738, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 32/599 (start=2048, end=2112)\n",
      "2018-11-27T04:44:45.425320: step 4825, loss 0.608225, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 33/599 (start=2112, end=2176)\n",
      "2018-11-27T04:44:45.765088: step 4826, loss 0.341192, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 34/599 (start=2176, end=2240)\n",
      "2018-11-27T04:44:46.095791: step 4827, loss 0.555051, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 35/599 (start=2240, end=2304)\n",
      "2018-11-27T04:44:46.426533: step 4828, loss 0.507316, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 36/599 (start=2304, end=2368)\n",
      "2018-11-27T04:44:46.770605: step 4829, loss 0.494492, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 37/599 (start=2368, end=2432)\n",
      "2018-11-27T04:44:47.115211: step 4830, loss 0.551583, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 38/599 (start=2432, end=2496)\n",
      "2018-11-27T04:44:47.453236: step 4831, loss 0.382508, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 39/599 (start=2496, end=2560)\n",
      "2018-11-27T04:44:47.771849: step 4832, loss 0.78288, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 40/599 (start=2560, end=2624)\n",
      "2018-11-27T04:44:48.108217: step 4833, loss 0.43602, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 41/599 (start=2624, end=2688)\n",
      "2018-11-27T04:44:48.442646: step 4834, loss 0.603774, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 42/599 (start=2688, end=2752)\n",
      "2018-11-27T04:44:48.768390: step 4835, loss 0.573286, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 43/599 (start=2752, end=2816)\n",
      "2018-11-27T04:44:49.117104: step 4836, loss 0.650432, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 44/599 (start=2816, end=2880)\n",
      "2018-11-27T04:44:49.450352: step 4837, loss 0.352251, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 45/599 (start=2880, end=2944)\n",
      "2018-11-27T04:44:49.799353: step 4838, loss 0.589945, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 46/599 (start=2944, end=3008)\n",
      "2018-11-27T04:44:50.117530: step 4839, loss 0.603717, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 47/599 (start=3008, end=3072)\n",
      "2018-11-27T04:44:50.437017: step 4840, loss 0.507146, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 48/599 (start=3072, end=3136)\n",
      "2018-11-27T04:44:50.778333: step 4841, loss 0.602488, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 49/599 (start=3136, end=3200)\n",
      "2018-11-27T04:44:51.097781: step 4842, loss 0.661765, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 50/599 (start=3200, end=3264)\n",
      "2018-11-27T04:44:51.415171: step 4843, loss 0.478076, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 51/599 (start=3264, end=3328)\n",
      "2018-11-27T04:44:51.767716: step 4844, loss 0.678744, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 52/599 (start=3328, end=3392)\n",
      "2018-11-27T04:44:52.099412: step 4845, loss 0.640649, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 53/599 (start=3392, end=3456)\n",
      "2018-11-27T04:44:52.423274: step 4846, loss 0.57712, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 54/599 (start=3456, end=3520)\n",
      "2018-11-27T04:44:52.776427: step 4847, loss 0.519893, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 55/599 (start=3520, end=3584)\n",
      "2018-11-27T04:44:53.115965: step 4848, loss 0.482188, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 56/599 (start=3584, end=3648)\n",
      "2018-11-27T04:44:53.446114: step 4849, loss 0.555716, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 57/599 (start=3648, end=3712)\n",
      "2018-11-27T04:44:53.771995: step 4850, loss 0.568139, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 58/599 (start=3712, end=3776)\n",
      "2018-11-27T04:44:54.094952: step 4851, loss 0.388481, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 59/599 (start=3776, end=3840)\n",
      "2018-11-27T04:44:54.413017: step 4852, loss 0.449035, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 60/599 (start=3840, end=3904)\n",
      "2018-11-27T04:44:54.728759: step 4853, loss 0.443965, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 61/599 (start=3904, end=3968)\n",
      "2018-11-27T04:44:55.061333: step 4854, loss 0.472329, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 62/599 (start=3968, end=4032)\n",
      "2018-11-27T04:44:55.382385: step 4855, loss 0.628973, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 63/599 (start=4032, end=4096)\n",
      "2018-11-27T04:44:55.719087: step 4856, loss 0.331416, acc 0.984375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 64/599 (start=4096, end=4160)\n",
      "2018-11-27T04:44:56.035381: step 4857, loss 0.509818, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 65/599 (start=4160, end=4224)\n",
      "2018-11-27T04:44:56.378008: step 4858, loss 0.417969, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 66/599 (start=4224, end=4288)\n",
      "2018-11-27T04:44:56.726156: step 4859, loss 0.60797, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 67/599 (start=4288, end=4352)\n",
      "2018-11-27T04:44:57.068536: step 4860, loss 0.552366, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 68/599 (start=4352, end=4416)\n",
      "2018-11-27T04:44:57.389023: step 4861, loss 0.499471, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 69/599 (start=4416, end=4480)\n",
      "2018-11-27T04:44:57.710799: step 4862, loss 0.542492, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 70/599 (start=4480, end=4544)\n",
      "2018-11-27T04:44:58.045713: step 4863, loss 0.540471, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 71/599 (start=4544, end=4608)\n",
      "2018-11-27T04:44:58.376431: step 4864, loss 0.553336, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 72/599 (start=4608, end=4672)\n",
      "2018-11-27T04:44:58.703496: step 4865, loss 0.399434, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 73/599 (start=4672, end=4736)\n",
      "2018-11-27T04:44:59.037916: step 4866, loss 0.5289, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 74/599 (start=4736, end=4800)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:44:59.386711: step 4867, loss 0.576061, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 75/599 (start=4800, end=4864)\n",
      "2018-11-27T04:44:59.733593: step 4868, loss 0.557833, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 76/599 (start=4864, end=4928)\n",
      "2018-11-27T04:45:00.071599: step 4869, loss 0.578377, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 77/599 (start=4928, end=4992)\n",
      "2018-11-27T04:45:00.435147: step 4870, loss 0.373215, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 78/599 (start=4992, end=5056)\n",
      "2018-11-27T04:45:00.776719: step 4871, loss 0.517383, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 79/599 (start=5056, end=5120)\n",
      "2018-11-27T04:45:01.119092: step 4872, loss 0.795514, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 80/599 (start=5120, end=5184)\n",
      "2018-11-27T04:45:01.436412: step 4873, loss 0.530578, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 81/599 (start=5184, end=5248)\n",
      "2018-11-27T04:45:01.772263: step 4874, loss 0.429574, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 82/599 (start=5248, end=5312)\n",
      "2018-11-27T04:45:02.093366: step 4875, loss 0.433453, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 83/599 (start=5312, end=5376)\n",
      "2018-11-27T04:45:02.407110: step 4876, loss 0.511498, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 84/599 (start=5376, end=5440)\n",
      "2018-11-27T04:45:02.759688: step 4877, loss 0.504991, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 85/599 (start=5440, end=5504)\n",
      "2018-11-27T04:45:03.094050: step 4878, loss 0.583088, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 86/599 (start=5504, end=5568)\n",
      "2018-11-27T04:45:03.413002: step 4879, loss 0.355033, acc 0.984375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 87/599 (start=5568, end=5632)\n",
      "2018-11-27T04:45:03.762836: step 4880, loss 0.570291, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 88/599 (start=5632, end=5696)\n",
      "2018-11-27T04:45:04.104744: step 4881, loss 0.511527, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 89/599 (start=5696, end=5760)\n",
      "2018-11-27T04:45:04.415470: step 4882, loss 0.828153, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 90/599 (start=5760, end=5824)\n",
      "2018-11-27T04:45:04.747236: step 4883, loss 0.436339, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 91/599 (start=5824, end=5888)\n",
      "2018-11-27T04:45:05.064929: step 4884, loss 0.473979, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 92/599 (start=5888, end=5952)\n",
      "2018-11-27T04:45:05.391552: step 4885, loss 0.316551, acc 0.984375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 93/599 (start=5952, end=6016)\n",
      "2018-11-27T04:45:05.727584: step 4886, loss 0.50952, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 94/599 (start=6016, end=6080)\n",
      "2018-11-27T04:45:06.068099: step 4887, loss 0.455351, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 95/599 (start=6080, end=6144)\n",
      "2018-11-27T04:45:06.414650: step 4888, loss 0.480674, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 96/599 (start=6144, end=6208)\n",
      "2018-11-27T04:45:06.756675: step 4889, loss 0.383145, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 97/599 (start=6208, end=6272)\n",
      "2018-11-27T04:45:07.095102: step 4890, loss 0.477972, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 98/599 (start=6272, end=6336)\n",
      "2018-11-27T04:45:07.423074: step 4891, loss 0.455748, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 99/599 (start=6336, end=6400)\n",
      "2018-11-27T04:45:07.746687: step 4892, loss 0.636797, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 100/599 (start=6400, end=6464)\n",
      "2018-11-27T04:45:08.081305: step 4893, loss 0.463696, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 101/599 (start=6464, end=6528)\n",
      "2018-11-27T04:45:08.422862: step 4894, loss 0.496158, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 102/599 (start=6528, end=6592)\n",
      "2018-11-27T04:45:08.747748: step 4895, loss 0.603539, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 103/599 (start=6592, end=6656)\n",
      "2018-11-27T04:45:09.078089: step 4896, loss 0.532991, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 104/599 (start=6656, end=6720)\n",
      "2018-11-27T04:45:09.411945: step 4897, loss 0.28404, acc 1\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 105/599 (start=6720, end=6784)\n",
      "2018-11-27T04:45:09.764521: step 4898, loss 0.580743, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 106/599 (start=6784, end=6848)\n",
      "2018-11-27T04:45:10.101680: step 4899, loss 0.440915, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 107/599 (start=6848, end=6912)\n",
      "2018-11-27T04:45:10.425346: step 4900, loss 0.551146, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:45:16.813523: step 4900, loss 1.96389, acc 0.510775\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-4900\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 108/599 (start=6912, end=6976)\n",
      "2018-11-27T04:45:17.586957: step 4901, loss 0.442188, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 109/599 (start=6976, end=7040)\n",
      "2018-11-27T04:45:17.920906: step 4902, loss 0.597972, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 110/599 (start=7040, end=7104)\n",
      "2018-11-27T04:45:18.241619: step 4903, loss 0.6458, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 111/599 (start=7104, end=7168)\n",
      "2018-11-27T04:45:18.558581: step 4904, loss 0.576724, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 112/599 (start=7168, end=7232)\n",
      "2018-11-27T04:45:18.896192: step 4905, loss 0.617323, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 113/599 (start=7232, end=7296)\n",
      "2018-11-27T04:45:19.240922: step 4906, loss 0.688371, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 114/599 (start=7296, end=7360)\n",
      "2018-11-27T04:45:19.560100: step 4907, loss 0.48125, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 115/599 (start=7360, end=7424)\n",
      "2018-11-27T04:45:19.881493: step 4908, loss 0.556317, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 116/599 (start=7424, end=7488)\n",
      "2018-11-27T04:45:20.192099: step 4909, loss 0.447094, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 117/599 (start=7488, end=7552)\n",
      "2018-11-27T04:45:20.518707: step 4910, loss 0.683424, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 118/599 (start=7552, end=7616)\n",
      "2018-11-27T04:45:20.860145: step 4911, loss 0.60825, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 119/599 (start=7616, end=7680)\n",
      "2018-11-27T04:45:21.203273: step 4912, loss 0.477922, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 120/599 (start=7680, end=7744)\n",
      "2018-11-27T04:45:21.548398: step 4913, loss 0.471205, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 121/599 (start=7744, end=7808)\n",
      "2018-11-27T04:45:21.882425: step 4914, loss 0.72506, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 122/599 (start=7808, end=7872)\n",
      "2018-11-27T04:45:22.203997: step 4915, loss 0.586026, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 123/599 (start=7872, end=7936)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:45:22.526426: step 4916, loss 0.324227, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 124/599 (start=7936, end=8000)\n",
      "2018-11-27T04:45:22.857018: step 4917, loss 0.634445, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 125/599 (start=8000, end=8064)\n",
      "2018-11-27T04:45:23.197346: step 4918, loss 0.616424, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 126/599 (start=8064, end=8128)\n",
      "2018-11-27T04:45:23.534273: step 4919, loss 0.524345, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 127/599 (start=8128, end=8192)\n",
      "2018-11-27T04:45:23.877290: step 4920, loss 0.56326, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 128/599 (start=8192, end=8256)\n",
      "2018-11-27T04:45:24.209743: step 4921, loss 0.440689, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 129/599 (start=8256, end=8320)\n",
      "2018-11-27T04:45:24.548749: step 4922, loss 0.571673, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 130/599 (start=8320, end=8384)\n",
      "2018-11-27T04:45:24.886967: step 4923, loss 0.421016, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 131/599 (start=8384, end=8448)\n",
      "2018-11-27T04:45:25.208514: step 4924, loss 0.70899, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 132/599 (start=8448, end=8512)\n",
      "2018-11-27T04:45:25.542374: step 4925, loss 0.579195, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 133/599 (start=8512, end=8576)\n",
      "2018-11-27T04:45:25.875473: step 4926, loss 0.480739, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 134/599 (start=8576, end=8640)\n",
      "2018-11-27T04:45:26.211344: step 4927, loss 0.41359, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 135/599 (start=8640, end=8704)\n",
      "2018-11-27T04:45:26.551029: step 4928, loss 0.670209, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 136/599 (start=8704, end=8768)\n",
      "2018-11-27T04:45:26.872305: step 4929, loss 0.586855, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 137/599 (start=8768, end=8832)\n",
      "2018-11-27T04:45:27.201666: step 4930, loss 0.602814, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 138/599 (start=8832, end=8896)\n",
      "2018-11-27T04:45:27.539218: step 4931, loss 0.484541, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 139/599 (start=8896, end=8960)\n",
      "2018-11-27T04:45:27.851718: step 4932, loss 0.549895, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 140/599 (start=8960, end=9024)\n",
      "2018-11-27T04:45:28.193008: step 4933, loss 0.617992, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 141/599 (start=9024, end=9088)\n",
      "2018-11-27T04:45:28.534280: step 4934, loss 0.383477, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 142/599 (start=9088, end=9152)\n",
      "2018-11-27T04:45:28.882616: step 4935, loss 0.516919, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 143/599 (start=9152, end=9216)\n",
      "2018-11-27T04:45:29.224102: step 4936, loss 0.652536, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 144/599 (start=9216, end=9280)\n",
      "2018-11-27T04:45:29.558134: step 4937, loss 0.543276, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 145/599 (start=9280, end=9344)\n",
      "2018-11-27T04:45:29.869401: step 4938, loss 0.392437, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 146/599 (start=9344, end=9408)\n",
      "2018-11-27T04:45:30.215431: step 4939, loss 0.476639, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 147/599 (start=9408, end=9472)\n",
      "2018-11-27T04:45:30.563617: step 4940, loss 0.547317, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 148/599 (start=9472, end=9536)\n",
      "2018-11-27T04:45:30.912712: step 4941, loss 0.577412, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 149/599 (start=9536, end=9600)\n",
      "2018-11-27T04:45:31.248483: step 4942, loss 0.564885, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 150/599 (start=9600, end=9664)\n",
      "2018-11-27T04:45:31.568539: step 4943, loss 0.667652, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 151/599 (start=9664, end=9728)\n",
      "2018-11-27T04:45:31.914081: step 4944, loss 0.528219, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 152/599 (start=9728, end=9792)\n",
      "2018-11-27T04:45:32.248814: step 4945, loss 0.572658, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 153/599 (start=9792, end=9856)\n",
      "2018-11-27T04:45:32.594202: step 4946, loss 0.689402, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 154/599 (start=9856, end=9920)\n",
      "2018-11-27T04:45:32.930868: step 4947, loss 0.494655, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 155/599 (start=9920, end=9984)\n",
      "2018-11-27T04:45:33.263308: step 4948, loss 0.574561, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 156/599 (start=9984, end=10048)\n",
      "2018-11-27T04:45:33.606149: step 4949, loss 0.389562, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 157/599 (start=10048, end=10112)\n",
      "2018-11-27T04:45:33.941819: step 4950, loss 0.488768, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 158/599 (start=10112, end=10176)\n",
      "2018-11-27T04:45:34.283340: step 4951, loss 0.474574, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 159/599 (start=10176, end=10240)\n",
      "2018-11-27T04:45:34.617380: step 4952, loss 0.526718, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 160/599 (start=10240, end=10304)\n",
      "2018-11-27T04:45:34.928389: step 4953, loss 0.467383, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 161/599 (start=10304, end=10368)\n",
      "2018-11-27T04:45:35.246058: step 4954, loss 0.510329, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 162/599 (start=10368, end=10432)\n",
      "2018-11-27T04:45:35.555102: step 4955, loss 0.522433, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 163/599 (start=10432, end=10496)\n",
      "2018-11-27T04:45:35.864181: step 4956, loss 0.385823, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 164/599 (start=10496, end=10560)\n",
      "2018-11-27T04:45:36.202010: step 4957, loss 0.439078, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 165/599 (start=10560, end=10624)\n",
      "2018-11-27T04:45:36.515461: step 4958, loss 0.78341, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 166/599 (start=10624, end=10688)\n",
      "2018-11-27T04:45:36.827104: step 4959, loss 0.523177, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 167/599 (start=10688, end=10752)\n",
      "2018-11-27T04:45:37.159593: step 4960, loss 0.417633, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 168/599 (start=10752, end=10816)\n",
      "2018-11-27T04:45:37.479371: step 4961, loss 0.369151, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 169/599 (start=10816, end=10880)\n",
      "2018-11-27T04:45:37.802817: step 4962, loss 0.560147, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 170/599 (start=10880, end=10944)\n",
      "2018-11-27T04:45:38.106344: step 4963, loss 0.525735, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 171/599 (start=10944, end=11008)\n",
      "2018-11-27T04:45:38.455346: step 4964, loss 0.519504, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 172/599 (start=11008, end=11072)\n",
      "2018-11-27T04:45:38.805437: step 4965, loss 0.525003, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 173/599 (start=11072, end=11136)\n",
      "2018-11-27T04:45:39.120526: step 4966, loss 0.458607, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 174/599 (start=11136, end=11200)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:45:39.451987: step 4967, loss 0.52728, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 175/599 (start=11200, end=11264)\n",
      "2018-11-27T04:45:39.804846: step 4968, loss 0.379859, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 176/599 (start=11264, end=11328)\n",
      "2018-11-27T04:45:40.148766: step 4969, loss 0.485873, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 177/599 (start=11328, end=11392)\n",
      "2018-11-27T04:45:40.481616: step 4970, loss 0.669812, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 178/599 (start=11392, end=11456)\n",
      "2018-11-27T04:45:40.800973: step 4971, loss 0.533117, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 179/599 (start=11456, end=11520)\n",
      "2018-11-27T04:45:41.133981: step 4972, loss 0.323411, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 180/599 (start=11520, end=11584)\n",
      "2018-11-27T04:45:41.478852: step 4973, loss 0.431866, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 181/599 (start=11584, end=11648)\n",
      "2018-11-27T04:45:41.813146: step 4974, loss 0.436384, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 182/599 (start=11648, end=11712)\n",
      "2018-11-27T04:45:42.160637: step 4975, loss 0.600073, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 183/599 (start=11712, end=11776)\n",
      "2018-11-27T04:45:42.505204: step 4976, loss 0.670175, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 184/599 (start=11776, end=11840)\n",
      "2018-11-27T04:45:42.853995: step 4977, loss 0.710632, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 185/599 (start=11840, end=11904)\n",
      "2018-11-27T04:45:43.209769: step 4978, loss 0.613355, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 186/599 (start=11904, end=11968)\n",
      "2018-11-27T04:45:43.529130: step 4979, loss 0.345852, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 187/599 (start=11968, end=12032)\n",
      "2018-11-27T04:45:43.877060: step 4980, loss 0.450123, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 188/599 (start=12032, end=12096)\n",
      "2018-11-27T04:45:44.208872: step 4981, loss 0.56584, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 189/599 (start=12096, end=12160)\n",
      "2018-11-27T04:45:44.529662: step 4982, loss 0.680175, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 190/599 (start=12160, end=12224)\n",
      "2018-11-27T04:45:44.878416: step 4983, loss 0.60937, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 191/599 (start=12224, end=12288)\n",
      "2018-11-27T04:45:45.216623: step 4984, loss 0.561628, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 192/599 (start=12288, end=12352)\n",
      "2018-11-27T04:45:45.529899: step 4985, loss 0.596863, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 193/599 (start=12352, end=12416)\n",
      "2018-11-27T04:45:45.859230: step 4986, loss 0.437195, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 194/599 (start=12416, end=12480)\n",
      "2018-11-27T04:45:46.184512: step 4987, loss 0.433333, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 195/599 (start=12480, end=12544)\n",
      "2018-11-27T04:45:46.506565: step 4988, loss 0.446991, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 196/599 (start=12544, end=12608)\n",
      "2018-11-27T04:45:46.832031: step 4989, loss 0.590998, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 197/599 (start=12608, end=12672)\n",
      "2018-11-27T04:45:47.165317: step 4990, loss 0.446105, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 198/599 (start=12672, end=12736)\n",
      "2018-11-27T04:45:47.505323: step 4991, loss 0.539586, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 199/599 (start=12736, end=12800)\n",
      "2018-11-27T04:45:47.834762: step 4992, loss 0.537115, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 200/599 (start=12800, end=12864)\n",
      "2018-11-27T04:45:48.169845: step 4993, loss 0.471144, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 201/599 (start=12864, end=12928)\n",
      "2018-11-27T04:45:48.494706: step 4994, loss 0.51755, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 202/599 (start=12928, end=12992)\n",
      "2018-11-27T04:45:48.830838: step 4995, loss 0.572343, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 203/599 (start=12992, end=13056)\n",
      "2018-11-27T04:45:49.149626: step 4996, loss 0.578431, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 204/599 (start=13056, end=13120)\n",
      "2018-11-27T04:45:49.488137: step 4997, loss 0.521664, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 205/599 (start=13120, end=13184)\n",
      "2018-11-27T04:45:49.830419: step 4998, loss 0.787752, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 206/599 (start=13184, end=13248)\n",
      "2018-11-27T04:45:50.174013: step 4999, loss 0.419387, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 207/599 (start=13248, end=13312)\n",
      "2018-11-27T04:45:50.525516: step 5000, loss 0.4245, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:45:56.748963: step 5000, loss 1.96716, acc 0.513518\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-5000\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 208/599 (start=13312, end=13376)\n",
      "2018-11-27T04:45:57.503197: step 5001, loss 0.42463, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 209/599 (start=13376, end=13440)\n",
      "2018-11-27T04:45:57.820767: step 5002, loss 0.741618, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 210/599 (start=13440, end=13504)\n",
      "2018-11-27T04:45:58.175283: step 5003, loss 0.471399, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 211/599 (start=13504, end=13568)\n",
      "2018-11-27T04:45:58.514362: step 5004, loss 0.58712, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 212/599 (start=13568, end=13632)\n",
      "2018-11-27T04:45:58.832153: step 5005, loss 0.440431, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 213/599 (start=13632, end=13696)\n",
      "2018-11-27T04:45:59.148259: step 5006, loss 0.340706, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 214/599 (start=13696, end=13760)\n",
      "2018-11-27T04:45:59.484435: step 5007, loss 0.54596, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 215/599 (start=13760, end=13824)\n",
      "2018-11-27T04:45:59.800043: step 5008, loss 0.572023, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 216/599 (start=13824, end=13888)\n",
      "2018-11-27T04:46:00.132595: step 5009, loss 0.501047, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 217/599 (start=13888, end=13952)\n",
      "2018-11-27T04:46:00.457938: step 5010, loss 0.581274, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 218/599 (start=13952, end=14016)\n",
      "2018-11-27T04:46:00.797454: step 5011, loss 0.447478, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 219/599 (start=14016, end=14080)\n",
      "2018-11-27T04:46:01.133120: step 5012, loss 0.435357, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 220/599 (start=14080, end=14144)\n",
      "2018-11-27T04:46:01.476417: step 5013, loss 0.596463, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 221/599 (start=14144, end=14208)\n",
      "2018-11-27T04:46:01.815903: step 5014, loss 0.636199, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 222/599 (start=14208, end=14272)\n",
      "2018-11-27T04:46:02.148313: step 5015, loss 0.696757, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 223/599 (start=14272, end=14336)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:46:02.470514: step 5016, loss 0.496047, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 224/599 (start=14336, end=14400)\n",
      "2018-11-27T04:46:02.787921: step 5017, loss 0.850391, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 225/599 (start=14400, end=14464)\n",
      "2018-11-27T04:46:03.127424: step 5018, loss 0.340306, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 226/599 (start=14464, end=14528)\n",
      "2018-11-27T04:46:03.463962: step 5019, loss 0.696398, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 227/599 (start=14528, end=14592)\n",
      "2018-11-27T04:46:03.819391: step 5020, loss 0.482699, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 228/599 (start=14592, end=14656)\n",
      "2018-11-27T04:46:04.141121: step 5021, loss 0.506635, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 229/599 (start=14656, end=14720)\n",
      "2018-11-27T04:46:04.450720: step 5022, loss 0.5405, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 230/599 (start=14720, end=14784)\n",
      "2018-11-27T04:46:04.763673: step 5023, loss 0.659367, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 231/599 (start=14784, end=14848)\n",
      "2018-11-27T04:46:05.106735: step 5024, loss 0.642605, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 232/599 (start=14848, end=14912)\n",
      "2018-11-27T04:46:05.422491: step 5025, loss 0.498075, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 233/599 (start=14912, end=14976)\n",
      "2018-11-27T04:46:05.758596: step 5026, loss 0.360782, acc 0.984375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 234/599 (start=14976, end=15040)\n",
      "2018-11-27T04:46:06.114044: step 5027, loss 0.396056, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 235/599 (start=15040, end=15104)\n",
      "2018-11-27T04:46:06.459521: step 5028, loss 0.40358, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 236/599 (start=15104, end=15168)\n",
      "2018-11-27T04:46:06.804583: step 5029, loss 0.645602, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 237/599 (start=15168, end=15232)\n",
      "2018-11-27T04:46:07.144849: step 5030, loss 0.47225, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 238/599 (start=15232, end=15296)\n",
      "2018-11-27T04:46:07.485529: step 5031, loss 0.580335, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 239/599 (start=15296, end=15360)\n",
      "2018-11-27T04:46:07.819026: step 5032, loss 0.511411, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 240/599 (start=15360, end=15424)\n",
      "2018-11-27T04:46:08.139923: step 5033, loss 0.396556, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 241/599 (start=15424, end=15488)\n",
      "2018-11-27T04:46:08.457252: step 5034, loss 0.488267, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 242/599 (start=15488, end=15552)\n",
      "2018-11-27T04:46:08.780922: step 5035, loss 0.41919, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 243/599 (start=15552, end=15616)\n",
      "2018-11-27T04:46:09.115012: step 5036, loss 0.393315, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 244/599 (start=15616, end=15680)\n",
      "2018-11-27T04:46:09.430177: step 5037, loss 0.746889, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 245/599 (start=15680, end=15744)\n",
      "2018-11-27T04:46:09.752809: step 5038, loss 0.488425, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 246/599 (start=15744, end=15808)\n",
      "2018-11-27T04:46:10.071148: step 5039, loss 0.617633, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 247/599 (start=15808, end=15872)\n",
      "2018-11-27T04:46:10.406553: step 5040, loss 0.338724, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 248/599 (start=15872, end=15936)\n",
      "2018-11-27T04:46:10.714865: step 5041, loss 0.492214, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 249/599 (start=15936, end=16000)\n",
      "2018-11-27T04:46:11.039423: step 5042, loss 0.595425, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 250/599 (start=16000, end=16064)\n",
      "2018-11-27T04:46:11.377428: step 5043, loss 0.701173, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 251/599 (start=16064, end=16128)\n",
      "2018-11-27T04:46:11.687871: step 5044, loss 0.581582, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 252/599 (start=16128, end=16192)\n",
      "2018-11-27T04:46:12.026279: step 5045, loss 0.670954, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 253/599 (start=16192, end=16256)\n",
      "2018-11-27T04:46:12.352557: step 5046, loss 0.440697, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 254/599 (start=16256, end=16320)\n",
      "2018-11-27T04:46:12.692956: step 5047, loss 0.562614, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 255/599 (start=16320, end=16384)\n",
      "2018-11-27T04:46:13.025415: step 5048, loss 0.365476, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 256/599 (start=16384, end=16448)\n",
      "2018-11-27T04:46:13.363510: step 5049, loss 0.677462, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 257/599 (start=16448, end=16512)\n",
      "2018-11-27T04:46:13.700586: step 5050, loss 0.556507, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 258/599 (start=16512, end=16576)\n",
      "2018-11-27T04:46:14.031323: step 5051, loss 0.389318, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 259/599 (start=16576, end=16640)\n",
      "2018-11-27T04:46:14.358162: step 5052, loss 0.704366, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 260/599 (start=16640, end=16704)\n",
      "2018-11-27T04:46:14.684680: step 5053, loss 0.693846, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 261/599 (start=16704, end=16768)\n",
      "2018-11-27T04:46:15.017117: step 5054, loss 0.457091, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 262/599 (start=16768, end=16832)\n",
      "2018-11-27T04:46:15.331013: step 5055, loss 0.601579, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 263/599 (start=16832, end=16896)\n",
      "2018-11-27T04:46:15.663428: step 5056, loss 0.326893, acc 0.984375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 264/599 (start=16896, end=16960)\n",
      "2018-11-27T04:46:15.992442: step 5057, loss 0.593474, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 265/599 (start=16960, end=17024)\n",
      "2018-11-27T04:46:16.313986: step 5058, loss 0.58299, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 266/599 (start=17024, end=17088)\n",
      "2018-11-27T04:46:16.650037: step 5059, loss 0.665574, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 267/599 (start=17088, end=17152)\n",
      "2018-11-27T04:46:16.970926: step 5060, loss 0.500838, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 268/599 (start=17152, end=17216)\n",
      "2018-11-27T04:46:17.291388: step 5061, loss 0.725352, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 269/599 (start=17216, end=17280)\n",
      "2018-11-27T04:46:17.628264: step 5062, loss 0.499219, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 270/599 (start=17280, end=17344)\n",
      "2018-11-27T04:46:17.962891: step 5063, loss 0.44474, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 271/599 (start=17344, end=17408)\n",
      "2018-11-27T04:46:18.294896: step 5064, loss 0.522602, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 272/599 (start=17408, end=17472)\n",
      "2018-11-27T04:46:18.623913: step 5065, loss 0.593531, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 273/599 (start=17472, end=17536)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:46:18.957165: step 5066, loss 0.773112, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 274/599 (start=17536, end=17600)\n",
      "2018-11-27T04:46:19.303510: step 5067, loss 0.496352, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 275/599 (start=17600, end=17664)\n",
      "2018-11-27T04:46:19.644086: step 5068, loss 0.439382, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 276/599 (start=17664, end=17728)\n",
      "2018-11-27T04:46:19.969401: step 5069, loss 0.48491, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 277/599 (start=17728, end=17792)\n",
      "2018-11-27T04:46:20.304273: step 5070, loss 0.533793, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 278/599 (start=17792, end=17856)\n",
      "2018-11-27T04:46:20.643736: step 5071, loss 0.391026, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 279/599 (start=17856, end=17920)\n",
      "2018-11-27T04:46:20.994411: step 5072, loss 0.535695, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 280/599 (start=17920, end=17984)\n",
      "2018-11-27T04:46:21.322355: step 5073, loss 0.521413, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 281/599 (start=17984, end=18048)\n",
      "2018-11-27T04:46:21.660460: step 5074, loss 0.433973, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 282/599 (start=18048, end=18112)\n",
      "2018-11-27T04:46:21.979041: step 5075, loss 0.59301, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 283/599 (start=18112, end=18176)\n",
      "2018-11-27T04:46:22.330894: step 5076, loss 0.437796, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 284/599 (start=18176, end=18240)\n",
      "2018-11-27T04:46:22.643584: step 5077, loss 0.717031, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 285/599 (start=18240, end=18304)\n",
      "2018-11-27T04:46:22.994611: step 5078, loss 0.618868, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 286/599 (start=18304, end=18368)\n",
      "2018-11-27T04:46:23.358029: step 5079, loss 0.509196, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 287/599 (start=18368, end=18432)\n",
      "2018-11-27T04:46:23.702440: step 5080, loss 0.39376, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 288/599 (start=18432, end=18496)\n",
      "2018-11-27T04:46:24.024874: step 5081, loss 0.58975, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 289/599 (start=18496, end=18560)\n",
      "2018-11-27T04:46:24.367294: step 5082, loss 0.405957, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 290/599 (start=18560, end=18624)\n",
      "2018-11-27T04:46:24.693932: step 5083, loss 0.748551, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 291/599 (start=18624, end=18688)\n",
      "2018-11-27T04:46:25.018915: step 5084, loss 0.44829, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 292/599 (start=18688, end=18752)\n",
      "2018-11-27T04:46:25.358990: step 5085, loss 0.494567, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 293/599 (start=18752, end=18816)\n",
      "2018-11-27T04:46:25.705001: step 5086, loss 0.513578, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 294/599 (start=18816, end=18880)\n",
      "2018-11-27T04:46:26.031441: step 5087, loss 0.432486, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 295/599 (start=18880, end=18944)\n",
      "2018-11-27T04:46:26.369924: step 5088, loss 0.601812, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 296/599 (start=18944, end=19008)\n",
      "2018-11-27T04:46:26.705086: step 5089, loss 0.502689, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 297/599 (start=19008, end=19072)\n",
      "2018-11-27T04:46:27.039877: step 5090, loss 0.471896, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 298/599 (start=19072, end=19136)\n",
      "2018-11-27T04:46:27.372265: step 5091, loss 0.492104, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 299/599 (start=19136, end=19200)\n",
      "2018-11-27T04:46:27.737385: step 5092, loss 0.663372, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 300/599 (start=19200, end=19264)\n",
      "2018-11-27T04:46:28.077076: step 5093, loss 0.419883, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 301/599 (start=19264, end=19328)\n",
      "2018-11-27T04:46:28.390756: step 5094, loss 0.65944, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 302/599 (start=19328, end=19392)\n",
      "2018-11-27T04:46:28.701780: step 5095, loss 0.434703, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 303/599 (start=19392, end=19456)\n",
      "2018-11-27T04:46:29.042822: step 5096, loss 0.57603, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 304/599 (start=19456, end=19520)\n",
      "2018-11-27T04:46:29.365379: step 5097, loss 0.483012, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 305/599 (start=19520, end=19584)\n",
      "2018-11-27T04:46:29.684415: step 5098, loss 0.334686, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 306/599 (start=19584, end=19648)\n",
      "2018-11-27T04:46:30.013754: step 5099, loss 0.379932, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 307/599 (start=19648, end=19712)\n",
      "2018-11-27T04:46:30.348913: step 5100, loss 0.349488, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:46:36.600620: step 5100, loss 1.95387, acc 0.503879\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-5100\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 308/599 (start=19712, end=19776)\n",
      "2018-11-27T04:46:37.346427: step 5101, loss 0.59738, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 309/599 (start=19776, end=19840)\n",
      "2018-11-27T04:46:37.684431: step 5102, loss 0.546872, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 310/599 (start=19840, end=19904)\n",
      "2018-11-27T04:46:38.026479: step 5103, loss 0.668768, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 311/599 (start=19904, end=19968)\n",
      "2018-11-27T04:46:38.348469: step 5104, loss 0.589238, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 312/599 (start=19968, end=20032)\n",
      "2018-11-27T04:46:38.672124: step 5105, loss 0.578936, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 313/599 (start=20032, end=20096)\n",
      "2018-11-27T04:46:38.995322: step 5106, loss 0.606005, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 314/599 (start=20096, end=20160)\n",
      "2018-11-27T04:46:39.330045: step 5107, loss 0.769089, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 315/599 (start=20160, end=20224)\n",
      "2018-11-27T04:46:39.665446: step 5108, loss 0.573002, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 316/599 (start=20224, end=20288)\n",
      "2018-11-27T04:46:40.010868: step 5109, loss 0.504216, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 317/599 (start=20288, end=20352)\n",
      "2018-11-27T04:46:40.360007: step 5110, loss 0.536717, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 318/599 (start=20352, end=20416)\n",
      "2018-11-27T04:46:40.695405: step 5111, loss 0.629214, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 319/599 (start=20416, end=20480)\n",
      "2018-11-27T04:46:41.021333: step 5112, loss 0.528801, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 320/599 (start=20480, end=20544)\n",
      "2018-11-27T04:46:41.355905: step 5113, loss 0.65723, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 321/599 (start=20544, end=20608)\n",
      "2018-11-27T04:46:41.696430: step 5114, loss 0.551245, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 322/599 (start=20608, end=20672)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:46:42.019169: step 5115, loss 0.410462, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 323/599 (start=20672, end=20736)\n",
      "2018-11-27T04:46:42.385901: step 5116, loss 0.562356, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 324/599 (start=20736, end=20800)\n",
      "2018-11-27T04:46:42.719050: step 5117, loss 0.609326, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 325/599 (start=20800, end=20864)\n",
      "2018-11-27T04:46:43.046102: step 5118, loss 0.555538, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 326/599 (start=20864, end=20928)\n",
      "2018-11-27T04:46:43.385589: step 5119, loss 0.38188, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 327/599 (start=20928, end=20992)\n",
      "2018-11-27T04:46:43.698906: step 5120, loss 0.50053, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 328/599 (start=20992, end=21056)\n",
      "2018-11-27T04:46:44.014801: step 5121, loss 0.545727, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 329/599 (start=21056, end=21120)\n",
      "2018-11-27T04:46:44.350766: step 5122, loss 0.486872, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 330/599 (start=21120, end=21184)\n",
      "2018-11-27T04:46:44.685378: step 5123, loss 0.490707, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 331/599 (start=21184, end=21248)\n",
      "2018-11-27T04:46:45.007002: step 5124, loss 0.510924, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 332/599 (start=21248, end=21312)\n",
      "2018-11-27T04:46:45.330281: step 5125, loss 0.584545, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 333/599 (start=21312, end=21376)\n",
      "2018-11-27T04:46:45.663306: step 5126, loss 0.48281, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 334/599 (start=21376, end=21440)\n",
      "2018-11-27T04:46:45.998723: step 5127, loss 0.505088, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 335/599 (start=21440, end=21504)\n",
      "2018-11-27T04:46:46.320001: step 5128, loss 0.50789, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 336/599 (start=21504, end=21568)\n",
      "2018-11-27T04:46:46.664404: step 5129, loss 0.444312, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 337/599 (start=21568, end=21632)\n",
      "2018-11-27T04:46:47.003402: step 5130, loss 0.502776, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 338/599 (start=21632, end=21696)\n",
      "2018-11-27T04:46:47.332229: step 5131, loss 0.48553, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 339/599 (start=21696, end=21760)\n",
      "2018-11-27T04:46:47.645975: step 5132, loss 0.593606, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 340/599 (start=21760, end=21824)\n",
      "2018-11-27T04:46:47.957125: step 5133, loss 0.583579, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 341/599 (start=21824, end=21888)\n",
      "2018-11-27T04:46:48.284198: step 5134, loss 0.707647, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 342/599 (start=21888, end=21952)\n",
      "2018-11-27T04:46:48.630133: step 5135, loss 0.516949, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 343/599 (start=21952, end=22016)\n",
      "2018-11-27T04:46:48.945762: step 5136, loss 0.444247, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 344/599 (start=22016, end=22080)\n",
      "2018-11-27T04:46:49.272187: step 5137, loss 0.584098, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 345/599 (start=22080, end=22144)\n",
      "2018-11-27T04:46:49.607420: step 5138, loss 0.502011, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 346/599 (start=22144, end=22208)\n",
      "2018-11-27T04:46:49.935986: step 5139, loss 0.481787, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 347/599 (start=22208, end=22272)\n",
      "2018-11-27T04:46:50.268531: step 5140, loss 0.562082, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 348/599 (start=22272, end=22336)\n",
      "2018-11-27T04:46:50.610793: step 5141, loss 0.601795, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 349/599 (start=22336, end=22400)\n",
      "2018-11-27T04:46:50.955326: step 5142, loss 0.610414, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 350/599 (start=22400, end=22464)\n",
      "2018-11-27T04:46:51.270711: step 5143, loss 0.456545, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 351/599 (start=22464, end=22528)\n",
      "2018-11-27T04:46:51.581167: step 5144, loss 0.601409, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 352/599 (start=22528, end=22592)\n",
      "2018-11-27T04:46:51.898617: step 5145, loss 0.362908, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 353/599 (start=22592, end=22656)\n",
      "2018-11-27T04:46:52.220502: step 5146, loss 0.506867, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 354/599 (start=22656, end=22720)\n",
      "2018-11-27T04:46:52.548281: step 5147, loss 0.458755, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 355/599 (start=22720, end=22784)\n",
      "2018-11-27T04:46:52.869273: step 5148, loss 0.517219, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 356/599 (start=22784, end=22848)\n",
      "2018-11-27T04:46:53.197324: step 5149, loss 0.722209, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 357/599 (start=22848, end=22912)\n",
      "2018-11-27T04:46:53.529401: step 5150, loss 0.668079, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 358/599 (start=22912, end=22976)\n",
      "2018-11-27T04:46:53.858056: step 5151, loss 0.663012, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 359/599 (start=22976, end=23040)\n",
      "2018-11-27T04:46:54.184831: step 5152, loss 0.673019, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 360/599 (start=23040, end=23104)\n",
      "2018-11-27T04:46:54.530023: step 5153, loss 0.457437, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 361/599 (start=23104, end=23168)\n",
      "2018-11-27T04:46:54.864181: step 5154, loss 0.682753, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 362/599 (start=23168, end=23232)\n",
      "2018-11-27T04:46:55.190968: step 5155, loss 0.427349, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 363/599 (start=23232, end=23296)\n",
      "2018-11-27T04:46:55.528996: step 5156, loss 0.451224, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 364/599 (start=23296, end=23360)\n",
      "2018-11-27T04:46:55.876654: step 5157, loss 0.511807, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 365/599 (start=23360, end=23424)\n",
      "2018-11-27T04:46:56.210265: step 5158, loss 0.526448, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 366/599 (start=23424, end=23488)\n",
      "2018-11-27T04:46:56.544773: step 5159, loss 0.529739, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 367/599 (start=23488, end=23552)\n",
      "2018-11-27T04:46:56.861022: step 5160, loss 0.849954, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 368/599 (start=23552, end=23616)\n",
      "2018-11-27T04:46:57.210541: step 5161, loss 0.581118, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 369/599 (start=23616, end=23680)\n",
      "2018-11-27T04:46:57.527324: step 5162, loss 0.594406, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 370/599 (start=23680, end=23744)\n",
      "2018-11-27T04:46:57.880722: step 5163, loss 0.430736, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 371/599 (start=23744, end=23808)\n",
      "2018-11-27T04:46:58.220628: step 5164, loss 0.710695, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 372/599 (start=23808, end=23872)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:46:58.552748: step 5165, loss 0.49062, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 373/599 (start=23872, end=23936)\n",
      "2018-11-27T04:46:58.863122: step 5166, loss 0.332746, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 374/599 (start=23936, end=24000)\n",
      "2018-11-27T04:46:59.204555: step 5167, loss 0.460272, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 375/599 (start=24000, end=24064)\n",
      "2018-11-27T04:46:59.527559: step 5168, loss 0.48481, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 376/599 (start=24064, end=24128)\n",
      "2018-11-27T04:46:59.870138: step 5169, loss 0.431801, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 377/599 (start=24128, end=24192)\n",
      "2018-11-27T04:47:00.229523: step 5170, loss 0.453565, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 378/599 (start=24192, end=24256)\n",
      "2018-11-27T04:47:00.574143: step 5171, loss 0.499944, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 379/599 (start=24256, end=24320)\n",
      "2018-11-27T04:47:00.917595: step 5172, loss 0.520183, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 380/599 (start=24320, end=24384)\n",
      "2018-11-27T04:47:01.242907: step 5173, loss 0.432237, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 381/599 (start=24384, end=24448)\n",
      "2018-11-27T04:47:01.590255: step 5174, loss 0.378917, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 382/599 (start=24448, end=24512)\n",
      "2018-11-27T04:47:01.935225: step 5175, loss 0.512492, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 383/599 (start=24512, end=24576)\n",
      "2018-11-27T04:47:02.247724: step 5176, loss 0.403885, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 384/599 (start=24576, end=24640)\n",
      "2018-11-27T04:47:02.561052: step 5177, loss 0.36915, acc 0.984375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 385/599 (start=24640, end=24704)\n",
      "2018-11-27T04:47:02.867122: step 5178, loss 0.538764, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 386/599 (start=24704, end=24768)\n",
      "2018-11-27T04:47:03.187273: step 5179, loss 0.581213, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 387/599 (start=24768, end=24832)\n",
      "2018-11-27T04:47:03.494633: step 5180, loss 0.774683, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 388/599 (start=24832, end=24896)\n",
      "2018-11-27T04:47:03.836416: step 5181, loss 0.576505, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 389/599 (start=24896, end=24960)\n",
      "2018-11-27T04:47:04.169907: step 5182, loss 0.479906, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 390/599 (start=24960, end=25024)\n",
      "2018-11-27T04:47:04.480641: step 5183, loss 0.53427, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 391/599 (start=25024, end=25088)\n",
      "2018-11-27T04:47:04.794765: step 5184, loss 0.613486, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 392/599 (start=25088, end=25152)\n",
      "2018-11-27T04:47:05.137950: step 5185, loss 0.432207, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 393/599 (start=25152, end=25216)\n",
      "2018-11-27T04:47:05.492102: step 5186, loss 0.702004, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 394/599 (start=25216, end=25280)\n",
      "2018-11-27T04:47:05.826925: step 5187, loss 0.531427, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 395/599 (start=25280, end=25344)\n",
      "2018-11-27T04:47:06.151040: step 5188, loss 0.402974, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 396/599 (start=25344, end=25408)\n",
      "2018-11-27T04:47:06.494009: step 5189, loss 0.690607, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 397/599 (start=25408, end=25472)\n",
      "2018-11-27T04:47:06.832246: step 5190, loss 0.704225, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 398/599 (start=25472, end=25536)\n",
      "2018-11-27T04:47:07.169524: step 5191, loss 0.698953, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 399/599 (start=25536, end=25600)\n",
      "2018-11-27T04:47:07.488345: step 5192, loss 0.419633, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 400/599 (start=25600, end=25664)\n",
      "2018-11-27T04:47:07.827218: step 5193, loss 0.526891, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 401/599 (start=25664, end=25728)\n",
      "2018-11-27T04:47:08.140480: step 5194, loss 0.502782, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 402/599 (start=25728, end=25792)\n",
      "2018-11-27T04:47:08.470523: step 5195, loss 0.602478, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 403/599 (start=25792, end=25856)\n",
      "2018-11-27T04:47:08.807237: step 5196, loss 0.728301, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 404/599 (start=25856, end=25920)\n",
      "2018-11-27T04:47:09.147951: step 5197, loss 0.630556, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 405/599 (start=25920, end=25984)\n",
      "2018-11-27T04:47:09.471603: step 5198, loss 0.545303, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 406/599 (start=25984, end=26048)\n",
      "2018-11-27T04:47:09.803918: step 5199, loss 0.495356, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 407/599 (start=26048, end=26112)\n",
      "2018-11-27T04:47:10.115958: step 5200, loss 0.617421, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:47:16.232804: step 5200, loss 1.97428, acc 0.510932\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-5200\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 408/599 (start=26112, end=26176)\n",
      "2018-11-27T04:47:16.981856: step 5201, loss 0.750596, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 409/599 (start=26176, end=26240)\n",
      "2018-11-27T04:47:17.311632: step 5202, loss 0.487939, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 410/599 (start=26240, end=26304)\n",
      "2018-11-27T04:47:17.648026: step 5203, loss 0.539856, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 411/599 (start=26304, end=26368)\n",
      "2018-11-27T04:47:17.986439: step 5204, loss 0.516644, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 412/599 (start=26368, end=26432)\n",
      "2018-11-27T04:47:18.329383: step 5205, loss 0.690145, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 413/599 (start=26432, end=26496)\n",
      "2018-11-27T04:47:18.650830: step 5206, loss 0.494439, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 414/599 (start=26496, end=26560)\n",
      "2018-11-27T04:47:18.962710: step 5207, loss 0.548971, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 415/599 (start=26560, end=26624)\n",
      "2018-11-27T04:47:19.275349: step 5208, loss 0.491128, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 416/599 (start=26624, end=26688)\n",
      "2018-11-27T04:47:19.621922: step 5209, loss 0.677855, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 417/599 (start=26688, end=26752)\n",
      "2018-11-27T04:47:19.961331: step 5210, loss 0.56098, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 418/599 (start=26752, end=26816)\n",
      "2018-11-27T04:47:20.273937: step 5211, loss 0.562219, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 419/599 (start=26816, end=26880)\n",
      "2018-11-27T04:47:20.586943: step 5212, loss 0.527424, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 420/599 (start=26880, end=26944)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:47:20.913156: step 5213, loss 0.629124, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 421/599 (start=26944, end=27008)\n",
      "2018-11-27T04:47:21.260373: step 5214, loss 0.429962, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 422/599 (start=27008, end=27072)\n",
      "2018-11-27T04:47:21.593916: step 5215, loss 0.451888, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 423/599 (start=27072, end=27136)\n",
      "2018-11-27T04:47:21.935132: step 5216, loss 0.460955, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 424/599 (start=27136, end=27200)\n",
      "2018-11-27T04:47:22.294132: step 5217, loss 0.666857, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 425/599 (start=27200, end=27264)\n",
      "2018-11-27T04:47:22.639919: step 5218, loss 0.811201, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 426/599 (start=27264, end=27328)\n",
      "2018-11-27T04:47:22.976494: step 5219, loss 0.477005, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 427/599 (start=27328, end=27392)\n",
      "2018-11-27T04:47:23.311852: step 5220, loss 0.35696, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 428/599 (start=27392, end=27456)\n",
      "2018-11-27T04:47:23.654405: step 5221, loss 0.53862, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 429/599 (start=27456, end=27520)\n",
      "2018-11-27T04:47:23.986318: step 5222, loss 0.539378, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 430/599 (start=27520, end=27584)\n",
      "2018-11-27T04:47:24.314231: step 5223, loss 0.604801, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 431/599 (start=27584, end=27648)\n",
      "2018-11-27T04:47:24.623347: step 5224, loss 0.511635, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 432/599 (start=27648, end=27712)\n",
      "2018-11-27T04:47:24.952126: step 5225, loss 0.544374, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 433/599 (start=27712, end=27776)\n",
      "2018-11-27T04:47:25.289392: step 5226, loss 0.658048, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 434/599 (start=27776, end=27840)\n",
      "2018-11-27T04:47:25.630512: step 5227, loss 0.685406, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 435/599 (start=27840, end=27904)\n",
      "2018-11-27T04:47:25.963810: step 5228, loss 0.466059, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 436/599 (start=27904, end=27968)\n",
      "2018-11-27T04:47:26.299423: step 5229, loss 0.492256, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 437/599 (start=27968, end=28032)\n",
      "2018-11-27T04:47:26.617160: step 5230, loss 0.535244, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 438/599 (start=28032, end=28096)\n",
      "2018-11-27T04:47:26.938716: step 5231, loss 0.79604, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 439/599 (start=28096, end=28160)\n",
      "2018-11-27T04:47:27.255983: step 5232, loss 0.566697, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 440/599 (start=28160, end=28224)\n",
      "2018-11-27T04:47:27.569450: step 5233, loss 0.598968, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 441/599 (start=28224, end=28288)\n",
      "2018-11-27T04:47:27.923160: step 5234, loss 0.624947, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 442/599 (start=28288, end=28352)\n",
      "2018-11-27T04:47:28.246451: step 5235, loss 0.570226, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 443/599 (start=28352, end=28416)\n",
      "2018-11-27T04:47:28.582992: step 5236, loss 0.566601, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 444/599 (start=28416, end=28480)\n",
      "2018-11-27T04:47:28.898214: step 5237, loss 0.499958, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 445/599 (start=28480, end=28544)\n",
      "2018-11-27T04:47:29.248592: step 5238, loss 0.533124, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 446/599 (start=28544, end=28608)\n",
      "2018-11-27T04:47:29.566689: step 5239, loss 0.614567, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 447/599 (start=28608, end=28672)\n",
      "2018-11-27T04:47:29.905156: step 5240, loss 0.616821, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 448/599 (start=28672, end=28736)\n",
      "2018-11-27T04:47:30.248777: step 5241, loss 0.581094, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 449/599 (start=28736, end=28800)\n",
      "2018-11-27T04:47:30.564766: step 5242, loss 0.517109, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 450/599 (start=28800, end=28864)\n",
      "2018-11-27T04:47:30.906972: step 5243, loss 0.38293, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 451/599 (start=28864, end=28928)\n",
      "2018-11-27T04:47:31.243865: step 5244, loss 0.57255, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 452/599 (start=28928, end=28992)\n",
      "2018-11-27T04:47:31.588000: step 5245, loss 0.774821, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 453/599 (start=28992, end=29056)\n",
      "2018-11-27T04:47:31.943636: step 5246, loss 0.554391, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 454/599 (start=29056, end=29120)\n",
      "2018-11-27T04:47:32.278761: step 5247, loss 0.60062, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 455/599 (start=29120, end=29184)\n",
      "2018-11-27T04:47:32.617123: step 5248, loss 0.575739, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 456/599 (start=29184, end=29248)\n",
      "2018-11-27T04:47:32.951410: step 5249, loss 0.724377, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 457/599 (start=29248, end=29312)\n",
      "2018-11-27T04:47:33.288458: step 5250, loss 0.612077, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 458/599 (start=29312, end=29376)\n",
      "2018-11-27T04:47:33.597605: step 5251, loss 0.540001, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 459/599 (start=29376, end=29440)\n",
      "2018-11-27T04:47:33.919445: step 5252, loss 0.621964, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 460/599 (start=29440, end=29504)\n",
      "2018-11-27T04:47:34.262469: step 5253, loss 0.39284, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 461/599 (start=29504, end=29568)\n",
      "2018-11-27T04:47:34.594117: step 5254, loss 0.44081, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 462/599 (start=29568, end=29632)\n",
      "2018-11-27T04:47:34.901376: step 5255, loss 0.619488, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 463/599 (start=29632, end=29696)\n",
      "2018-11-27T04:47:35.221710: step 5256, loss 0.771668, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 464/599 (start=29696, end=29760)\n",
      "2018-11-27T04:47:35.559197: step 5257, loss 0.340356, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 465/599 (start=29760, end=29824)\n",
      "2018-11-27T04:47:35.872236: step 5258, loss 0.480342, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 466/599 (start=29824, end=29888)\n",
      "2018-11-27T04:47:36.223934: step 5259, loss 0.446636, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 467/599 (start=29888, end=29952)\n",
      "2018-11-27T04:47:36.549571: step 5260, loss 0.589322, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 468/599 (start=29952, end=30016)\n",
      "2018-11-27T04:47:36.872606: step 5261, loss 0.600197, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 469/599 (start=30016, end=30080)\n",
      "2018-11-27T04:47:37.214822: step 5262, loss 0.506453, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 470/599 (start=30080, end=30144)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:47:37.562539: step 5263, loss 0.41419, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 471/599 (start=30144, end=30208)\n",
      "2018-11-27T04:47:37.915920: step 5264, loss 0.498184, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 472/599 (start=30208, end=30272)\n",
      "2018-11-27T04:47:38.232780: step 5265, loss 0.710273, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 473/599 (start=30272, end=30336)\n",
      "2018-11-27T04:47:38.567888: step 5266, loss 0.396726, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 474/599 (start=30336, end=30400)\n",
      "2018-11-27T04:47:38.908616: step 5267, loss 0.633897, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 475/599 (start=30400, end=30464)\n",
      "2018-11-27T04:47:39.234751: step 5268, loss 0.667654, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 476/599 (start=30464, end=30528)\n",
      "2018-11-27T04:47:39.559980: step 5269, loss 0.605689, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 477/599 (start=30528, end=30592)\n",
      "2018-11-27T04:47:39.899582: step 5270, loss 0.620645, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 478/599 (start=30592, end=30656)\n",
      "2018-11-27T04:47:40.222386: step 5271, loss 0.529022, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 479/599 (start=30656, end=30720)\n",
      "2018-11-27T04:47:40.550656: step 5272, loss 0.37968, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 480/599 (start=30720, end=30784)\n",
      "2018-11-27T04:47:40.880254: step 5273, loss 0.675481, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 481/599 (start=30784, end=30848)\n",
      "2018-11-27T04:47:41.195184: step 5274, loss 0.689599, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 482/599 (start=30848, end=30912)\n",
      "2018-11-27T04:47:41.538326: step 5275, loss 0.767878, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 483/599 (start=30912, end=30976)\n",
      "2018-11-27T04:47:41.869401: step 5276, loss 0.582317, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 484/599 (start=30976, end=31040)\n",
      "2018-11-27T04:47:42.201495: step 5277, loss 0.575932, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 485/599 (start=31040, end=31104)\n",
      "2018-11-27T04:47:42.542696: step 5278, loss 0.498886, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 486/599 (start=31104, end=31168)\n",
      "2018-11-27T04:47:42.882736: step 5279, loss 0.562405, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 487/599 (start=31168, end=31232)\n",
      "2018-11-27T04:47:43.215703: step 5280, loss 0.653626, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 488/599 (start=31232, end=31296)\n",
      "2018-11-27T04:47:43.535029: step 5281, loss 0.555239, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 489/599 (start=31296, end=31360)\n",
      "2018-11-27T04:47:43.878510: step 5282, loss 0.426159, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 490/599 (start=31360, end=31424)\n",
      "2018-11-27T04:47:44.216307: step 5283, loss 0.867318, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 491/599 (start=31424, end=31488)\n",
      "2018-11-27T04:47:44.536121: step 5284, loss 0.662554, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 492/599 (start=31488, end=31552)\n",
      "2018-11-27T04:47:44.862080: step 5285, loss 0.638161, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 493/599 (start=31552, end=31616)\n",
      "2018-11-27T04:47:45.202499: step 5286, loss 0.562549, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 494/599 (start=31616, end=31680)\n",
      "2018-11-27T04:47:45.532745: step 5287, loss 0.551368, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 495/599 (start=31680, end=31744)\n",
      "2018-11-27T04:47:45.866722: step 5288, loss 0.576504, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 496/599 (start=31744, end=31808)\n",
      "2018-11-27T04:47:46.218474: step 5289, loss 0.588507, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 497/599 (start=31808, end=31872)\n",
      "2018-11-27T04:47:46.564967: step 5290, loss 0.358003, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 498/599 (start=31872, end=31936)\n",
      "2018-11-27T04:47:46.903401: step 5291, loss 0.494562, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 499/599 (start=31936, end=32000)\n",
      "2018-11-27T04:47:47.242163: step 5292, loss 0.447001, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 500/599 (start=32000, end=32064)\n",
      "2018-11-27T04:47:47.591220: step 5293, loss 0.472977, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 501/599 (start=32064, end=32128)\n",
      "2018-11-27T04:47:47.925138: step 5294, loss 0.672779, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 502/599 (start=32128, end=32192)\n",
      "2018-11-27T04:47:48.275856: step 5295, loss 0.676818, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 503/599 (start=32192, end=32256)\n",
      "2018-11-27T04:47:48.615686: step 5296, loss 0.466713, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 504/599 (start=32256, end=32320)\n",
      "2018-11-27T04:47:48.956734: step 5297, loss 0.4759, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 505/599 (start=32320, end=32384)\n",
      "2018-11-27T04:47:49.298127: step 5298, loss 0.438834, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 506/599 (start=32384, end=32448)\n",
      "2018-11-27T04:47:49.615631: step 5299, loss 0.814138, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 507/599 (start=32448, end=32512)\n",
      "2018-11-27T04:47:49.945019: step 5300, loss 0.500652, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:47:56.227509: step 5300, loss 1.97479, acc 0.516025\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-5300\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 508/599 (start=32512, end=32576)\n",
      "2018-11-27T04:47:57.004331: step 5301, loss 0.601418, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 509/599 (start=32576, end=32640)\n",
      "2018-11-27T04:47:57.360207: step 5302, loss 0.701402, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 510/599 (start=32640, end=32704)\n",
      "2018-11-27T04:47:57.685979: step 5303, loss 0.833746, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 511/599 (start=32704, end=32768)\n",
      "2018-11-27T04:47:58.023388: step 5304, loss 0.743353, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 512/599 (start=32768, end=32832)\n",
      "2018-11-27T04:47:58.367565: step 5305, loss 0.587122, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 513/599 (start=32832, end=32896)\n",
      "2018-11-27T04:47:58.693861: step 5306, loss 0.718362, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 514/599 (start=32896, end=32960)\n",
      "2018-11-27T04:47:59.024824: step 5307, loss 0.594634, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 515/599 (start=32960, end=33024)\n",
      "2018-11-27T04:47:59.351702: step 5308, loss 0.61651, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 516/599 (start=33024, end=33088)\n",
      "2018-11-27T04:47:59.705610: step 5309, loss 0.504297, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 517/599 (start=33088, end=33152)\n",
      "2018-11-27T04:48:00.047557: step 5310, loss 0.921847, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 518/599 (start=33152, end=33216)\n",
      "2018-11-27T04:48:00.403162: step 5311, loss 0.610078, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 519/599 (start=33216, end=33280)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:48:00.720502: step 5312, loss 0.451957, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 520/599 (start=33280, end=33344)\n",
      "2018-11-27T04:48:01.070858: step 5313, loss 0.486562, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 521/599 (start=33344, end=33408)\n",
      "2018-11-27T04:48:01.415417: step 5314, loss 0.58186, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 522/599 (start=33408, end=33472)\n",
      "2018-11-27T04:48:01.754760: step 5315, loss 0.74332, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 523/599 (start=33472, end=33536)\n",
      "2018-11-27T04:48:02.095511: step 5316, loss 0.559323, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 524/599 (start=33536, end=33600)\n",
      "2018-11-27T04:48:02.435516: step 5317, loss 0.671712, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 525/599 (start=33600, end=33664)\n",
      "2018-11-27T04:48:02.787404: step 5318, loss 0.569641, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 526/599 (start=33664, end=33728)\n",
      "2018-11-27T04:48:03.132426: step 5319, loss 0.571849, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 527/599 (start=33728, end=33792)\n",
      "2018-11-27T04:48:03.441959: step 5320, loss 0.870494, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 528/599 (start=33792, end=33856)\n",
      "2018-11-27T04:48:03.786922: step 5321, loss 0.619118, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 529/599 (start=33856, end=33920)\n",
      "2018-11-27T04:48:04.129517: step 5322, loss 0.422588, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 530/599 (start=33920, end=33984)\n",
      "2018-11-27T04:48:04.455366: step 5323, loss 0.301086, acc 0.984375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 531/599 (start=33984, end=34048)\n",
      "2018-11-27T04:48:04.792326: step 5324, loss 0.572686, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 532/599 (start=34048, end=34112)\n",
      "2018-11-27T04:48:05.109095: step 5325, loss 0.505439, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 533/599 (start=34112, end=34176)\n",
      "2018-11-27T04:48:05.448482: step 5326, loss 0.743647, acc 0.796875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 534/599 (start=34176, end=34240)\n",
      "2018-11-27T04:48:05.792261: step 5327, loss 0.46348, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 535/599 (start=34240, end=34304)\n",
      "2018-11-27T04:48:06.128579: step 5328, loss 0.486775, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 536/599 (start=34304, end=34368)\n",
      "2018-11-27T04:48:06.474776: step 5329, loss 0.928647, acc 0.75\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 537/599 (start=34368, end=34432)\n",
      "2018-11-27T04:48:06.822483: step 5330, loss 0.397618, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 538/599 (start=34432, end=34496)\n",
      "2018-11-27T04:48:07.139647: step 5331, loss 0.73795, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 539/599 (start=34496, end=34560)\n",
      "2018-11-27T04:48:07.487385: step 5332, loss 0.612651, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 540/599 (start=34560, end=34624)\n",
      "2018-11-27T04:48:07.827708: step 5333, loss 0.405309, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 541/599 (start=34624, end=34688)\n",
      "2018-11-27T04:48:08.178616: step 5334, loss 0.601885, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 542/599 (start=34688, end=34752)\n",
      "2018-11-27T04:48:08.498160: step 5335, loss 0.683985, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 543/599 (start=34752, end=34816)\n",
      "2018-11-27T04:48:08.846221: step 5336, loss 0.575962, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 544/599 (start=34816, end=34880)\n",
      "2018-11-27T04:48:09.188834: step 5337, loss 0.497162, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 545/599 (start=34880, end=34944)\n",
      "2018-11-27T04:48:09.528011: step 5338, loss 0.680892, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 546/599 (start=34944, end=35008)\n",
      "2018-11-27T04:48:09.860561: step 5339, loss 0.653415, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 547/599 (start=35008, end=35072)\n",
      "2018-11-27T04:48:10.175574: step 5340, loss 0.510274, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 548/599 (start=35072, end=35136)\n",
      "2018-11-27T04:48:10.517828: step 5341, loss 0.663301, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 549/599 (start=35136, end=35200)\n",
      "2018-11-27T04:48:10.855796: step 5342, loss 0.471138, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 550/599 (start=35200, end=35264)\n",
      "2018-11-27T04:48:11.208470: step 5343, loss 0.72581, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 551/599 (start=35264, end=35328)\n",
      "2018-11-27T04:48:11.568009: step 5344, loss 0.463687, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 552/599 (start=35328, end=35392)\n",
      "2018-11-27T04:48:11.900047: step 5345, loss 0.481304, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 553/599 (start=35392, end=35456)\n",
      "2018-11-27T04:48:12.236148: step 5346, loss 0.500337, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 554/599 (start=35456, end=35520)\n",
      "2018-11-27T04:48:12.576711: step 5347, loss 0.537551, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 555/599 (start=35520, end=35584)\n",
      "2018-11-27T04:48:12.903193: step 5348, loss 0.588807, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 556/599 (start=35584, end=35648)\n",
      "2018-11-27T04:48:13.245221: step 5349, loss 0.801875, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 557/599 (start=35648, end=35712)\n",
      "2018-11-27T04:48:13.598140: step 5350, loss 0.5605, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 558/599 (start=35712, end=35776)\n",
      "2018-11-27T04:48:13.950815: step 5351, loss 0.723585, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 559/599 (start=35776, end=35840)\n",
      "2018-11-27T04:48:14.287834: step 5352, loss 0.769025, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 560/599 (start=35840, end=35904)\n",
      "2018-11-27T04:48:14.629352: step 5353, loss 0.655068, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 561/599 (start=35904, end=35968)\n",
      "2018-11-27T04:48:14.985544: step 5354, loss 0.662743, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 562/599 (start=35968, end=36032)\n",
      "2018-11-27T04:48:15.327703: step 5355, loss 0.719356, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 563/599 (start=36032, end=36096)\n",
      "2018-11-27T04:48:15.643843: step 5356, loss 0.452043, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 564/599 (start=36096, end=36160)\n",
      "2018-11-27T04:48:15.985165: step 5357, loss 0.495169, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 565/599 (start=36160, end=36224)\n",
      "2018-11-27T04:48:16.334564: step 5358, loss 0.593159, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 566/599 (start=36224, end=36288)\n",
      "2018-11-27T04:48:16.664038: step 5359, loss 0.668604, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 567/599 (start=36288, end=36352)\n",
      "2018-11-27T04:48:17.010689: step 5360, loss 0.596268, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 568/599 (start=36352, end=36416)\n",
      "2018-11-27T04:48:17.358783: step 5361, loss 0.6392, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 569/599 (start=36416, end=36480)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:48:17.689910: step 5362, loss 0.580097, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 570/599 (start=36480, end=36544)\n",
      "2018-11-27T04:48:18.014332: step 5363, loss 0.567819, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 571/599 (start=36544, end=36608)\n",
      "2018-11-27T04:48:18.354394: step 5364, loss 0.455375, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 572/599 (start=36608, end=36672)\n",
      "2018-11-27T04:48:18.683040: step 5365, loss 0.548942, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 573/599 (start=36672, end=36736)\n",
      "2018-11-27T04:48:19.047881: step 5366, loss 0.708514, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 574/599 (start=36736, end=36800)\n",
      "2018-11-27T04:48:19.378497: step 5367, loss 0.549254, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 575/599 (start=36800, end=36864)\n",
      "2018-11-27T04:48:19.703080: step 5368, loss 0.479856, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 576/599 (start=36864, end=36928)\n",
      "2018-11-27T04:48:20.047049: step 5369, loss 0.510929, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 577/599 (start=36928, end=36992)\n",
      "2018-11-27T04:48:20.393895: step 5370, loss 0.492042, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 578/599 (start=36992, end=37056)\n",
      "2018-11-27T04:48:20.741128: step 5371, loss 0.360625, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 579/599 (start=37056, end=37120)\n",
      "2018-11-27T04:48:21.075480: step 5372, loss 0.588088, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 580/599 (start=37120, end=37184)\n",
      "2018-11-27T04:48:21.415083: step 5373, loss 0.677846, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 581/599 (start=37184, end=37248)\n",
      "2018-11-27T04:48:21.759047: step 5374, loss 0.414732, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 582/599 (start=37248, end=37312)\n",
      "2018-11-27T04:48:22.081795: step 5375, loss 0.612507, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 583/599 (start=37312, end=37376)\n",
      "2018-11-27T04:48:22.432871: step 5376, loss 0.576344, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 584/599 (start=37376, end=37440)\n",
      "2018-11-27T04:48:22.751944: step 5377, loss 0.518391, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 585/599 (start=37440, end=37504)\n",
      "2018-11-27T04:48:23.073278: step 5378, loss 0.454287, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 586/599 (start=37504, end=37568)\n",
      "2018-11-27T04:48:23.394867: step 5379, loss 0.520895, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 587/599 (start=37568, end=37632)\n",
      "2018-11-27T04:48:23.705958: step 5380, loss 0.604461, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 588/599 (start=37632, end=37696)\n",
      "2018-11-27T04:48:24.052805: step 5381, loss 0.584153, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 589/599 (start=37696, end=37760)\n",
      "2018-11-27T04:48:24.395425: step 5382, loss 0.612358, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 590/599 (start=37760, end=37824)\n",
      "2018-11-27T04:48:24.706669: step 5383, loss 0.433354, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 591/599 (start=37824, end=37888)\n",
      "2018-11-27T04:48:25.022210: step 5384, loss 0.585321, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 592/599 (start=37888, end=37952)\n",
      "2018-11-27T04:48:25.361219: step 5385, loss 0.63677, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 593/599 (start=37952, end=38016)\n",
      "2018-11-27T04:48:25.702167: step 5386, loss 0.756924, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 594/599 (start=38016, end=38080)\n",
      "2018-11-27T04:48:26.060113: step 5387, loss 0.421832, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 595/599 (start=38080, end=38144)\n",
      "2018-11-27T04:48:26.381050: step 5388, loss 0.433252, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 596/599 (start=38144, end=38208)\n",
      "2018-11-27T04:48:26.702673: step 5389, loss 0.581515, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 597/599 (start=38208, end=38272)\n",
      "2018-11-27T04:48:27.021629: step 5390, loss 0.479755, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 8/10, Batch 598/599 (start=38272, end=38281)\n",
      "2018-11-27T04:48:27.242418: step 5391, loss 0.435694, acc 1\n",
      "***********************************************\n",
      "Epoch 9/10\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 0/599 (start=0, end=64)\n",
      "2018-11-27T04:48:27.569917: step 5392, loss 0.412652, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 1/599 (start=64, end=128)\n",
      "2018-11-27T04:48:27.905815: step 5393, loss 0.372557, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 2/599 (start=128, end=192)\n",
      "2018-11-27T04:48:28.247106: step 5394, loss 0.4477, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 3/599 (start=192, end=256)\n",
      "2018-11-27T04:48:28.596510: step 5395, loss 0.518336, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 4/599 (start=256, end=320)\n",
      "2018-11-27T04:48:28.939544: step 5396, loss 0.50794, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 5/599 (start=320, end=384)\n",
      "2018-11-27T04:48:29.281886: step 5397, loss 0.362712, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 6/599 (start=384, end=448)\n",
      "2018-11-27T04:48:29.631268: step 5398, loss 0.468594, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 7/599 (start=448, end=512)\n",
      "2018-11-27T04:48:29.967668: step 5399, loss 0.577413, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 8/599 (start=512, end=576)\n",
      "2018-11-27T04:48:30.306221: step 5400, loss 0.407896, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:48:36.652147: step 5400, loss 1.96917, acc 0.513596\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-5400\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 9/599 (start=576, end=640)\n",
      "2018-11-27T04:48:37.375356: step 5401, loss 0.416837, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 10/599 (start=640, end=704)\n",
      "2018-11-27T04:48:37.715714: step 5402, loss 0.381425, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 11/599 (start=704, end=768)\n",
      "2018-11-27T04:48:38.068417: step 5403, loss 0.416519, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 12/599 (start=768, end=832)\n",
      "2018-11-27T04:48:38.399899: step 5404, loss 0.519258, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 13/599 (start=832, end=896)\n",
      "2018-11-27T04:48:38.726320: step 5405, loss 0.35611, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 14/599 (start=896, end=960)\n",
      "2018-11-27T04:48:39.065783: step 5406, loss 0.491758, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 15/599 (start=960, end=1024)\n",
      "2018-11-27T04:48:39.404553: step 5407, loss 0.507919, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 16/599 (start=1024, end=1088)\n",
      "2018-11-27T04:48:39.748711: step 5408, loss 0.546191, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 17/599 (start=1088, end=1152)\n",
      "2018-11-27T04:48:40.077612: step 5409, loss 0.594951, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 18/599 (start=1152, end=1216)\n",
      "2018-11-27T04:48:40.413981: step 5410, loss 0.38956, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 19/599 (start=1216, end=1280)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:48:40.748936: step 5411, loss 0.432948, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 20/599 (start=1280, end=1344)\n",
      "2018-11-27T04:48:41.091854: step 5412, loss 0.558875, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 21/599 (start=1344, end=1408)\n",
      "2018-11-27T04:48:41.434028: step 5413, loss 0.462312, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 22/599 (start=1408, end=1472)\n",
      "2018-11-27T04:48:41.767099: step 5414, loss 0.467555, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 23/599 (start=1472, end=1536)\n",
      "2018-11-27T04:48:42.101754: step 5415, loss 0.63241, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 24/599 (start=1536, end=1600)\n",
      "2018-11-27T04:48:42.439252: step 5416, loss 0.624519, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 25/599 (start=1600, end=1664)\n",
      "2018-11-27T04:48:42.741836: step 5417, loss 0.508322, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 26/599 (start=1664, end=1728)\n",
      "2018-11-27T04:48:43.054424: step 5418, loss 0.507908, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 27/599 (start=1728, end=1792)\n",
      "2018-11-27T04:48:43.365004: step 5419, loss 0.442906, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 28/599 (start=1792, end=1856)\n",
      "2018-11-27T04:48:43.684058: step 5420, loss 0.570862, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 29/599 (start=1856, end=1920)\n",
      "2018-11-27T04:48:44.027348: step 5421, loss 0.419658, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 30/599 (start=1920, end=1984)\n",
      "2018-11-27T04:48:44.366729: step 5422, loss 0.512368, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 31/599 (start=1984, end=2048)\n",
      "2018-11-27T04:48:44.695477: step 5423, loss 0.424918, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 32/599 (start=2048, end=2112)\n",
      "2018-11-27T04:48:45.035753: step 5424, loss 0.437483, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 33/599 (start=2112, end=2176)\n",
      "2018-11-27T04:48:45.368849: step 5425, loss 0.421797, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 34/599 (start=2176, end=2240)\n",
      "2018-11-27T04:48:45.677918: step 5426, loss 0.363603, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 35/599 (start=2240, end=2304)\n",
      "2018-11-27T04:48:45.998067: step 5427, loss 0.342583, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 36/599 (start=2304, end=2368)\n",
      "2018-11-27T04:48:46.334453: step 5428, loss 0.326132, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 37/599 (start=2368, end=2432)\n",
      "2018-11-27T04:48:46.665007: step 5429, loss 0.525636, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 38/599 (start=2432, end=2496)\n",
      "2018-11-27T04:48:47.007224: step 5430, loss 0.38613, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 39/599 (start=2496, end=2560)\n",
      "2018-11-27T04:48:47.340397: step 5431, loss 0.538323, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 40/599 (start=2560, end=2624)\n",
      "2018-11-27T04:48:47.684539: step 5432, loss 0.377559, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 41/599 (start=2624, end=2688)\n",
      "2018-11-27T04:48:48.015351: step 5433, loss 0.558531, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 42/599 (start=2688, end=2752)\n",
      "2018-11-27T04:48:48.354307: step 5434, loss 0.554431, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 43/599 (start=2752, end=2816)\n",
      "2018-11-27T04:48:48.669239: step 5435, loss 0.417342, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 44/599 (start=2816, end=2880)\n",
      "2018-11-27T04:48:49.009454: step 5436, loss 0.365876, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 45/599 (start=2880, end=2944)\n",
      "2018-11-27T04:48:49.345217: step 5437, loss 0.500473, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 46/599 (start=2944, end=3008)\n",
      "2018-11-27T04:48:49.687877: step 5438, loss 0.524453, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 47/599 (start=3008, end=3072)\n",
      "2018-11-27T04:48:50.011924: step 5439, loss 0.469176, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 48/599 (start=3072, end=3136)\n",
      "2018-11-27T04:48:50.343788: step 5440, loss 0.725108, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 49/599 (start=3136, end=3200)\n",
      "2018-11-27T04:48:50.686754: step 5441, loss 0.397938, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 50/599 (start=3200, end=3264)\n",
      "2018-11-27T04:48:51.020022: step 5442, loss 0.499477, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 51/599 (start=3264, end=3328)\n",
      "2018-11-27T04:48:51.358630: step 5443, loss 0.364012, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 52/599 (start=3328, end=3392)\n",
      "2018-11-27T04:48:51.696266: step 5444, loss 0.575498, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 53/599 (start=3392, end=3456)\n",
      "2018-11-27T04:48:52.038417: step 5445, loss 0.355999, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 54/599 (start=3456, end=3520)\n",
      "2018-11-27T04:48:52.370928: step 5446, loss 0.471803, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 55/599 (start=3520, end=3584)\n",
      "2018-11-27T04:48:52.685330: step 5447, loss 0.411068, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 56/599 (start=3584, end=3648)\n",
      "2018-11-27T04:48:53.022743: step 5448, loss 0.567579, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 57/599 (start=3648, end=3712)\n",
      "2018-11-27T04:48:53.347615: step 5449, loss 0.440293, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 58/599 (start=3712, end=3776)\n",
      "2018-11-27T04:48:53.673734: step 5450, loss 0.41701, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 59/599 (start=3776, end=3840)\n",
      "2018-11-27T04:48:53.991537: step 5451, loss 0.450512, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 60/599 (start=3840, end=3904)\n",
      "2018-11-27T04:48:54.324028: step 5452, loss 0.438085, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 61/599 (start=3904, end=3968)\n",
      "2018-11-27T04:48:54.666718: step 5453, loss 0.563278, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 62/599 (start=3968, end=4032)\n",
      "2018-11-27T04:48:54.987076: step 5454, loss 0.446282, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 63/599 (start=4032, end=4096)\n",
      "2018-11-27T04:48:55.328321: step 5455, loss 0.523677, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 64/599 (start=4096, end=4160)\n",
      "2018-11-27T04:48:55.643916: step 5456, loss 0.49684, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 65/599 (start=4160, end=4224)\n",
      "2018-11-27T04:48:55.968822: step 5457, loss 0.62608, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 66/599 (start=4224, end=4288)\n",
      "2018-11-27T04:48:56.296656: step 5458, loss 0.390591, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 67/599 (start=4288, end=4352)\n",
      "2018-11-27T04:48:56.632287: step 5459, loss 0.43506, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 68/599 (start=4352, end=4416)\n",
      "2018-11-27T04:48:56.950613: step 5460, loss 0.371808, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 69/599 (start=4416, end=4480)\n",
      "2018-11-27T04:48:57.270638: step 5461, loss 0.459975, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 70/599 (start=4480, end=4544)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:48:57.607908: step 5462, loss 0.435887, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 71/599 (start=4544, end=4608)\n",
      "2018-11-27T04:48:57.947119: step 5463, loss 0.479924, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 72/599 (start=4608, end=4672)\n",
      "2018-11-27T04:48:58.272391: step 5464, loss 0.56416, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 73/599 (start=4672, end=4736)\n",
      "2018-11-27T04:48:58.624497: step 5465, loss 0.38902, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 74/599 (start=4736, end=4800)\n",
      "2018-11-27T04:48:58.956849: step 5466, loss 0.567839, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 75/599 (start=4800, end=4864)\n",
      "2018-11-27T04:48:59.300044: step 5467, loss 0.401249, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 76/599 (start=4864, end=4928)\n",
      "2018-11-27T04:48:59.630466: step 5468, loss 0.368435, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 77/599 (start=4928, end=4992)\n",
      "2018-11-27T04:48:59.969659: step 5469, loss 0.441549, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 78/599 (start=4992, end=5056)\n",
      "2018-11-27T04:49:00.313527: step 5470, loss 0.358232, acc 0.984375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 79/599 (start=5056, end=5120)\n",
      "2018-11-27T04:49:00.663283: step 5471, loss 0.509083, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 80/599 (start=5120, end=5184)\n",
      "2018-11-27T04:49:00.975883: step 5472, loss 0.547593, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 81/599 (start=5184, end=5248)\n",
      "2018-11-27T04:49:01.306632: step 5473, loss 0.421969, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 82/599 (start=5248, end=5312)\n",
      "2018-11-27T04:49:01.644625: step 5474, loss 0.410473, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 83/599 (start=5312, end=5376)\n",
      "2018-11-27T04:49:01.975633: step 5475, loss 0.364859, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 84/599 (start=5376, end=5440)\n",
      "2018-11-27T04:49:02.316970: step 5476, loss 0.465811, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 85/599 (start=5440, end=5504)\n",
      "2018-11-27T04:49:02.654619: step 5477, loss 0.468869, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 86/599 (start=5504, end=5568)\n",
      "2018-11-27T04:49:02.982598: step 5478, loss 0.453526, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 87/599 (start=5568, end=5632)\n",
      "2018-11-27T04:49:03.305297: step 5479, loss 0.385927, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 88/599 (start=5632, end=5696)\n",
      "2018-11-27T04:49:03.643507: step 5480, loss 0.523056, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 89/599 (start=5696, end=5760)\n",
      "2018-11-27T04:49:03.988296: step 5481, loss 0.708213, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 90/599 (start=5760, end=5824)\n",
      "2018-11-27T04:49:04.300150: step 5482, loss 0.324956, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 91/599 (start=5824, end=5888)\n",
      "2018-11-27T04:49:04.645818: step 5483, loss 0.420585, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 92/599 (start=5888, end=5952)\n",
      "2018-11-27T04:49:04.955269: step 5484, loss 0.448887, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 93/599 (start=5952, end=6016)\n",
      "2018-11-27T04:49:05.302098: step 5485, loss 0.509453, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 94/599 (start=6016, end=6080)\n",
      "2018-11-27T04:49:05.635698: step 5486, loss 0.514451, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 95/599 (start=6080, end=6144)\n",
      "2018-11-27T04:49:05.973767: step 5487, loss 0.442879, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 96/599 (start=6144, end=6208)\n",
      "2018-11-27T04:49:06.298866: step 5488, loss 0.370895, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 97/599 (start=6208, end=6272)\n",
      "2018-11-27T04:49:06.643760: step 5489, loss 0.457322, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 98/599 (start=6272, end=6336)\n",
      "2018-11-27T04:49:06.982420: step 5490, loss 0.430205, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 99/599 (start=6336, end=6400)\n",
      "2018-11-27T04:49:07.305002: step 5491, loss 0.410984, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 100/599 (start=6400, end=6464)\n",
      "2018-11-27T04:49:07.644325: step 5492, loss 0.577248, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 101/599 (start=6464, end=6528)\n",
      "2018-11-27T04:49:07.980024: step 5493, loss 0.536303, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 102/599 (start=6528, end=6592)\n",
      "2018-11-27T04:49:08.332291: step 5494, loss 0.59495, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 103/599 (start=6592, end=6656)\n",
      "2018-11-27T04:49:08.673812: step 5495, loss 0.423992, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 104/599 (start=6656, end=6720)\n",
      "2018-11-27T04:49:09.012283: step 5496, loss 0.558082, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 105/599 (start=6720, end=6784)\n",
      "2018-11-27T04:49:09.356592: step 5497, loss 0.737835, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 106/599 (start=6784, end=6848)\n",
      "2018-11-27T04:49:09.671835: step 5498, loss 0.373043, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 107/599 (start=6848, end=6912)\n",
      "2018-11-27T04:49:10.013420: step 5499, loss 0.399931, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 108/599 (start=6912, end=6976)\n",
      "2018-11-27T04:49:10.323001: step 5500, loss 0.43555, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:49:16.522149: step 5500, loss 1.99249, acc 0.510853\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-5500\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 109/599 (start=6976, end=7040)\n",
      "2018-11-27T04:49:17.242121: step 5501, loss 0.391439, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 110/599 (start=7040, end=7104)\n",
      "2018-11-27T04:49:17.547148: step 5502, loss 0.716603, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 111/599 (start=7104, end=7168)\n",
      "2018-11-27T04:49:17.892106: step 5503, loss 0.660187, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 112/599 (start=7168, end=7232)\n",
      "2018-11-27T04:49:18.229196: step 5504, loss 0.430176, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 113/599 (start=7232, end=7296)\n",
      "2018-11-27T04:49:18.569322: step 5505, loss 0.621619, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 114/599 (start=7296, end=7360)\n",
      "2018-11-27T04:49:18.912683: step 5506, loss 0.483321, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 115/599 (start=7360, end=7424)\n",
      "2018-11-27T04:49:19.229809: step 5507, loss 0.395534, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 116/599 (start=7424, end=7488)\n",
      "2018-11-27T04:49:19.558560: step 5508, loss 0.531698, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 117/599 (start=7488, end=7552)\n",
      "2018-11-27T04:49:19.898602: step 5509, loss 0.293784, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 118/599 (start=7552, end=7616)\n",
      "2018-11-27T04:49:20.240010: step 5510, loss 0.517745, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 119/599 (start=7616, end=7680)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:49:20.569540: step 5511, loss 0.512773, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 120/599 (start=7680, end=7744)\n",
      "2018-11-27T04:49:20.904194: step 5512, loss 0.792479, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 121/599 (start=7744, end=7808)\n",
      "2018-11-27T04:49:21.243606: step 5513, loss 0.419975, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 122/599 (start=7808, end=7872)\n",
      "2018-11-27T04:49:21.550628: step 5514, loss 0.415779, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 123/599 (start=7872, end=7936)\n",
      "2018-11-27T04:49:21.887199: step 5515, loss 0.563986, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 124/599 (start=7936, end=8000)\n",
      "2018-11-27T04:49:22.203357: step 5516, loss 0.374189, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 125/599 (start=8000, end=8064)\n",
      "2018-11-27T04:49:22.535521: step 5517, loss 0.627527, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 126/599 (start=8064, end=8128)\n",
      "2018-11-27T04:49:22.866070: step 5518, loss 0.671828, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 127/599 (start=8128, end=8192)\n",
      "2018-11-27T04:49:23.199272: step 5519, loss 0.626966, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 128/599 (start=8192, end=8256)\n",
      "2018-11-27T04:49:23.548321: step 5520, loss 0.418127, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 129/599 (start=8256, end=8320)\n",
      "2018-11-27T04:49:23.895954: step 5521, loss 0.449707, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 130/599 (start=8320, end=8384)\n",
      "2018-11-27T04:49:24.216646: step 5522, loss 0.459592, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 131/599 (start=8384, end=8448)\n",
      "2018-11-27T04:49:24.530133: step 5523, loss 0.380552, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 132/599 (start=8448, end=8512)\n",
      "2018-11-27T04:49:24.870786: step 5524, loss 0.622206, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 133/599 (start=8512, end=8576)\n",
      "2018-11-27T04:49:25.207207: step 5525, loss 0.426905, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 134/599 (start=8576, end=8640)\n",
      "2018-11-27T04:49:25.540878: step 5526, loss 0.435797, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 135/599 (start=8640, end=8704)\n",
      "2018-11-27T04:49:25.879750: step 5527, loss 0.523183, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 136/599 (start=8704, end=8768)\n",
      "2018-11-27T04:49:26.218364: step 5528, loss 0.358092, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 137/599 (start=8768, end=8832)\n",
      "2018-11-27T04:49:26.559967: step 5529, loss 0.393692, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 138/599 (start=8832, end=8896)\n",
      "2018-11-27T04:49:26.911559: step 5530, loss 0.45779, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 139/599 (start=8896, end=8960)\n",
      "2018-11-27T04:49:27.223618: step 5531, loss 0.463612, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 140/599 (start=8960, end=9024)\n",
      "2018-11-27T04:49:27.568657: step 5532, loss 0.318863, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 141/599 (start=9024, end=9088)\n",
      "2018-11-27T04:49:27.898887: step 5533, loss 0.389748, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 142/599 (start=9088, end=9152)\n",
      "2018-11-27T04:49:28.239792: step 5534, loss 0.512882, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 143/599 (start=9152, end=9216)\n",
      "2018-11-27T04:49:28.564967: step 5535, loss 0.583164, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 144/599 (start=9216, end=9280)\n",
      "2018-11-27T04:49:28.897642: step 5536, loss 0.526513, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 145/599 (start=9280, end=9344)\n",
      "2018-11-27T04:49:29.208784: step 5537, loss 0.389115, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 146/599 (start=9344, end=9408)\n",
      "2018-11-27T04:49:29.560438: step 5538, loss 0.493618, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 147/599 (start=9408, end=9472)\n",
      "2018-11-27T04:49:29.901767: step 5539, loss 0.447668, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 148/599 (start=9472, end=9536)\n",
      "2018-11-27T04:49:30.230380: step 5540, loss 0.509103, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 149/599 (start=9536, end=9600)\n",
      "2018-11-27T04:49:30.570877: step 5541, loss 0.541651, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 150/599 (start=9600, end=9664)\n",
      "2018-11-27T04:49:30.904588: step 5542, loss 0.552573, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 151/599 (start=9664, end=9728)\n",
      "2018-11-27T04:49:31.231332: step 5543, loss 0.592453, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 152/599 (start=9728, end=9792)\n",
      "2018-11-27T04:49:31.562881: step 5544, loss 0.392663, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 153/599 (start=9792, end=9856)\n",
      "2018-11-27T04:49:31.911043: step 5545, loss 0.468506, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 154/599 (start=9856, end=9920)\n",
      "2018-11-27T04:49:32.241780: step 5546, loss 0.44058, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 155/599 (start=9920, end=9984)\n",
      "2018-11-27T04:49:32.582819: step 5547, loss 0.542084, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 156/599 (start=9984, end=10048)\n",
      "2018-11-27T04:49:32.895040: step 5548, loss 0.401377, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 157/599 (start=10048, end=10112)\n",
      "2018-11-27T04:49:33.230963: step 5549, loss 0.357462, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 158/599 (start=10112, end=10176)\n",
      "2018-11-27T04:49:33.581366: step 5550, loss 0.432092, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 159/599 (start=10176, end=10240)\n",
      "2018-11-27T04:49:33.899685: step 5551, loss 0.719183, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 160/599 (start=10240, end=10304)\n",
      "2018-11-27T04:49:34.215718: step 5552, loss 0.46169, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 161/599 (start=10304, end=10368)\n",
      "2018-11-27T04:49:34.536548: step 5553, loss 0.581435, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 162/599 (start=10368, end=10432)\n",
      "2018-11-27T04:49:34.850008: step 5554, loss 0.574672, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 163/599 (start=10432, end=10496)\n",
      "2018-11-27T04:49:35.161069: step 5555, loss 0.403919, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 164/599 (start=10496, end=10560)\n",
      "2018-11-27T04:49:35.489833: step 5556, loss 0.565242, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 165/599 (start=10560, end=10624)\n",
      "2018-11-27T04:49:35.811455: step 5557, loss 0.50162, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 166/599 (start=10624, end=10688)\n",
      "2018-11-27T04:49:36.154233: step 5558, loss 0.311606, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 167/599 (start=10688, end=10752)\n",
      "2018-11-27T04:49:36.499804: step 5559, loss 0.488986, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 168/599 (start=10752, end=10816)\n",
      "2018-11-27T04:49:36.812201: step 5560, loss 0.505778, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 169/599 (start=10816, end=10880)\n",
      "2018-11-27T04:49:37.164725: step 5561, loss 0.400341, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 170/599 (start=10880, end=10944)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:49:37.486682: step 5562, loss 0.454973, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 171/599 (start=10944, end=11008)\n",
      "2018-11-27T04:49:37.832977: step 5563, loss 0.369297, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 172/599 (start=11008, end=11072)\n",
      "2018-11-27T04:49:38.161368: step 5564, loss 0.316905, acc 0.984375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 173/599 (start=11072, end=11136)\n",
      "2018-11-27T04:49:38.471565: step 5565, loss 0.447937, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 174/599 (start=11136, end=11200)\n",
      "2018-11-27T04:49:38.779476: step 5566, loss 0.546815, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 175/599 (start=11200, end=11264)\n",
      "2018-11-27T04:49:39.112359: step 5567, loss 0.682435, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 176/599 (start=11264, end=11328)\n",
      "2018-11-27T04:49:39.449594: step 5568, loss 0.498359, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 177/599 (start=11328, end=11392)\n",
      "2018-11-27T04:49:39.768630: step 5569, loss 0.568835, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 178/599 (start=11392, end=11456)\n",
      "2018-11-27T04:49:40.102878: step 5570, loss 0.443894, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 179/599 (start=11456, end=11520)\n",
      "2018-11-27T04:49:40.440174: step 5571, loss 0.446237, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 180/599 (start=11520, end=11584)\n",
      "2018-11-27T04:49:40.774798: step 5572, loss 0.531428, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 181/599 (start=11584, end=11648)\n",
      "2018-11-27T04:49:41.111425: step 5573, loss 0.745628, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 182/599 (start=11648, end=11712)\n",
      "2018-11-27T04:49:41.434361: step 5574, loss 0.437833, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 183/599 (start=11712, end=11776)\n",
      "2018-11-27T04:49:41.771575: step 5575, loss 0.663877, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 184/599 (start=11776, end=11840)\n",
      "2018-11-27T04:49:42.115597: step 5576, loss 0.620198, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 185/599 (start=11840, end=11904)\n",
      "2018-11-27T04:49:42.445059: step 5577, loss 0.554376, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 186/599 (start=11904, end=11968)\n",
      "2018-11-27T04:49:42.780527: step 5578, loss 0.540594, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 187/599 (start=11968, end=12032)\n",
      "2018-11-27T04:49:43.142123: step 5579, loss 0.394989, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 188/599 (start=12032, end=12096)\n",
      "2018-11-27T04:49:43.472444: step 5580, loss 0.662339, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 189/599 (start=12096, end=12160)\n",
      "2018-11-27T04:49:43.817965: step 5581, loss 0.369333, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 190/599 (start=12160, end=12224)\n",
      "2018-11-27T04:49:44.166854: step 5582, loss 0.36457, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 191/599 (start=12224, end=12288)\n",
      "2018-11-27T04:49:44.497504: step 5583, loss 0.717719, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 192/599 (start=12288, end=12352)\n",
      "2018-11-27T04:49:44.839940: step 5584, loss 0.318678, acc 0.984375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 193/599 (start=12352, end=12416)\n",
      "2018-11-27T04:49:45.184365: step 5585, loss 0.57632, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 194/599 (start=12416, end=12480)\n",
      "2018-11-27T04:49:45.526826: step 5586, loss 0.322401, acc 0.984375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 195/599 (start=12480, end=12544)\n",
      "2018-11-27T04:49:45.863028: step 5587, loss 0.479906, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 196/599 (start=12544, end=12608)\n",
      "2018-11-27T04:49:46.193926: step 5588, loss 0.275848, acc 0.984375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 197/599 (start=12608, end=12672)\n",
      "2018-11-27T04:49:46.532662: step 5589, loss 0.658561, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 198/599 (start=12672, end=12736)\n",
      "2018-11-27T04:49:46.855711: step 5590, loss 0.548784, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 199/599 (start=12736, end=12800)\n",
      "2018-11-27T04:49:47.188338: step 5591, loss 0.648805, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 200/599 (start=12800, end=12864)\n",
      "2018-11-27T04:49:47.508937: step 5592, loss 0.487731, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 201/599 (start=12864, end=12928)\n",
      "2018-11-27T04:49:47.855812: step 5593, loss 0.805747, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 202/599 (start=12928, end=12992)\n",
      "2018-11-27T04:49:48.197527: step 5594, loss 0.496416, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 203/599 (start=12992, end=13056)\n",
      "2018-11-27T04:49:48.516635: step 5595, loss 0.537714, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 204/599 (start=13056, end=13120)\n",
      "2018-11-27T04:49:48.839505: step 5596, loss 0.51561, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 205/599 (start=13120, end=13184)\n",
      "2018-11-27T04:49:49.166412: step 5597, loss 0.534233, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 206/599 (start=13184, end=13248)\n",
      "2018-11-27T04:49:49.493637: step 5598, loss 0.398285, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 207/599 (start=13248, end=13312)\n",
      "2018-11-27T04:49:49.851031: step 5599, loss 0.739697, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 208/599 (start=13312, end=13376)\n",
      "2018-11-27T04:49:50.196445: step 5600, loss 0.39202, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:49:56.518861: step 5600, loss 1.9931, acc 0.509913\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-5600\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 209/599 (start=13376, end=13440)\n",
      "2018-11-27T04:49:57.262977: step 5601, loss 0.455118, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 210/599 (start=13440, end=13504)\n",
      "2018-11-27T04:49:57.610144: step 5602, loss 0.590517, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 211/599 (start=13504, end=13568)\n",
      "2018-11-27T04:49:57.928023: step 5603, loss 0.498785, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 212/599 (start=13568, end=13632)\n",
      "2018-11-27T04:49:58.248502: step 5604, loss 0.707104, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 213/599 (start=13632, end=13696)\n",
      "2018-11-27T04:49:58.572405: step 5605, loss 0.732613, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 214/599 (start=13696, end=13760)\n",
      "2018-11-27T04:49:58.905265: step 5606, loss 0.499164, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 215/599 (start=13760, end=13824)\n",
      "2018-11-27T04:49:59.216102: step 5607, loss 0.465967, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 216/599 (start=13824, end=13888)\n",
      "2018-11-27T04:49:59.556152: step 5608, loss 0.511288, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 217/599 (start=13888, end=13952)\n",
      "2018-11-27T04:49:59.879336: step 5609, loss 0.304372, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 218/599 (start=13952, end=14016)\n",
      "2018-11-27T04:50:00.211384: step 5610, loss 0.56144, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 219/599 (start=14016, end=14080)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:50:00.550599: step 5611, loss 0.398496, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 220/599 (start=14080, end=14144)\n",
      "2018-11-27T04:50:00.887127: step 5612, loss 0.424465, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 221/599 (start=14144, end=14208)\n",
      "2018-11-27T04:50:01.232954: step 5613, loss 0.41187, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 222/599 (start=14208, end=14272)\n",
      "2018-11-27T04:50:01.564779: step 5614, loss 0.31344, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 223/599 (start=14272, end=14336)\n",
      "2018-11-27T04:50:01.905716: step 5615, loss 0.50482, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 224/599 (start=14336, end=14400)\n",
      "2018-11-27T04:50:02.232478: step 5616, loss 0.542087, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 225/599 (start=14400, end=14464)\n",
      "2018-11-27T04:50:02.548885: step 5617, loss 0.484094, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 226/599 (start=14464, end=14528)\n",
      "2018-11-27T04:50:02.875970: step 5618, loss 0.461196, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 227/599 (start=14528, end=14592)\n",
      "2018-11-27T04:50:03.211742: step 5619, loss 0.365069, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 228/599 (start=14592, end=14656)\n",
      "2018-11-27T04:50:03.559472: step 5620, loss 0.554712, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 229/599 (start=14656, end=14720)\n",
      "2018-11-27T04:50:03.887071: step 5621, loss 0.425687, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 230/599 (start=14720, end=14784)\n",
      "2018-11-27T04:50:04.199270: step 5622, loss 0.347402, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 231/599 (start=14784, end=14848)\n",
      "2018-11-27T04:50:04.537761: step 5623, loss 0.413949, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 232/599 (start=14848, end=14912)\n",
      "2018-11-27T04:50:04.870652: step 5624, loss 0.450154, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 233/599 (start=14912, end=14976)\n",
      "2018-11-27T04:50:05.180896: step 5625, loss 0.517036, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 234/599 (start=14976, end=15040)\n",
      "2018-11-27T04:50:05.510007: step 5626, loss 0.475761, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 235/599 (start=15040, end=15104)\n",
      "2018-11-27T04:50:05.822472: step 5627, loss 0.484545, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 236/599 (start=15104, end=15168)\n",
      "2018-11-27T04:50:06.133896: step 5628, loss 0.55879, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 237/599 (start=15168, end=15232)\n",
      "2018-11-27T04:50:06.462484: step 5629, loss 0.635568, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 238/599 (start=15232, end=15296)\n",
      "2018-11-27T04:50:06.794481: step 5630, loss 0.518561, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 239/599 (start=15296, end=15360)\n",
      "2018-11-27T04:50:07.121984: step 5631, loss 0.5334, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 240/599 (start=15360, end=15424)\n",
      "2018-11-27T04:50:07.431980: step 5632, loss 0.322506, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 241/599 (start=15424, end=15488)\n",
      "2018-11-27T04:50:07.766621: step 5633, loss 0.580452, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 242/599 (start=15488, end=15552)\n",
      "2018-11-27T04:50:08.087314: step 5634, loss 0.427653, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 243/599 (start=15552, end=15616)\n",
      "2018-11-27T04:50:08.418093: step 5635, loss 0.504605, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 244/599 (start=15616, end=15680)\n",
      "2018-11-27T04:50:08.751153: step 5636, loss 0.357715, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 245/599 (start=15680, end=15744)\n",
      "2018-11-27T04:50:09.070959: step 5637, loss 0.439813, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 246/599 (start=15744, end=15808)\n",
      "2018-11-27T04:50:09.415201: step 5638, loss 0.474141, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 247/599 (start=15808, end=15872)\n",
      "2018-11-27T04:50:09.745985: step 5639, loss 0.352184, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 248/599 (start=15872, end=15936)\n",
      "2018-11-27T04:50:10.068963: step 5640, loss 0.532962, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 249/599 (start=15936, end=16000)\n",
      "2018-11-27T04:50:10.408834: step 5641, loss 0.560898, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 250/599 (start=16000, end=16064)\n",
      "2018-11-27T04:50:10.744989: step 5642, loss 0.632165, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 251/599 (start=16064, end=16128)\n",
      "2018-11-27T04:50:11.086774: step 5643, loss 0.464529, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 252/599 (start=16128, end=16192)\n",
      "2018-11-27T04:50:11.426631: step 5644, loss 0.421413, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 253/599 (start=16192, end=16256)\n",
      "2018-11-27T04:50:11.734241: step 5645, loss 0.579322, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 254/599 (start=16256, end=16320)\n",
      "2018-11-27T04:50:12.070061: step 5646, loss 0.55429, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 255/599 (start=16320, end=16384)\n",
      "2018-11-27T04:50:12.400775: step 5647, loss 0.666319, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 256/599 (start=16384, end=16448)\n",
      "2018-11-27T04:50:12.739471: step 5648, loss 0.501257, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 257/599 (start=16448, end=16512)\n",
      "2018-11-27T04:50:13.082633: step 5649, loss 0.578767, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 258/599 (start=16512, end=16576)\n",
      "2018-11-27T04:50:13.398524: step 5650, loss 0.598067, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 259/599 (start=16576, end=16640)\n",
      "2018-11-27T04:50:13.741115: step 5651, loss 0.404038, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 260/599 (start=16640, end=16704)\n",
      "2018-11-27T04:50:14.081830: step 5652, loss 0.32085, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 261/599 (start=16704, end=16768)\n",
      "2018-11-27T04:50:14.425117: step 5653, loss 0.409201, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 262/599 (start=16768, end=16832)\n",
      "2018-11-27T04:50:14.746769: step 5654, loss 0.442636, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 263/599 (start=16832, end=16896)\n",
      "2018-11-27T04:50:15.093924: step 5655, loss 0.506487, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 264/599 (start=16896, end=16960)\n",
      "2018-11-27T04:50:15.431886: step 5656, loss 0.425981, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 265/599 (start=16960, end=17024)\n",
      "2018-11-27T04:50:15.765865: step 5657, loss 0.512791, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 266/599 (start=17024, end=17088)\n",
      "2018-11-27T04:50:16.105744: step 5658, loss 0.564312, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 267/599 (start=17088, end=17152)\n",
      "2018-11-27T04:50:16.430257: step 5659, loss 0.579219, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 268/599 (start=17152, end=17216)\n",
      "2018-11-27T04:50:16.742693: step 5660, loss 0.379798, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 269/599 (start=17216, end=17280)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:50:17.062446: step 5661, loss 0.641339, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 270/599 (start=17280, end=17344)\n",
      "2018-11-27T04:50:17.380475: step 5662, loss 0.43534, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 271/599 (start=17344, end=17408)\n",
      "2018-11-27T04:50:17.717239: step 5663, loss 0.478812, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 272/599 (start=17408, end=17472)\n",
      "2018-11-27T04:50:18.029548: step 5664, loss 0.509534, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 273/599 (start=17472, end=17536)\n",
      "2018-11-27T04:50:18.364616: step 5665, loss 0.362101, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 274/599 (start=17536, end=17600)\n",
      "2018-11-27T04:50:18.694806: step 5666, loss 0.621649, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 275/599 (start=17600, end=17664)\n",
      "2018-11-27T04:50:19.018757: step 5667, loss 0.522262, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 276/599 (start=17664, end=17728)\n",
      "2018-11-27T04:50:19.361210: step 5668, loss 0.547051, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 277/599 (start=17728, end=17792)\n",
      "2018-11-27T04:50:19.691510: step 5669, loss 0.629213, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 278/599 (start=17792, end=17856)\n",
      "2018-11-27T04:50:20.015990: step 5670, loss 0.299583, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 279/599 (start=17856, end=17920)\n",
      "2018-11-27T04:50:20.351585: step 5671, loss 0.45433, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 280/599 (start=17920, end=17984)\n",
      "2018-11-27T04:50:20.683911: step 5672, loss 0.372031, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 281/599 (start=17984, end=18048)\n",
      "2018-11-27T04:50:20.997172: step 5673, loss 0.496449, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 282/599 (start=18048, end=18112)\n",
      "2018-11-27T04:50:21.309783: step 5674, loss 0.471359, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 283/599 (start=18112, end=18176)\n",
      "2018-11-27T04:50:21.656973: step 5675, loss 0.385904, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 284/599 (start=18176, end=18240)\n",
      "2018-11-27T04:50:21.987652: step 5676, loss 0.550012, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 285/599 (start=18240, end=18304)\n",
      "2018-11-27T04:50:22.333986: step 5677, loss 0.381855, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 286/599 (start=18304, end=18368)\n",
      "2018-11-27T04:50:22.682715: step 5678, loss 0.781631, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 287/599 (start=18368, end=18432)\n",
      "2018-11-27T04:50:23.013111: step 5679, loss 0.483181, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 288/599 (start=18432, end=18496)\n",
      "2018-11-27T04:50:23.340622: step 5680, loss 0.466375, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 289/599 (start=18496, end=18560)\n",
      "2018-11-27T04:50:23.693099: step 5681, loss 0.402976, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 290/599 (start=18560, end=18624)\n",
      "2018-11-27T04:50:24.043150: step 5682, loss 0.709886, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 291/599 (start=18624, end=18688)\n",
      "2018-11-27T04:50:24.370140: step 5683, loss 0.448259, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 292/599 (start=18688, end=18752)\n",
      "2018-11-27T04:50:24.702330: step 5684, loss 0.575337, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 293/599 (start=18752, end=18816)\n",
      "2018-11-27T04:50:25.022870: step 5685, loss 0.541836, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 294/599 (start=18816, end=18880)\n",
      "2018-11-27T04:50:25.350460: step 5686, loss 0.453623, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 295/599 (start=18880, end=18944)\n",
      "2018-11-27T04:50:25.687928: step 5687, loss 0.557076, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 296/599 (start=18944, end=19008)\n",
      "2018-11-27T04:50:26.007987: step 5688, loss 0.565976, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 297/599 (start=19008, end=19072)\n",
      "2018-11-27T04:50:26.344587: step 5689, loss 0.550765, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 298/599 (start=19072, end=19136)\n",
      "2018-11-27T04:50:26.682064: step 5690, loss 0.527383, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 299/599 (start=19136, end=19200)\n",
      "2018-11-27T04:50:27.021056: step 5691, loss 0.494807, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 300/599 (start=19200, end=19264)\n",
      "2018-11-27T04:50:27.356272: step 5692, loss 0.495707, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 301/599 (start=19264, end=19328)\n",
      "2018-11-27T04:50:27.703403: step 5693, loss 0.467425, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 302/599 (start=19328, end=19392)\n",
      "2018-11-27T04:50:28.045607: step 5694, loss 0.441115, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 303/599 (start=19392, end=19456)\n",
      "2018-11-27T04:50:28.386925: step 5695, loss 0.638133, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 304/599 (start=19456, end=19520)\n",
      "2018-11-27T04:50:28.736672: step 5696, loss 0.506428, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 305/599 (start=19520, end=19584)\n",
      "2018-11-27T04:50:29.084597: step 5697, loss 0.535067, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 306/599 (start=19584, end=19648)\n",
      "2018-11-27T04:50:29.431213: step 5698, loss 0.579372, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 307/599 (start=19648, end=19712)\n",
      "2018-11-27T04:50:29.749082: step 5699, loss 0.661251, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 308/599 (start=19712, end=19776)\n",
      "2018-11-27T04:50:30.075979: step 5700, loss 0.526795, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:50:36.325388: step 5700, loss 1.98605, acc 0.498394\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-5700\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 309/599 (start=19776, end=19840)\n",
      "2018-11-27T04:50:37.096215: step 5701, loss 0.48451, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 310/599 (start=19840, end=19904)\n",
      "2018-11-27T04:50:37.416606: step 5702, loss 0.394684, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 311/599 (start=19904, end=19968)\n",
      "2018-11-27T04:50:37.734209: step 5703, loss 0.5068, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 312/599 (start=19968, end=20032)\n",
      "2018-11-27T04:50:38.054917: step 5704, loss 0.544021, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 313/599 (start=20032, end=20096)\n",
      "2018-11-27T04:50:38.377446: step 5705, loss 0.727428, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 314/599 (start=20096, end=20160)\n",
      "2018-11-27T04:50:38.713097: step 5706, loss 0.561677, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 315/599 (start=20160, end=20224)\n",
      "2018-11-27T04:50:39.057540: step 5707, loss 0.490217, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 316/599 (start=20224, end=20288)\n",
      "2018-11-27T04:50:39.398994: step 5708, loss 0.334684, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 317/599 (start=20288, end=20352)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:50:39.738230: step 5709, loss 0.483337, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 318/599 (start=20352, end=20416)\n",
      "2018-11-27T04:50:40.071047: step 5710, loss 0.363447, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 319/599 (start=20416, end=20480)\n",
      "2018-11-27T04:50:40.389077: step 5711, loss 0.675059, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 320/599 (start=20480, end=20544)\n",
      "2018-11-27T04:50:40.738296: step 5712, loss 0.768613, acc 0.78125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 321/599 (start=20544, end=20608)\n",
      "2018-11-27T04:50:41.074861: step 5713, loss 0.403015, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 322/599 (start=20608, end=20672)\n",
      "2018-11-27T04:50:41.434324: step 5714, loss 0.543029, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 323/599 (start=20672, end=20736)\n",
      "2018-11-27T04:50:41.779287: step 5715, loss 0.49497, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 324/599 (start=20736, end=20800)\n",
      "2018-11-27T04:50:42.111650: step 5716, loss 0.486238, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 325/599 (start=20800, end=20864)\n",
      "2018-11-27T04:50:42.420559: step 5717, loss 0.47219, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 326/599 (start=20864, end=20928)\n",
      "2018-11-27T04:50:42.760854: step 5718, loss 0.364928, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 327/599 (start=20928, end=20992)\n",
      "2018-11-27T04:50:43.086670: step 5719, loss 0.576817, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 328/599 (start=20992, end=21056)\n",
      "2018-11-27T04:50:43.422749: step 5720, loss 0.494823, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 329/599 (start=21056, end=21120)\n",
      "2018-11-27T04:50:43.761581: step 5721, loss 0.649723, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 330/599 (start=21120, end=21184)\n",
      "2018-11-27T04:50:44.104760: step 5722, loss 0.514027, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 331/599 (start=21184, end=21248)\n",
      "2018-11-27T04:50:44.443335: step 5723, loss 0.49667, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 332/599 (start=21248, end=21312)\n",
      "2018-11-27T04:50:44.781228: step 5724, loss 0.419362, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 333/599 (start=21312, end=21376)\n",
      "2018-11-27T04:50:45.126944: step 5725, loss 0.470427, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 334/599 (start=21376, end=21440)\n",
      "2018-11-27T04:50:45.439993: step 5726, loss 0.47511, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 335/599 (start=21440, end=21504)\n",
      "2018-11-27T04:50:45.771407: step 5727, loss 0.429145, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 336/599 (start=21504, end=21568)\n",
      "2018-11-27T04:50:46.108891: step 5728, loss 0.443436, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 337/599 (start=21568, end=21632)\n",
      "2018-11-27T04:50:46.447501: step 5729, loss 0.594507, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 338/599 (start=21632, end=21696)\n",
      "2018-11-27T04:50:46.789066: step 5730, loss 0.50995, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 339/599 (start=21696, end=21760)\n",
      "2018-11-27T04:50:47.124878: step 5731, loss 0.36319, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 340/599 (start=21760, end=21824)\n",
      "2018-11-27T04:50:47.458374: step 5732, loss 0.643086, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 341/599 (start=21824, end=21888)\n",
      "2018-11-27T04:50:47.796665: step 5733, loss 0.641072, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 342/599 (start=21888, end=21952)\n",
      "2018-11-27T04:50:48.140602: step 5734, loss 0.495287, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 343/599 (start=21952, end=22016)\n",
      "2018-11-27T04:50:48.461918: step 5735, loss 0.569142, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 344/599 (start=22016, end=22080)\n",
      "2018-11-27T04:50:48.788496: step 5736, loss 0.61746, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 345/599 (start=22080, end=22144)\n",
      "2018-11-27T04:50:49.112408: step 5737, loss 0.432227, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 346/599 (start=22144, end=22208)\n",
      "2018-11-27T04:50:49.430755: step 5738, loss 0.761399, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 347/599 (start=22208, end=22272)\n",
      "2018-11-27T04:50:49.759680: step 5739, loss 0.481491, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 348/599 (start=22272, end=22336)\n",
      "2018-11-27T04:50:50.099999: step 5740, loss 0.394423, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 349/599 (start=22336, end=22400)\n",
      "2018-11-27T04:50:50.457028: step 5741, loss 0.59269, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 350/599 (start=22400, end=22464)\n",
      "2018-11-27T04:50:50.790809: step 5742, loss 0.380976, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 351/599 (start=22464, end=22528)\n",
      "2018-11-27T04:50:51.131135: step 5743, loss 0.555788, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 352/599 (start=22528, end=22592)\n",
      "2018-11-27T04:50:51.478528: step 5744, loss 0.41286, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 353/599 (start=22592, end=22656)\n",
      "2018-11-27T04:50:51.807800: step 5745, loss 0.432916, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 354/599 (start=22656, end=22720)\n",
      "2018-11-27T04:50:52.122391: step 5746, loss 0.53372, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 355/599 (start=22720, end=22784)\n",
      "2018-11-27T04:50:52.439558: step 5747, loss 0.644715, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 356/599 (start=22784, end=22848)\n",
      "2018-11-27T04:50:52.775624: step 5748, loss 0.621218, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 357/599 (start=22848, end=22912)\n",
      "2018-11-27T04:50:53.116794: step 5749, loss 0.485806, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 358/599 (start=22912, end=22976)\n",
      "2018-11-27T04:50:53.428942: step 5750, loss 0.677043, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 359/599 (start=22976, end=23040)\n",
      "2018-11-27T04:50:53.769917: step 5751, loss 0.66468, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 360/599 (start=23040, end=23104)\n",
      "2018-11-27T04:50:54.114951: step 5752, loss 0.440774, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 361/599 (start=23104, end=23168)\n",
      "2018-11-27T04:50:54.442623: step 5753, loss 0.592582, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 362/599 (start=23168, end=23232)\n",
      "2018-11-27T04:50:54.781372: step 5754, loss 0.43757, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 363/599 (start=23232, end=23296)\n",
      "2018-11-27T04:50:55.103656: step 5755, loss 0.609776, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 364/599 (start=23296, end=23360)\n",
      "2018-11-27T04:50:55.432517: step 5756, loss 0.455491, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 365/599 (start=23360, end=23424)\n",
      "2018-11-27T04:50:55.774961: step 5757, loss 0.52462, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 366/599 (start=23424, end=23488)\n",
      "2018-11-27T04:50:56.119422: step 5758, loss 0.516129, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 367/599 (start=23488, end=23552)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:50:56.460121: step 5759, loss 0.486435, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 368/599 (start=23552, end=23616)\n",
      "2018-11-27T04:50:56.782315: step 5760, loss 0.4853, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 369/599 (start=23616, end=23680)\n",
      "2018-11-27T04:50:57.130180: step 5761, loss 0.368576, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 370/599 (start=23680, end=23744)\n",
      "2018-11-27T04:50:57.444752: step 5762, loss 0.49891, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 371/599 (start=23744, end=23808)\n",
      "2018-11-27T04:50:57.759326: step 5763, loss 0.79615, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 372/599 (start=23808, end=23872)\n",
      "2018-11-27T04:50:58.092227: step 5764, loss 0.472894, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 373/599 (start=23872, end=23936)\n",
      "2018-11-27T04:50:58.416150: step 5765, loss 0.390315, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 374/599 (start=23936, end=24000)\n",
      "2018-11-27T04:50:58.738054: step 5766, loss 0.501013, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 375/599 (start=24000, end=24064)\n",
      "2018-11-27T04:50:59.068895: step 5767, loss 0.521092, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 376/599 (start=24064, end=24128)\n",
      "2018-11-27T04:50:59.399755: step 5768, loss 0.38868, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 377/599 (start=24128, end=24192)\n",
      "2018-11-27T04:50:59.725356: step 5769, loss 0.414414, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 378/599 (start=24192, end=24256)\n",
      "2018-11-27T04:51:00.071158: step 5770, loss 0.60846, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 379/599 (start=24256, end=24320)\n",
      "2018-11-27T04:51:00.401434: step 5771, loss 0.623954, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 380/599 (start=24320, end=24384)\n",
      "2018-11-27T04:51:00.737427: step 5772, loss 0.453322, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 381/599 (start=24384, end=24448)\n",
      "2018-11-27T04:51:01.075796: step 5773, loss 0.392694, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 382/599 (start=24448, end=24512)\n",
      "2018-11-27T04:51:01.397862: step 5774, loss 0.459062, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 383/599 (start=24512, end=24576)\n",
      "2018-11-27T04:51:01.734333: step 5775, loss 0.5177, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 384/599 (start=24576, end=24640)\n",
      "2018-11-27T04:51:02.060597: step 5776, loss 0.429375, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 385/599 (start=24640, end=24704)\n",
      "2018-11-27T04:51:02.407662: step 5777, loss 0.457809, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 386/599 (start=24704, end=24768)\n",
      "2018-11-27T04:51:02.720643: step 5778, loss 0.626147, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 387/599 (start=24768, end=24832)\n",
      "2018-11-27T04:51:03.062093: step 5779, loss 0.47833, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 388/599 (start=24832, end=24896)\n",
      "2018-11-27T04:51:03.389082: step 5780, loss 0.591746, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 389/599 (start=24896, end=24960)\n",
      "2018-11-27T04:51:03.730931: step 5781, loss 0.66648, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 390/599 (start=24960, end=25024)\n",
      "2018-11-27T04:51:04.063114: step 5782, loss 0.46998, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 391/599 (start=25024, end=25088)\n",
      "2018-11-27T04:51:04.402308: step 5783, loss 0.466119, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 392/599 (start=25088, end=25152)\n",
      "2018-11-27T04:51:04.721236: step 5784, loss 0.431364, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 393/599 (start=25152, end=25216)\n",
      "2018-11-27T04:51:05.059131: step 5785, loss 0.578609, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 394/599 (start=25216, end=25280)\n",
      "2018-11-27T04:51:05.388233: step 5786, loss 0.507235, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 395/599 (start=25280, end=25344)\n",
      "2018-11-27T04:51:05.713895: step 5787, loss 0.428627, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 396/599 (start=25344, end=25408)\n",
      "2018-11-27T04:51:06.033992: step 5788, loss 0.49536, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 397/599 (start=25408, end=25472)\n",
      "2018-11-27T04:51:06.353410: step 5789, loss 0.417507, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 398/599 (start=25472, end=25536)\n",
      "2018-11-27T04:51:06.712219: step 5790, loss 0.706054, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 399/599 (start=25536, end=25600)\n",
      "2018-11-27T04:51:07.048149: step 5791, loss 0.417947, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 400/599 (start=25600, end=25664)\n",
      "2018-11-27T04:51:07.372623: step 5792, loss 0.37957, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 401/599 (start=25664, end=25728)\n",
      "2018-11-27T04:51:07.692578: step 5793, loss 0.404605, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 402/599 (start=25728, end=25792)\n",
      "2018-11-27T04:51:08.042441: step 5794, loss 0.449261, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 403/599 (start=25792, end=25856)\n",
      "2018-11-27T04:51:08.387922: step 5795, loss 0.554422, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 404/599 (start=25856, end=25920)\n",
      "2018-11-27T04:51:08.715234: step 5796, loss 0.518936, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 405/599 (start=25920, end=25984)\n",
      "2018-11-27T04:51:09.048528: step 5797, loss 0.686476, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 406/599 (start=25984, end=26048)\n",
      "2018-11-27T04:51:09.372014: step 5798, loss 0.493913, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 407/599 (start=26048, end=26112)\n",
      "2018-11-27T04:51:09.710042: step 5799, loss 0.634774, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 408/599 (start=26112, end=26176)\n",
      "2018-11-27T04:51:10.024205: step 5800, loss 0.671156, acc 0.8125\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:51:16.358638: step 5800, loss 1.98642, acc 0.500039\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-5800\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 409/599 (start=26176, end=26240)\n",
      "2018-11-27T04:51:17.093755: step 5801, loss 0.731202, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 410/599 (start=26240, end=26304)\n",
      "2018-11-27T04:51:17.440558: step 5802, loss 0.346266, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 411/599 (start=26304, end=26368)\n",
      "2018-11-27T04:51:17.778402: step 5803, loss 0.431892, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 412/599 (start=26368, end=26432)\n",
      "2018-11-27T04:51:18.107946: step 5804, loss 0.395732, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 413/599 (start=26432, end=26496)\n",
      "2018-11-27T04:51:18.436857: step 5805, loss 0.429809, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 414/599 (start=26496, end=26560)\n",
      "2018-11-27T04:51:18.779784: step 5806, loss 0.437345, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 415/599 (start=26560, end=26624)\n",
      "2018-11-27T04:51:19.091081: step 5807, loss 0.465854, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 416/599 (start=26624, end=26688)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:51:19.420485: step 5808, loss 0.451489, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 417/599 (start=26688, end=26752)\n",
      "2018-11-27T04:51:19.752083: step 5809, loss 0.576524, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 418/599 (start=26752, end=26816)\n",
      "2018-11-27T04:51:20.065946: step 5810, loss 0.576305, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 419/599 (start=26816, end=26880)\n",
      "2018-11-27T04:51:20.407663: step 5811, loss 0.585595, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 420/599 (start=26880, end=26944)\n",
      "2018-11-27T04:51:20.748003: step 5812, loss 0.497314, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 421/599 (start=26944, end=27008)\n",
      "2018-11-27T04:51:21.078466: step 5813, loss 0.580248, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 422/599 (start=27008, end=27072)\n",
      "2018-11-27T04:51:21.396152: step 5814, loss 0.422999, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 423/599 (start=27072, end=27136)\n",
      "2018-11-27T04:51:21.715944: step 5815, loss 0.487254, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 424/599 (start=27136, end=27200)\n",
      "2018-11-27T04:51:22.038564: step 5816, loss 0.396861, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 425/599 (start=27200, end=27264)\n",
      "2018-11-27T04:51:22.374497: step 5817, loss 0.513712, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 426/599 (start=27264, end=27328)\n",
      "2018-11-27T04:51:22.681707: step 5818, loss 0.378394, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 427/599 (start=27328, end=27392)\n",
      "2018-11-27T04:51:23.018553: step 5819, loss 0.415835, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 428/599 (start=27392, end=27456)\n",
      "2018-11-27T04:51:23.366502: step 5820, loss 0.630374, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 429/599 (start=27456, end=27520)\n",
      "2018-11-27T04:51:23.691561: step 5821, loss 0.481595, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 430/599 (start=27520, end=27584)\n",
      "2018-11-27T04:51:24.036026: step 5822, loss 0.644252, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 431/599 (start=27584, end=27648)\n",
      "2018-11-27T04:51:24.350265: step 5823, loss 0.538068, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 432/599 (start=27648, end=27712)\n",
      "2018-11-27T04:51:24.671504: step 5824, loss 0.675021, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 433/599 (start=27712, end=27776)\n",
      "2018-11-27T04:51:25.014813: step 5825, loss 0.399092, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 434/599 (start=27776, end=27840)\n",
      "2018-11-27T04:51:25.336300: step 5826, loss 0.614173, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 435/599 (start=27840, end=27904)\n",
      "2018-11-27T04:51:25.652359: step 5827, loss 0.440859, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 436/599 (start=27904, end=27968)\n",
      "2018-11-27T04:51:25.994397: step 5828, loss 0.63542, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 437/599 (start=27968, end=28032)\n",
      "2018-11-27T04:51:26.333387: step 5829, loss 0.304219, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 438/599 (start=28032, end=28096)\n",
      "2018-11-27T04:51:26.668564: step 5830, loss 0.740561, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 439/599 (start=28096, end=28160)\n",
      "2018-11-27T04:51:27.007835: step 5831, loss 0.38966, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 440/599 (start=28160, end=28224)\n",
      "2018-11-27T04:51:27.367317: step 5832, loss 0.544171, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 441/599 (start=28224, end=28288)\n",
      "2018-11-27T04:51:27.697362: step 5833, loss 0.410212, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 442/599 (start=28288, end=28352)\n",
      "2018-11-27T04:51:28.038196: step 5834, loss 0.899137, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 443/599 (start=28352, end=28416)\n",
      "2018-11-27T04:51:28.357585: step 5835, loss 0.489941, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 444/599 (start=28416, end=28480)\n",
      "2018-11-27T04:51:28.699914: step 5836, loss 0.721587, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 445/599 (start=28480, end=28544)\n",
      "2018-11-27T04:51:29.033791: step 5837, loss 0.491758, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 446/599 (start=28544, end=28608)\n",
      "2018-11-27T04:51:29.380690: step 5838, loss 0.559647, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 447/599 (start=28608, end=28672)\n",
      "2018-11-27T04:51:29.719231: step 5839, loss 0.589818, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 448/599 (start=28672, end=28736)\n",
      "2018-11-27T04:51:30.064916: step 5840, loss 0.543769, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 449/599 (start=28736, end=28800)\n",
      "2018-11-27T04:51:30.389460: step 5841, loss 0.732749, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 450/599 (start=28800, end=28864)\n",
      "2018-11-27T04:51:30.725666: step 5842, loss 0.393383, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 451/599 (start=28864, end=28928)\n",
      "2018-11-27T04:51:31.038213: step 5843, loss 0.46694, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 452/599 (start=28928, end=28992)\n",
      "2018-11-27T04:51:31.367228: step 5844, loss 0.508779, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 453/599 (start=28992, end=29056)\n",
      "2018-11-27T04:51:31.701394: step 5845, loss 0.468559, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 454/599 (start=29056, end=29120)\n",
      "2018-11-27T04:51:32.024347: step 5846, loss 0.47598, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 455/599 (start=29120, end=29184)\n",
      "2018-11-27T04:51:32.362686: step 5847, loss 0.344885, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 456/599 (start=29184, end=29248)\n",
      "2018-11-27T04:51:32.710586: step 5848, loss 0.352738, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 457/599 (start=29248, end=29312)\n",
      "2018-11-27T04:51:33.034241: step 5849, loss 0.506916, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 458/599 (start=29312, end=29376)\n",
      "2018-11-27T04:51:33.379151: step 5850, loss 0.611788, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 459/599 (start=29376, end=29440)\n",
      "2018-11-27T04:51:33.696991: step 5851, loss 0.37878, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 460/599 (start=29440, end=29504)\n",
      "2018-11-27T04:51:34.021431: step 5852, loss 0.519475, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 461/599 (start=29504, end=29568)\n",
      "2018-11-27T04:51:34.367332: step 5853, loss 0.478407, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 462/599 (start=29568, end=29632)\n",
      "2018-11-27T04:51:34.701772: step 5854, loss 0.457797, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 463/599 (start=29632, end=29696)\n",
      "2018-11-27T04:51:35.055018: step 5855, loss 0.448931, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 464/599 (start=29696, end=29760)\n",
      "2018-11-27T04:51:35.386543: step 5856, loss 0.534144, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 465/599 (start=29760, end=29824)\n",
      "2018-11-27T04:51:35.730467: step 5857, loss 0.588922, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 466/599 (start=29824, end=29888)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:51:36.056173: step 5858, loss 0.52576, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 467/599 (start=29888, end=29952)\n",
      "2018-11-27T04:51:36.367804: step 5859, loss 0.387626, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 468/599 (start=29952, end=30016)\n",
      "2018-11-27T04:51:36.694480: step 5860, loss 0.76678, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 469/599 (start=30016, end=30080)\n",
      "2018-11-27T04:51:37.009975: step 5861, loss 0.539297, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 470/599 (start=30080, end=30144)\n",
      "2018-11-27T04:51:37.339923: step 5862, loss 0.536394, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 471/599 (start=30144, end=30208)\n",
      "2018-11-27T04:51:37.679866: step 5863, loss 0.363423, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 472/599 (start=30208, end=30272)\n",
      "2018-11-27T04:51:38.012816: step 5864, loss 0.500436, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 473/599 (start=30272, end=30336)\n",
      "2018-11-27T04:51:38.363445: step 5865, loss 0.449706, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 474/599 (start=30336, end=30400)\n",
      "2018-11-27T04:51:38.702237: step 5866, loss 0.501046, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 475/599 (start=30400, end=30464)\n",
      "2018-11-27T04:51:39.053748: step 5867, loss 0.528679, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 476/599 (start=30464, end=30528)\n",
      "2018-11-27T04:51:39.398309: step 5868, loss 0.560133, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 477/599 (start=30528, end=30592)\n",
      "2018-11-27T04:51:39.710125: step 5869, loss 0.630207, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 478/599 (start=30592, end=30656)\n",
      "2018-11-27T04:51:40.046009: step 5870, loss 0.550638, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 479/599 (start=30656, end=30720)\n",
      "2018-11-27T04:51:40.393192: step 5871, loss 0.54863, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 480/599 (start=30720, end=30784)\n",
      "2018-11-27T04:51:40.726059: step 5872, loss 0.589733, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 481/599 (start=30784, end=30848)\n",
      "2018-11-27T04:51:41.068222: step 5873, loss 0.418255, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 482/599 (start=30848, end=30912)\n",
      "2018-11-27T04:51:41.403893: step 5874, loss 0.664505, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 483/599 (start=30912, end=30976)\n",
      "2018-11-27T04:51:41.732708: step 5875, loss 0.620698, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 484/599 (start=30976, end=31040)\n",
      "2018-11-27T04:51:42.066438: step 5876, loss 0.549443, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 485/599 (start=31040, end=31104)\n",
      "2018-11-27T04:51:42.381833: step 5877, loss 0.340477, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 486/599 (start=31104, end=31168)\n",
      "2018-11-27T04:51:42.714262: step 5878, loss 0.465123, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 487/599 (start=31168, end=31232)\n",
      "2018-11-27T04:51:43.062336: step 5879, loss 0.536818, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 488/599 (start=31232, end=31296)\n",
      "2018-11-27T04:51:43.395325: step 5880, loss 0.650839, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 489/599 (start=31296, end=31360)\n",
      "2018-11-27T04:51:43.729618: step 5881, loss 0.457433, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 490/599 (start=31360, end=31424)\n",
      "2018-11-27T04:51:44.077426: step 5882, loss 0.399137, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 491/599 (start=31424, end=31488)\n",
      "2018-11-27T04:51:44.419010: step 5883, loss 0.362129, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 492/599 (start=31488, end=31552)\n",
      "2018-11-27T04:51:44.746948: step 5884, loss 0.528894, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 493/599 (start=31552, end=31616)\n",
      "2018-11-27T04:51:45.066878: step 5885, loss 0.608785, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 494/599 (start=31616, end=31680)\n",
      "2018-11-27T04:51:45.416279: step 5886, loss 0.714576, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 495/599 (start=31680, end=31744)\n",
      "2018-11-27T04:51:45.746450: step 5887, loss 0.554722, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 496/599 (start=31744, end=31808)\n",
      "2018-11-27T04:51:46.077375: step 5888, loss 0.687856, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 497/599 (start=31808, end=31872)\n",
      "2018-11-27T04:51:46.426328: step 5889, loss 0.373287, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 498/599 (start=31872, end=31936)\n",
      "2018-11-27T04:51:46.763295: step 5890, loss 0.49165, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 499/599 (start=31936, end=32000)\n",
      "2018-11-27T04:51:47.094471: step 5891, loss 0.519004, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 500/599 (start=32000, end=32064)\n",
      "2018-11-27T04:51:47.436000: step 5892, loss 0.426012, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 501/599 (start=32064, end=32128)\n",
      "2018-11-27T04:51:47.782639: step 5893, loss 0.621668, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 502/599 (start=32128, end=32192)\n",
      "2018-11-27T04:51:48.127229: step 5894, loss 0.516338, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 503/599 (start=32192, end=32256)\n",
      "2018-11-27T04:51:48.447486: step 5895, loss 0.578533, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 504/599 (start=32256, end=32320)\n",
      "2018-11-27T04:51:48.793760: step 5896, loss 0.528668, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 505/599 (start=32320, end=32384)\n",
      "2018-11-27T04:51:49.103287: step 5897, loss 0.751989, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 506/599 (start=32384, end=32448)\n",
      "2018-11-27T04:51:49.421956: step 5898, loss 0.365005, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 507/599 (start=32448, end=32512)\n",
      "2018-11-27T04:51:49.760994: step 5899, loss 0.539981, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 508/599 (start=32512, end=32576)\n",
      "2018-11-27T04:51:50.109520: step 5900, loss 0.501656, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2018-11-27T04:51:56.224421: step 5900, loss 2.01312, acc 0.519395\n",
      "\n",
      "Saved model checkpoint to /home/jcworkma/jack/w266-group-project_lyric-mood-classification/logs/tf/runs/Em-300_FS-3-4-5_NF-64_D-0.5_L2-0.01_B-64_Ep-10_W2V-1_V-50000/checkpoints/model-5900\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 509/599 (start=32576, end=32640)\n",
      "2018-11-27T04:51:56.992201: step 5901, loss 0.448263, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 510/599 (start=32640, end=32704)\n",
      "2018-11-27T04:51:57.314460: step 5902, loss 0.44481, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 511/599 (start=32704, end=32768)\n",
      "2018-11-27T04:51:57.635099: step 5903, loss 0.663247, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 512/599 (start=32768, end=32832)\n",
      "2018-11-27T04:51:57.963172: step 5904, loss 0.612644, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 513/599 (start=32832, end=32896)\n",
      "2018-11-27T04:51:58.277097: step 5905, loss 0.565089, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 514/599 (start=32896, end=32960)\n",
      "2018-11-27T04:51:58.622709: step 5906, loss 0.410439, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 515/599 (start=32960, end=33024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:51:58.962860: step 5907, loss 0.673091, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 516/599 (start=33024, end=33088)\n",
      "2018-11-27T04:51:59.302512: step 5908, loss 0.499873, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 517/599 (start=33088, end=33152)\n",
      "2018-11-27T04:51:59.629280: step 5909, loss 0.590708, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 518/599 (start=33152, end=33216)\n",
      "2018-11-27T04:51:59.947663: step 5910, loss 0.363601, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 519/599 (start=33216, end=33280)\n",
      "2018-11-27T04:52:00.289885: step 5911, loss 0.457008, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 520/599 (start=33280, end=33344)\n",
      "2018-11-27T04:52:00.605179: step 5912, loss 0.63443, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 521/599 (start=33344, end=33408)\n",
      "2018-11-27T04:52:00.964565: step 5913, loss 0.503383, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 522/599 (start=33408, end=33472)\n",
      "2018-11-27T04:52:01.311550: step 5914, loss 0.647661, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 523/599 (start=33472, end=33536)\n",
      "2018-11-27T04:52:01.649622: step 5915, loss 0.654738, acc 0.828125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 524/599 (start=33536, end=33600)\n",
      "2018-11-27T04:52:01.960734: step 5916, loss 0.514716, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 525/599 (start=33600, end=33664)\n",
      "2018-11-27T04:52:02.312189: step 5917, loss 0.502004, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 526/599 (start=33664, end=33728)\n",
      "2018-11-27T04:52:02.654535: step 5918, loss 0.690181, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 527/599 (start=33728, end=33792)\n",
      "2018-11-27T04:52:02.992350: step 5919, loss 0.650314, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 528/599 (start=33792, end=33856)\n",
      "2018-11-27T04:52:03.338342: step 5920, loss 0.563049, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 529/599 (start=33856, end=33920)\n",
      "2018-11-27T04:52:03.659287: step 5921, loss 0.513421, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 530/599 (start=33920, end=33984)\n",
      "2018-11-27T04:52:03.970428: step 5922, loss 0.549792, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 531/599 (start=33984, end=34048)\n",
      "2018-11-27T04:52:04.305488: step 5923, loss 0.587137, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 532/599 (start=34048, end=34112)\n",
      "2018-11-27T04:52:04.651269: step 5924, loss 0.592102, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 533/599 (start=34112, end=34176)\n",
      "2018-11-27T04:52:04.990473: step 5925, loss 0.435348, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 534/599 (start=34176, end=34240)\n",
      "2018-11-27T04:52:05.331783: step 5926, loss 0.713642, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 535/599 (start=34240, end=34304)\n",
      "2018-11-27T04:52:05.661819: step 5927, loss 0.529833, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 536/599 (start=34304, end=34368)\n",
      "2018-11-27T04:52:05.967488: step 5928, loss 0.675952, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 537/599 (start=34368, end=34432)\n",
      "2018-11-27T04:52:06.305646: step 5929, loss 0.388743, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 538/599 (start=34432, end=34496)\n",
      "2018-11-27T04:52:06.641535: step 5930, loss 0.46257, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 539/599 (start=34496, end=34560)\n",
      "2018-11-27T04:52:06.972408: step 5931, loss 0.553599, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 540/599 (start=34560, end=34624)\n",
      "2018-11-27T04:52:07.292752: step 5932, loss 0.439786, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 541/599 (start=34624, end=34688)\n",
      "2018-11-27T04:52:07.626973: step 5933, loss 0.430532, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 542/599 (start=34688, end=34752)\n",
      "2018-11-27T04:52:07.987467: step 5934, loss 0.486109, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 543/599 (start=34752, end=34816)\n",
      "2018-11-27T04:52:08.338806: step 5935, loss 0.511329, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 544/599 (start=34816, end=34880)\n",
      "2018-11-27T04:52:08.659645: step 5936, loss 0.668874, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 545/599 (start=34880, end=34944)\n",
      "2018-11-27T04:52:08.989028: step 5937, loss 0.510916, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 546/599 (start=34944, end=35008)\n",
      "2018-11-27T04:52:09.318494: step 5938, loss 0.490887, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 547/599 (start=35008, end=35072)\n",
      "2018-11-27T04:52:09.636973: step 5939, loss 0.470584, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 548/599 (start=35072, end=35136)\n",
      "2018-11-27T04:52:09.979367: step 5940, loss 0.538849, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 549/599 (start=35136, end=35200)\n",
      "2018-11-27T04:52:10.316601: step 5941, loss 0.768348, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 550/599 (start=35200, end=35264)\n",
      "2018-11-27T04:52:10.644512: step 5942, loss 0.486381, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 551/599 (start=35264, end=35328)\n",
      "2018-11-27T04:52:10.961082: step 5943, loss 0.633832, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 552/599 (start=35328, end=35392)\n",
      "2018-11-27T04:52:11.298297: step 5944, loss 0.497591, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 553/599 (start=35392, end=35456)\n",
      "2018-11-27T04:52:11.647576: step 5945, loss 0.677729, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 554/599 (start=35456, end=35520)\n",
      "2018-11-27T04:52:11.959851: step 5946, loss 0.415548, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 555/599 (start=35520, end=35584)\n",
      "2018-11-27T04:52:12.278924: step 5947, loss 0.545463, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 556/599 (start=35584, end=35648)\n",
      "2018-11-27T04:52:12.603716: step 5948, loss 0.611111, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 557/599 (start=35648, end=35712)\n",
      "2018-11-27T04:52:12.960669: step 5949, loss 0.467757, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 558/599 (start=35712, end=35776)\n",
      "2018-11-27T04:52:13.291860: step 5950, loss 0.506611, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 559/599 (start=35776, end=35840)\n",
      "2018-11-27T04:52:13.632782: step 5951, loss 0.479457, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 560/599 (start=35840, end=35904)\n",
      "2018-11-27T04:52:13.937396: step 5952, loss 0.444819, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 561/599 (start=35904, end=35968)\n",
      "2018-11-27T04:52:14.277463: step 5953, loss 0.356174, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 562/599 (start=35968, end=36032)\n",
      "2018-11-27T04:52:14.596267: step 5954, loss 0.517416, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 563/599 (start=36032, end=36096)\n",
      "2018-11-27T04:52:14.938457: step 5955, loss 0.361324, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 564/599 (start=36096, end=36160)\n",
      "2018-11-27T04:52:15.260948: step 5956, loss 0.546901, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 565/599 (start=36160, end=36224)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-27T04:52:15.575059: step 5957, loss 0.856854, acc 0.765625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 566/599 (start=36224, end=36288)\n",
      "2018-11-27T04:52:15.896687: step 5958, loss 0.507821, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 567/599 (start=36288, end=36352)\n",
      "2018-11-27T04:52:16.219212: step 5959, loss 0.403151, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 568/599 (start=36352, end=36416)\n",
      "2018-11-27T04:52:16.546452: step 5960, loss 0.52619, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 569/599 (start=36416, end=36480)\n",
      "2018-11-27T04:52:16.900019: step 5961, loss 0.475659, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 570/599 (start=36480, end=36544)\n",
      "2018-11-27T04:52:17.243043: step 5962, loss 0.412393, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 571/599 (start=36544, end=36608)\n",
      "2018-11-27T04:52:17.582299: step 5963, loss 0.562459, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 572/599 (start=36608, end=36672)\n",
      "2018-11-27T04:52:17.897983: step 5964, loss 0.3911, acc 0.96875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 573/599 (start=36672, end=36736)\n",
      "2018-11-27T04:52:18.249086: step 5965, loss 0.543007, acc 0.9375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 574/599 (start=36736, end=36800)\n",
      "2018-11-27T04:52:18.577513: step 5966, loss 0.72092, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 575/599 (start=36800, end=36864)\n",
      "2018-11-27T04:52:18.912095: step 5967, loss 0.671705, acc 0.84375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 576/599 (start=36864, end=36928)\n",
      "2018-11-27T04:52:19.275153: step 5968, loss 0.534087, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 577/599 (start=36928, end=36992)\n",
      "2018-11-27T04:52:19.590085: step 5969, loss 0.527687, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 578/599 (start=36992, end=37056)\n",
      "2018-11-27T04:52:19.935601: step 5970, loss 0.585933, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 579/599 (start=37056, end=37120)\n",
      "2018-11-27T04:52:20.251305: step 5971, loss 0.501991, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 580/599 (start=37120, end=37184)\n",
      "2018-11-27T04:52:20.586570: step 5972, loss 0.499129, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 581/599 (start=37184, end=37248)\n",
      "2018-11-27T04:52:20.924864: step 5973, loss 0.479018, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 582/599 (start=37248, end=37312)\n",
      "2018-11-27T04:52:21.270773: step 5974, loss 0.414088, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 583/599 (start=37312, end=37376)\n",
      "2018-11-27T04:52:21.613338: step 5975, loss 0.485368, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 584/599 (start=37376, end=37440)\n",
      "2018-11-27T04:52:21.934652: step 5976, loss 0.583429, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 585/599 (start=37440, end=37504)\n",
      "2018-11-27T04:52:22.252003: step 5977, loss 0.514943, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 586/599 (start=37504, end=37568)\n",
      "2018-11-27T04:52:22.569058: step 5978, loss 0.62154, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 587/599 (start=37568, end=37632)\n",
      "2018-11-27T04:52:22.915198: step 5979, loss 0.509092, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 588/599 (start=37632, end=37696)\n",
      "2018-11-27T04:52:23.256368: step 5980, loss 0.669739, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 589/599 (start=37696, end=37760)\n",
      "2018-11-27T04:52:23.596139: step 5981, loss 0.526008, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 590/599 (start=37760, end=37824)\n",
      "2018-11-27T04:52:23.944468: step 5982, loss 0.740582, acc 0.8125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 591/599 (start=37824, end=37888)\n",
      "2018-11-27T04:52:24.292740: step 5983, loss 0.674496, acc 0.90625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 592/599 (start=37888, end=37952)\n",
      "2018-11-27T04:52:24.629410: step 5984, loss 0.477888, acc 0.890625\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 593/599 (start=37952, end=38016)\n",
      "2018-11-27T04:52:24.981665: step 5985, loss 0.453195, acc 0.921875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 594/599 (start=38016, end=38080)\n",
      "2018-11-27T04:52:25.304989: step 5986, loss 0.365513, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 595/599 (start=38080, end=38144)\n",
      "2018-11-27T04:52:25.636125: step 5987, loss 0.390666, acc 0.953125\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 596/599 (start=38144, end=38208)\n",
      "2018-11-27T04:52:25.969073: step 5988, loss 0.63058, acc 0.875\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 597/599 (start=38208, end=38272)\n",
      "2018-11-27T04:52:26.309224: step 5989, loss 0.606917, acc 0.859375\n",
      "-----------------------------------------------\n",
      "Epoch 9/10, Batch 598/599 (start=38272, end=38281)\n",
      "2018-11-27T04:52:26.522065: step 5990, loss 0.532382, acc 0.888889\n",
      "\n",
      "Final Test Evaluation:\n",
      "2018-11-27T04:52:32.892868: step 5990, loss 2.02243, acc 0.507954\n"
     ]
    }
   ],
   "source": [
    "V = lyrics2vec.VOCAB_SIZE\n",
    "#V = 50000\n",
    "# need to convert lyrics into numpy 2d arrays\n",
    "# need to convert classes into dummies\n",
    "train_embeddings = False\n",
    "\n",
    "train(\n",
    "    vocab_size=V,\n",
    "    x_train=np.array(list(df_train.normalized_lyrics)),\n",
    "    y_train=pd.get_dummies(df_train.mood).values,\n",
    "    x_dev=np.array(list(df_dev.normalized_lyrics)),\n",
    "    y_dev=pd.get_dummies(df_dev.mood).values,\n",
    "    x_test=np.array(list(df_test.normalized_lyrics)),\n",
    "    y_test=pd.get_dummies(df_test.mood).values,\n",
    "    train_embeddings=train_embeddings,\n",
    "    embeddings=lyrics_vectorizer.final_embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "Reigning Champion: w2v0\n",
    "\n",
    "batch_size of 32 speeds up training but doesn't impact scalars too much  \n",
    "L2 has impact - kind of judged that .01 is the best  \n",
    "Embedding size - 300 > 128 for word2vec; no word2vec 128 still reigns supreme\n",
    "\n",
    "Open questions:\n",
    "* vocab size adjustments? currently a lot of nonsense words are in there - would we be more more accurate if we removed those?\n",
    "* word embeddings - lots of experiments to run there\n",
    "* dropout rate\n",
    "* Can we make a full pipeline that trains word embeddings based on params then uses word embedding to train model based on params then analyses results to find best loss+accuracy point and save accuracy? Then we could let 100 experiments go in a day and see what happens\n",
    "* concatenate word2vec with untrained embeddings\n",
    "* word2vec trainable\n",
    "\n",
    "Using expanded dataset with w2v0 and standard settings yielded test acc of ~52. Loss begin climbing at 4.5k steps at which point acc was ~50. Will drop epoch to 10 as steps went to 12k and 6k should be enough to see improvement. Trying again with w2v1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_w266_project",
   "language": "python",
   "name": ".venv_w266_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
